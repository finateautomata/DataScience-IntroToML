{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydotplus\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The feature space is already lagged. Therefore, you do not need to lag the variables yourself. The data track these companies over 3 years (2018-2020). We will train the data in 2018, validate in 2019, and forecast 2020. Further instructions are given in the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('hw2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>fyear</th>\n",
       "      <th>label</th>\n",
       "      <th>ret</th>\n",
       "      <th>acc</th>\n",
       "      <th>agr</th>\n",
       "      <th>cfp</th>\n",
       "      <th>ep</th>\n",
       "      <th>ni</th>\n",
       "      <th>op</th>\n",
       "      <th>...</th>\n",
       "      <th>grltnoa</th>\n",
       "      <th>conv</th>\n",
       "      <th>operprof</th>\n",
       "      <th>capxint</th>\n",
       "      <th>chpm</th>\n",
       "      <th>alm</th>\n",
       "      <th>hire</th>\n",
       "      <th>herf</th>\n",
       "      <th>bm_ia</th>\n",
       "      <th>me_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15417</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.252472</td>\n",
       "      <td>0.253973</td>\n",
       "      <td>0.148027</td>\n",
       "      <td>0.095118</td>\n",
       "      <td>0.323725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467402</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>-0.016902</td>\n",
       "      <td>0.469687</td>\n",
       "      <td>-0.050260</td>\n",
       "      <td>0.553758</td>\n",
       "      <td>1.622149</td>\n",
       "      <td>0.017834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89031</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>0.215518</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.126233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140084</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>-0.049337</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>0.215316</td>\n",
       "      <td>0.759126</td>\n",
       "      <td>0.318117</td>\n",
       "      <td>0.127617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11154</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.184267</td>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.155644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163587</td>\n",
       "      <td>0.091550</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>-0.035948</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>2.015340</td>\n",
       "      <td>0.072533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13556</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.326014</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>-0.095131</td>\n",
       "      <td>-1.258587</td>\n",
       "      <td>-1.340418</td>\n",
       "      <td>1.527239</td>\n",
       "      <td>-1.582117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.972048</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>1.954118</td>\n",
       "      <td>-0.075192</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>0.114982</td>\n",
       "      <td>0.965078</td>\n",
       "      <td>0.001005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14296</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249443</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>0.519260</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>-0.183793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982905</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.821456</td>\n",
       "      <td>0.269721</td>\n",
       "      <td>-0.048576</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>1.151220</td>\n",
       "      <td>0.039338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>14532</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.080498</td>\n",
       "      <td>-0.064369</td>\n",
       "      <td>-0.050658</td>\n",
       "      <td>0.064593</td>\n",
       "      <td>0.028561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.213658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.226537</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.683570</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>0.383695</td>\n",
       "      <td>1.508840</td>\n",
       "      <td>0.011199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>41355</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015946</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.147297</td>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.064142</td>\n",
       "      <td>-0.030160</td>\n",
       "      <td>0.383195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402497</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>0.031440</td>\n",
       "      <td>0.238843</td>\n",
       "      <td>-0.027287</td>\n",
       "      <td>0.964868</td>\n",
       "      <td>0.199428</td>\n",
       "      <td>2.990846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>13983</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112274</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>-0.219388</td>\n",
       "      <td>-0.345708</td>\n",
       "      <td>-0.429181</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116354</td>\n",
       "      <td>0.021174</td>\n",
       "      <td>-0.419594</td>\n",
       "      <td>0.178488</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.505660</td>\n",
       "      <td>1.127711</td>\n",
       "      <td>0.139232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>47619</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057480</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>-0.033335</td>\n",
       "      <td>-0.010621</td>\n",
       "      <td>-0.123947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093207</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>-0.014232</td>\n",
       "      <td>0.726728</td>\n",
       "      <td>-0.134865</td>\n",
       "      <td>0.242645</td>\n",
       "      <td>3.577947</td>\n",
       "      <td>0.009458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>80362</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057283</td>\n",
       "      <td>0.014109</td>\n",
       "      <td>0.062698</td>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.070647</td>\n",
       "      <td>-0.007255</td>\n",
       "      <td>0.144202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152382</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.607385</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.383695</td>\n",
       "      <td>1.894915</td>\n",
       "      <td>0.117197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      permno   fyear  label       ret       acc       agr       cfp        ep  \\\n",
       "0      15417  2018.0      0  0.102609  0.053313  0.252472  0.253973  0.148027   \n",
       "1      89031  2018.0      1  0.067922 -0.001165  0.215518  0.005945  0.002326   \n",
       "2      11154  2018.0      0 -0.071429  0.000948  0.032913  0.184267  0.024266   \n",
       "3      13556  2018.0      0 -0.326014  0.045752 -0.095131 -1.258587 -1.340418   \n",
       "4      14296  2018.0      0  0.249443  0.016353 -0.020653  0.519260  0.449600   \n",
       "...      ...     ...    ...       ...       ...       ...       ...       ...   \n",
       "2238   14532  2020.0     -1 -0.080498 -0.064369 -0.050658  0.064593  0.028561   \n",
       "2239   41355  2020.0      0  0.015946  0.043579  0.147297  0.082415  0.064142   \n",
       "2240   13983  2020.0      0  0.112274  0.003504 -0.219388 -0.345708 -0.429181   \n",
       "2241   47619  2020.0      0 -0.057480  0.007940 -0.033335 -0.010621 -0.123947   \n",
       "2242   80362  2020.0      0  0.057283  0.014109  0.062698  0.115561  0.070647   \n",
       "\n",
       "            ni        op  ...   grltnoa  conv  operprof   capxint      chpm  \\\n",
       "0     0.095118  0.323725  ...  0.123393   0.0  0.467402  0.089973 -0.016902   \n",
       "1     0.012251  0.126233  ...  0.024414   0.0  0.140084  0.033813 -0.049337   \n",
       "2     0.008571  0.155644  ...  0.114552   1.0  0.163587  0.091550 -0.066428   \n",
       "3     1.527239 -1.582117  ...  0.117086   0.0 -2.972048  0.020384  1.954118   \n",
       "4     0.854538 -0.183793  ...  0.329045   0.0  0.982905  0.014257  0.821456   \n",
       "...        ...       ...  ...       ...   ...       ...       ...       ...   \n",
       "2238  0.000000 -0.213658  ...  0.000831   0.0 -0.226537  0.000677  0.000912   \n",
       "2239 -0.030160  0.383195  ...  0.002844   0.0  0.402497  0.011861  0.031440   \n",
       "2240  0.004651  0.196534  ... -0.196021   0.0  0.116354  0.021174 -0.419594   \n",
       "2241  0.000000  0.096157  ... -0.016991   0.0  0.093207  0.034593 -0.014232   \n",
       "2242 -0.007255  0.144202  ...  0.030756   0.0  0.152382  0.003596  0.006251   \n",
       "\n",
       "           alm      hire      herf     bm_ia     me_ia  \n",
       "0     0.469687 -0.050260  0.553758  1.622149  0.017834  \n",
       "1     0.084011  0.215316  0.759126  0.318117  0.127617  \n",
       "2     0.144939 -0.035948  0.482831  2.015340  0.072533  \n",
       "3    -0.075192 -0.234043  0.114982  0.965078  0.001005  \n",
       "4     0.269721 -0.048576  0.223411  1.151220  0.039338  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "2238  0.683570 -0.010638  0.383695  1.508840  0.011199  \n",
       "2239  0.238843 -0.027287  0.964868  0.199428  2.990846  \n",
       "2240  0.178488 -0.050000  0.505660  1.127711  0.139232  \n",
       "2241  0.726728 -0.134865  0.242645  3.577947  0.009458  \n",
       "2242  0.607385  0.038462  0.383695  1.894915  0.117197  \n",
       "\n",
       "[2243 rows x 56 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2243, 56)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Keep only the labels between -1 and 3. \n",
    "Split the data into Train-Validation-Test:\n",
    "- Training data should contain features in 2018, do not forget to remove ‘label’\n",
    "- Training labels should only contain ‘label’ in 2018\n",
    "- Validation data should contain features in 2019, do not forget to remove ‘label’ • Validation labels should only contain ‘label’ in 2019\n",
    "- Test data should contain features in 2020, do not forget to remove ‘label’\n",
    "- Test labels should only contain ‘label’ in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter label between -1 and 3\n",
    "dat_1 = dat.loc[(dat['label'] >= -1) & (dat['label'] < 3)]\n",
    "\n",
    "# Training data\n",
    "train_feature = dat_1.loc[dat_1['fyear'] == 2018]\n",
    "train_label = train_feature['label']\n",
    "train_feature = train_feature.drop(columns=['label'])\n",
    "\n",
    "# validation data\n",
    "valid_feature = dat_1.loc[dat_1['fyear'] == 2019]\n",
    "valid_label = valid_feature['label']\n",
    "valid_feature = valid_feature.drop(columns=['label'])\n",
    "\n",
    "# test data\n",
    "test_feature = dat_1.loc[dat_1['fyear'] == 2020]\n",
    "test_label = test_feature['label']\n",
    "test_feature = test_feature.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute and report the prior probabilities $\\pi_j$ for all labels in the Training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "-1    0.070159\n",
       " 0    0.820393\n",
       " 1    0.088868\n",
       " 2    0.020580\n",
       "Name: prior, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label = pd.DataFrame(train_label).groupby('label').size()\n",
    "all_label = pd.DataFrame(all_label, columns=['count'], index=all_label.index)\n",
    "all_label['prior'] = all_label['count'] / sum(all_label['count'])\n",
    "\n",
    "pi = all_label['prior']\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using the Training set, calculate the likelihood for feature ‘ret’ to be 0.1 conditional on each value of the label $P_j = P(ret = 0.1|y = j)$ (Use the Normal PDF found in the following link: [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html])). Report the likelihood for each value of the label. You need to code this by hand, 0 points will be given if you use the pre-coded scipy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>0.1_likelihood</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.301106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.267751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.322697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.199889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean       var  0.1_likelihood\n",
       "label                                    \n",
       "-1     0.014262  0.013064        0.301106\n",
       " 0     0.010048  0.010146        0.267751\n",
       " 1     0.027497  0.012392        0.322697\n",
       " 2    -0.010655  0.008859        0.199889"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = dat_1.loc[dat_1['fyear'] == 2018]\n",
    "ret_mean = train.groupby(['label']).mean()['ret']\n",
    "ret_var = train.groupby(['label']).var()['ret']\n",
    "\n",
    "P = pd.DataFrame()\n",
    "P['mean'] = ret_mean\n",
    "P['var'] = ret_var\n",
    "P['0.1_likelihood'] = norm.pdf((0.1 - P['mean']) / np.sqrt(P['var']))\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use Guassian naive bayes from the scikit-learn library (found here:[scikitlearnfunction](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)) to classify the test data. Report the accuracy. You need to use the train+validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.2887\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(\n",
    "    pd.concat([train_feature, valid_feature], ignore_index=True), \n",
    "    pd.concat([train_label, valid_label], ignore_index=True)\n",
    "    ).predict(test_feature)\n",
    "\n",
    "test_accuracy = accuracy_score(test_label, y_pred)\n",
    "\n",
    "print(f'test accuracy is {round(test_accuracy, ndigits=4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute the confusion matrix (as shown in the lectures) and report the top 2 pairs with most (absolute number) incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  0,  0,  7],\n",
       "       [ 1, 22,  2, 52],\n",
       "       [ 0,  0,  1,  7],\n",
       "       [ 0,  0,  0,  4]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = confusion_matrix(test_label, y_pred, labels=[i for i in range(-1, 3)])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3], [0, 3]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = confusion_matrix(test_label, y_pred, labels=[i for i in range(-1, 3)])\n",
    "\n",
    "def find_n_max(matrix, n):\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        max_incorrect = 0\n",
    "        max_row = 0\n",
    "        max_col = 0\n",
    "\n",
    "        for row in range(len(matrix)):\n",
    "            \n",
    "            for column in range(len(matrix[0])):\n",
    "                \n",
    "                val = matrix[row, column]\n",
    "                \n",
    "                if (val > max_incorrect) & (row != column):\n",
    "\n",
    "                    max_incorrect = val\n",
    "                    max_row = row\n",
    "                    max_col = column\n",
    "                \n",
    "        matrix[max_row, max_col] = 0 # change the biggest value to 0 and loop again\n",
    "        pairs.append([max_row, max_col]) # row and column\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "find_n_max(mat, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data given by `pairs`, we know that the top 2 pairs with most incorrect are [2, 0] and [2, -1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Implement Gaussian Mixture model on the data as shown in class. \n",
    "Tune the covariance type parameter on the validation data. \n",
    "\n",
    "Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy. Train the model twice, the first model should use the covariance type that yielded the highest accuracy in the validation stage. The second model should use the covariance type that yielded the second highest accuracy in the validation stage. \n",
    "\n",
    "Comment on the accuracy on the test set of the the models you ran. \n",
    "\n",
    "Hint: Use ‘n components=3, init params=”kmeans”, random state=34’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for covariance type full = 0.6842105263157895\n",
      "Validation accuracy for covariance type tied = 0.09952153110047847\n",
      "Validation accuracy for covariance type diag = 0.02966507177033493\n",
      "Validation accuracy for covariance type spherical = 0.1799043062200957\n"
     ]
    }
   ],
   "source": [
    "for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "\n",
    "    clf = GaussianMixture(n_components=4, covariance_type=cov, init_params='kmeans', random_state=34)\n",
    "    clf.means_init = np.array([train_feature[train_label == i].mean(axis=0) for i in range(-1, 3)])\n",
    "    clf.fit(train_feature, train_label)\n",
    "    pred = clf.predict(valid_feature)\n",
    "\n",
    "    print ('Validation accuracy for covariance type '+ cov + ' = ' + str(accuracy_score(valid_label, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, we can see that the first model is `full` and the second is `tied`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for covariance type full is 0.1958762886597938\n",
      "Test accuracy for covariance type tied is 0.08247422680412371\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_feature, valid_feature], ignore_index=True)\n",
    "y = pd.concat([train_label, valid_label], ignore_index=True)\n",
    "\n",
    "# first model\n",
    "clf1 = GaussianMixture(n_components=4, covariance_type='full', init_params='kmeans', random_state=34)\n",
    "clf1.means_init = np.array([X[y == i].mean(axis=0) for i in range(-1, 3)])\n",
    "clf1.fit(X, y)\n",
    "pred1 = clf1.predict(test_feature)\n",
    "\n",
    "print(f'Test accuracy for covariance type full is {accuracy_score(test_label, pred1)}')\n",
    "\n",
    "# second model\n",
    "clf2 = GaussianMixture(n_components=4, covariance_type='tied', init_params='kmeans', random_state=34)\n",
    "clf2.means_init = np.array([X[y == i].mean(axis=0) for i in range(-1, 3)])\n",
    "clf2.fit(X, y)\n",
    "pred2 = clf2.predict(test_feature)\n",
    "\n",
    "print(f'Test accuracy for covariance type tied is {accuracy_score(test_label, pred2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the test accuracy decreases sharply after we rerun the models using train+vaild data. \n",
    "\n",
    "We should be awared that the dataset was separated into three sets with same length. And there can be some similar pattern within this year thus causing overfit. \n",
    "\n",
    "After we use two-year data as the training data, the model could perform very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Bonus Question: Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for discriminant analysis is 0.7731958762886598\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis(solver='eigen')\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(test_feature)\n",
    "print(f'Test accuracy for discriminant analysis is {accuracy_score(test_label, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.26680976e-03,  8.39045315e+03, -7.07258418e+02,\n",
       "         8.99626216e+00,  6.34848515e+02, -5.23096410e+03,\n",
       "         4.63106899e+03,  2.60911958e+02, -8.26841552e+01,\n",
       "        -5.85193945e+02,  2.09892141e+01, -1.27152966e+01,\n",
       "        -3.46604694e+02, -1.22320953e-02,  7.57087533e+01,\n",
       "         1.96856917e+01, -4.39056956e+01,  8.86628905e+02,\n",
       "        -1.54733704e+02, -3.38125142e+01,  5.89892746e+03,\n",
       "        -4.74527389e+01, -3.84712575e+01,  8.69048746e+02,\n",
       "         6.35946546e+02,  6.27384783e-05,  2.60933259e+01,\n",
       "         1.58321785e+02, -2.62289300e+03,  1.22177948e+03,\n",
       "         2.67774506e+03,  1.54828547e+01, -2.93670725e+02,\n",
       "         1.34123417e+01,  9.83080979e+01, -4.71810209e+01,\n",
       "         8.04493418e+00, -1.54480307e+01, -1.51340882e+00,\n",
       "        -5.76851123e+01,  9.60669302e+01,  4.19385694e-02,\n",
       "        -1.76722372e-01,  2.24674843e-01, -1.78178848e+02,\n",
       "         2.39731427e+02, -2.88929038e+01,  1.01686606e+02,\n",
       "        -4.34962272e+03,  1.71502384e+02, -9.82155462e+02,\n",
       "        -1.72892547e+03, -1.44038304e+02,  4.61419076e+02,\n",
       "         1.35381904e+00],\n",
       "       [ 2.26470177e-03,  8.39039905e+03, -7.08155718e+02,\n",
       "         9.12159682e+00,  6.37535459e+02, -5.22457640e+03,\n",
       "         4.62385274e+03,  2.55339064e+02, -8.26496316e+01,\n",
       "        -5.85341052e+02,  2.24144863e+01, -1.21442857e+01,\n",
       "        -3.44820130e+02, -1.23526662e-02,  7.49249277e+01,\n",
       "         1.92538463e+01, -4.38863708e+01,  8.86254648e+02,\n",
       "        -1.55284723e+02, -3.48363727e+01,  5.99390596e+03,\n",
       "        -4.71853246e+01, -3.87006186e+01,  8.69657868e+02,\n",
       "         6.35508612e+02,  6.64142363e-05,  2.37444824e+01,\n",
       "         1.58446205e+02, -2.62890432e+03,  1.22496355e+03,\n",
       "         2.68252693e+03,  1.55070951e+01, -2.93822163e+02,\n",
       "         1.34143634e+01,  9.80224708e+01, -4.71569934e+01,\n",
       "         8.01399580e+00, -1.54597264e+01, -1.53794065e+00,\n",
       "        -5.76272309e+01,  9.55338551e+01,  4.18169867e-02,\n",
       "        -1.77210213e-01,  2.24604501e-01, -1.78213021e+02,\n",
       "         2.40622446e+02, -2.93823630e+01,  1.02030186e+02,\n",
       "        -4.34529817e+03,  1.71341441e+02, -9.76307688e+02,\n",
       "        -1.72937856e+03, -1.43449268e+02,  4.60328117e+02,\n",
       "         1.38225554e+00],\n",
       "       [ 2.26579515e-03,  8.39041959e+03, -7.09284437e+02,\n",
       "         9.21092301e+00,  6.37409364e+02, -5.22345870e+03,\n",
       "         4.62197892e+03,  2.55663892e+02, -8.25039767e+01,\n",
       "        -5.86249688e+02,  2.51513373e+01, -1.18854670e+01,\n",
       "        -3.45139928e+02, -1.23765020e-02,  7.50844460e+01,\n",
       "         1.93565879e+01, -4.39775992e+01,  8.86691277e+02,\n",
       "        -1.55030243e+02, -3.49329437e+01,  5.98817263e+03,\n",
       "        -4.70089835e+01, -3.86534281e+01,  8.70597493e+02,\n",
       "         6.32100833e+02,  6.77636484e-05,  2.44234659e+01,\n",
       "         1.59686988e+02, -2.63011049e+03,  1.22466677e+03,\n",
       "         2.68012812e+03,  1.55141466e+01, -2.93816228e+02,\n",
       "         1.34156139e+01,  9.70756624e+01, -4.71998775e+01,\n",
       "         8.01689852e+00, -1.54654066e+01, -1.40848621e+00,\n",
       "        -5.71635076e+01,  9.49628853e+01,  4.18540721e-02,\n",
       "        -1.74210839e-01,  2.25047850e-01, -1.78337564e+02,\n",
       "         2.38748449e+02, -2.96503328e+01,  1.01802950e+02,\n",
       "        -4.34372703e+03,  1.71491794e+02, -9.77041614e+02,\n",
       "        -1.72984914e+03, -1.43125469e+02,  4.59951531e+02,\n",
       "         1.40268124e+00],\n",
       "       [ 2.27023878e-03,  8.39033660e+03, -7.10704452e+02,\n",
       "         8.86071965e+00,  6.37616089e+02, -5.20840913e+03,\n",
       "         4.60815595e+03,  2.55147951e+02, -8.23599581e+01,\n",
       "        -5.84728887e+02,  2.95834284e+01, -1.12828216e+01,\n",
       "        -3.44749687e+02, -1.23544749e-02,  7.44875998e+01,\n",
       "         1.85700630e+01, -4.37941175e+01,  8.86613384e+02,\n",
       "        -1.55333856e+02, -3.46327603e+01,  5.96561976e+03,\n",
       "        -4.71693891e+01, -3.87306546e+01,  8.66780199e+02,\n",
       "         6.39897378e+02,  5.98242476e-05,  2.44218150e+01,\n",
       "         1.59587712e+02, -2.62454200e+03,  1.22497486e+03,\n",
       "         2.67169617e+03,  1.55417855e+01, -2.93906671e+02,\n",
       "         1.34119682e+01,  1.00146818e+02, -4.72690012e+01,\n",
       "         8.00586406e+00, -1.54599860e+01, -1.42345994e+00,\n",
       "        -5.71134686e+01,  9.48546653e+01,  4.17884551e-02,\n",
       "        -1.67408717e-01,  2.24731828e-01, -1.78351039e+02,\n",
       "         2.38898090e+02, -3.13627318e+01,  1.02050283e+02,\n",
       "        -4.34711448e+03,  1.71355642e+02, -9.77358926e+02,\n",
       "        -1.72981993e+03, -1.43565851e+02,  4.59603465e+02,\n",
       "         1.36281267e+00]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8468247.51331814, -8468133.5827645 , -8468180.1162174 ,\n",
       "       -8468016.42805395])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple computation of mean for the features in each class\n",
    "mean_vectors = []\n",
    "for cl in range(-1, 3):\n",
    "    \n",
    "    mean_vectors.append(np.mean(X[y == cl], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within-class Scatter Matrix:\n",
      " [[ 3.37090280e+01  2.26961249e+00  2.08486945e+00 ... -2.87292510e+00\n",
      "  -3.06798859e+01 -1.92177013e+01]\n",
      " [ 2.26961249e+00  8.20671455e+02  7.08876880e+00 ... -1.85227247e+00\n",
      "   3.62100656e+01 -6.39508675e+01]\n",
      " [ 2.08486945e+00  7.08876880e+00  2.56472760e+02 ... -4.29710349e+00\n",
      "  -4.74012464e+01 -9.26123489e+01]\n",
      " ...\n",
      " [-2.87292510e+00 -1.85227247e+00 -4.29710349e+00 ...  1.62216372e+02\n",
      "   2.37906197e+01 -1.36474141e+02]\n",
      " [-3.06798859e+01  3.62100656e+01 -4.74012464e+01 ...  2.37906197e+01\n",
      "   1.46418988e+03 -1.07294057e+03]\n",
      " [-1.92177013e+01 -6.39508675e+01 -9.26123489e+01 ... -1.36474141e+02\n",
      "  -1.07294057e+03  3.90703411e+04]]\n"
     ]
    }
   ],
   "source": [
    "# within class scatter matrix S_W \n",
    "\n",
    "S_W = np.zeros((53, 53)) # within class and between class \n",
    "for cl, mv in zip(range(-1, 3), mean_vectors):\n",
    "\n",
    "    class_sc_mat = np.zeros((53, 53))                  # scatter matrix for every class\n",
    "\n",
    "    for row in np.array(X[y == cl]):\n",
    "\n",
    "        row, mv = row.reshape(53, 1), np.array(mv).reshape(53, 1) # make column vectors\n",
    "        class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "\n",
    "print('within-class Scatter Matrix:\\n', S_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between-class Scatter Matrix:\n",
      " [[ 1.60678891e-01  1.08073875e-01  1.04489939e-01 ... -9.76099627e-02\n",
      "   1.47898062e+00  1.08505095e+00]\n",
      " [ 1.08073875e-01  1.18967756e-01  1.72230009e-01 ...  1.27061446e-01\n",
      "  -1.03087175e+00  3.22165415e+00]\n",
      " [ 1.04489939e-01  1.72230009e-01  4.44357839e-01 ...  4.78574905e-01\n",
      "  -4.21202614e+00  7.76959298e+00]\n",
      " ...\n",
      " [-9.76099627e-02  1.27061446e-01  4.78574905e-01 ...  9.52773411e-01\n",
      "  -9.88454415e+00  1.09363717e+01]\n",
      " [ 1.47898062e+00 -1.03087175e+00 -4.21202614e+00 ... -9.88454415e+00\n",
      "   1.05613427e+02 -1.06462894e+02]\n",
      " [ 1.08505095e+00  3.22165415e+00  7.76959298e+00 ...  1.09363717e+01\n",
      "  -1.06462894e+02  1.57831711e+02]]\n"
     ]
    }
   ],
   "source": [
    "# between-class scatter matrix S_B\n",
    "\n",
    "overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "S_B = np.zeros((53, 53))\n",
    "for i, mean_vec in enumerate(mean_vectors):  \n",
    "\n",
    "    n = np.array(X[y == i - 1]).shape[0]\n",
    "    mean_vec = np.array(mean_vec).reshape(53, 1) # make column vector\n",
    "    overall_mean = np.array(overall_mean).reshape(53, 1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "print('between-class Scatter Matrix:\\n', S_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.30980460e-02+0.j, -1.87114021e-02+0.j],\n",
       "       [-1.51556576e-03+0.j, -1.47933131e-03+0.j],\n",
       "       [-2.96345734e-02+0.j, -2.29568808e-02+0.j],\n",
       "       [-1.11506276e-01+0.j,  4.98448069e-02+0.j],\n",
       "       [ 1.25225281e-01+0.j, -4.85266274e-02+0.j],\n",
       "       [ 6.08224241e-02+0.j,  4.64768781e-02+0.j],\n",
       "       [-1.97884662e-03+0.j,  2.81086455e-03+0.j],\n",
       "       [ 6.82809213e-03+0.j, -4.93035745e-03+0.j],\n",
       "       [-4.93522368e-02+0.j,  5.74399164e-02+0.j],\n",
       "       [-9.91386841e-03+0.j,  3.31033711e-03+0.j],\n",
       "       [-1.79712796e-02+0.j, -1.71002894e-02+0.j],\n",
       "       [ 1.52886473e-06+0.j,  7.05213261e-07+0.j],\n",
       "       [ 8.58350842e-03+0.j,  5.59601876e-03+0.j],\n",
       "       [ 5.49628505e-03+0.j,  6.74053740e-04+0.j],\n",
       "       [ 2.36901594e-04+0.j, -6.27171761e-04+0.j],\n",
       "       [ 4.21602112e-04+0.j,  9.87152123e-03+0.j],\n",
       "       [ 4.54243613e-03+0.j,  6.86732317e-03+0.j],\n",
       "       [ 1.18260316e-02+0.j,  8.56657543e-03+0.j],\n",
       "       [-9.74268625e-01+0.j, -9.80222225e-01+0.j],\n",
       "       [-4.28345799e-03+0.j, -3.44262562e-04+0.j],\n",
       "       [ 2.31666206e-03+0.j,  2.17154971e-03+0.j],\n",
       "       [-7.80269670e-03+0.j, -1.13597723e-02+0.j],\n",
       "       [ 2.00267257e-02+0.j, -7.29519345e-03+0.j],\n",
       "       [-3.79791653e-08+0.j, -5.45130204e-08+0.j],\n",
       "       [ 2.04131015e-02+0.j,  3.01970062e-02+0.j],\n",
       "       [-1.23240972e-02+0.j,  1.85320074e-02+0.j],\n",
       "       [ 6.76119941e-02+0.j,  5.91897374e-02+0.j],\n",
       "       [-3.39479199e-02+0.j, -2.91609724e-02+0.j],\n",
       "       [-1.58316926e-02+0.j, -1.23822283e-01+0.j],\n",
       "       [-3.92757678e-04+0.j,  6.47702323e-05+0.j],\n",
       "       [ 1.85998437e-03+0.j,  9.04843361e-04+0.j],\n",
       "       [-2.81424730e-05+0.j, -2.21579064e-05+0.j],\n",
       "       [ 5.57912391e-03+0.j,  3.81244839e-03+0.j],\n",
       "       [ 2.53994666e-04+0.j, -1.26525629e-03+0.j],\n",
       "       [ 3.44139690e-04+0.j,  2.37130149e-04+0.j],\n",
       "       [ 1.73238751e-04+0.j,  3.60900533e-05+0.j],\n",
       "       [-8.48592269e-04+0.j,  2.25981439e-03+0.j],\n",
       "       [-4.90361226e-03+0.j,  7.26409371e-03+0.j],\n",
       "       [ 1.13310764e-02+0.j, -5.49287835e-03+0.j],\n",
       "       [ 1.16772981e-06+0.j,  1.24496638e-06+0.j],\n",
       "       [-3.48898525e-05+0.j,  8.95992984e-05+0.j],\n",
       "       [-2.56279513e-06+0.j,  6.01460543e-06+0.j],\n",
       "       [ 1.53938621e-03+0.j, -1.80843779e-03+0.j],\n",
       "       [ 6.40838560e-03+0.j, -3.68867284e-02+0.j],\n",
       "       [ 1.13128600e-02+0.j, -9.89664168e-03+0.j],\n",
       "       [-2.33445510e-03+0.j, -5.07527171e-03+0.j],\n",
       "       [-5.60146062e-02+0.j, -3.09854587e-02+0.j],\n",
       "       [ 7.41411253e-04+0.j,  3.15911034e-03+0.j],\n",
       "       [-5.86232854e-02+0.j, -6.07412206e-02+0.j],\n",
       "       [ 9.39660496e-03+0.j, -3.56673013e-03+0.j],\n",
       "       [-8.69675648e-03+0.j, -2.47293196e-03+0.j],\n",
       "       [ 1.63551899e-02+0.j,  1.13721279e-03+0.j],\n",
       "       [-4.27016377e-04+0.j, -1.52202858e-04+0.j]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(53, 1)  \n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(53, 1)\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "W = np.hstack((eig_pairs[0][1].reshape(53, 1), eig_pairs[1][1].reshape(53, 1)))\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
