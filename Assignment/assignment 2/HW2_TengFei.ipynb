{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from scipy.stats import norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydotplus\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "The feature space is already lagged. Therefore, you do not need to lag the variables yourself. The data track these companies over 3 years (2018-2020). We will train the data in 2018, validate in 2019, and forecast 2020. Further instructions are given in the questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = pd.read_csv('hw2_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>permno</th>\n",
       "      <th>fyear</th>\n",
       "      <th>label</th>\n",
       "      <th>ret</th>\n",
       "      <th>acc</th>\n",
       "      <th>agr</th>\n",
       "      <th>cfp</th>\n",
       "      <th>ep</th>\n",
       "      <th>ni</th>\n",
       "      <th>op</th>\n",
       "      <th>...</th>\n",
       "      <th>grltnoa</th>\n",
       "      <th>conv</th>\n",
       "      <th>operprof</th>\n",
       "      <th>capxint</th>\n",
       "      <th>chpm</th>\n",
       "      <th>alm</th>\n",
       "      <th>hire</th>\n",
       "      <th>herf</th>\n",
       "      <th>bm_ia</th>\n",
       "      <th>me_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15417</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.252472</td>\n",
       "      <td>0.253973</td>\n",
       "      <td>0.148027</td>\n",
       "      <td>0.095118</td>\n",
       "      <td>0.323725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467402</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>-0.016902</td>\n",
       "      <td>0.469687</td>\n",
       "      <td>-0.050260</td>\n",
       "      <td>0.553758</td>\n",
       "      <td>1.622149</td>\n",
       "      <td>0.017834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89031</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.067922</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>0.215518</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.126233</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140084</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>-0.049337</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>0.215316</td>\n",
       "      <td>0.759126</td>\n",
       "      <td>0.318117</td>\n",
       "      <td>0.127617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11154</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.184267</td>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.155644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163587</td>\n",
       "      <td>0.091550</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>-0.035948</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>2.015340</td>\n",
       "      <td>0.072533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13556</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.326014</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>-0.095131</td>\n",
       "      <td>-1.258587</td>\n",
       "      <td>-1.340418</td>\n",
       "      <td>1.527239</td>\n",
       "      <td>-1.582117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.972048</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>1.954118</td>\n",
       "      <td>-0.075192</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>0.114982</td>\n",
       "      <td>0.965078</td>\n",
       "      <td>0.001005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14296</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.249443</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>0.519260</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>-0.183793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982905</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.821456</td>\n",
       "      <td>0.269721</td>\n",
       "      <td>-0.048576</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>1.151220</td>\n",
       "      <td>0.039338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2238</th>\n",
       "      <td>14532</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>-0.080498</td>\n",
       "      <td>-0.064369</td>\n",
       "      <td>-0.050658</td>\n",
       "      <td>0.064593</td>\n",
       "      <td>0.028561</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.213658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.226537</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.683570</td>\n",
       "      <td>-0.010638</td>\n",
       "      <td>0.383695</td>\n",
       "      <td>1.508840</td>\n",
       "      <td>0.011199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2239</th>\n",
       "      <td>41355</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.015946</td>\n",
       "      <td>0.043579</td>\n",
       "      <td>0.147297</td>\n",
       "      <td>0.082415</td>\n",
       "      <td>0.064142</td>\n",
       "      <td>-0.030160</td>\n",
       "      <td>0.383195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.402497</td>\n",
       "      <td>0.011861</td>\n",
       "      <td>0.031440</td>\n",
       "      <td>0.238843</td>\n",
       "      <td>-0.027287</td>\n",
       "      <td>0.964868</td>\n",
       "      <td>0.199428</td>\n",
       "      <td>2.990846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>13983</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.112274</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>-0.219388</td>\n",
       "      <td>-0.345708</td>\n",
       "      <td>-0.429181</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>0.196534</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.196021</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.116354</td>\n",
       "      <td>0.021174</td>\n",
       "      <td>-0.419594</td>\n",
       "      <td>0.178488</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.505660</td>\n",
       "      <td>1.127711</td>\n",
       "      <td>0.139232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2241</th>\n",
       "      <td>47619</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.057480</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>-0.033335</td>\n",
       "      <td>-0.010621</td>\n",
       "      <td>-0.123947</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.096157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.016991</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.093207</td>\n",
       "      <td>0.034593</td>\n",
       "      <td>-0.014232</td>\n",
       "      <td>0.726728</td>\n",
       "      <td>-0.134865</td>\n",
       "      <td>0.242645</td>\n",
       "      <td>3.577947</td>\n",
       "      <td>0.009458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2242</th>\n",
       "      <td>80362</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.057283</td>\n",
       "      <td>0.014109</td>\n",
       "      <td>0.062698</td>\n",
       "      <td>0.115561</td>\n",
       "      <td>0.070647</td>\n",
       "      <td>-0.007255</td>\n",
       "      <td>0.144202</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030756</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.152382</td>\n",
       "      <td>0.003596</td>\n",
       "      <td>0.006251</td>\n",
       "      <td>0.607385</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.383695</td>\n",
       "      <td>1.894915</td>\n",
       "      <td>0.117197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2243 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      permno   fyear  label       ret       acc       agr       cfp        ep  \\\n",
       "0      15417  2018.0      0  0.102609  0.053313  0.252472  0.253973  0.148027   \n",
       "1      89031  2018.0      1  0.067922 -0.001165  0.215518  0.005945  0.002326   \n",
       "2      11154  2018.0      0 -0.071429  0.000948  0.032913  0.184267  0.024266   \n",
       "3      13556  2018.0      0 -0.326014  0.045752 -0.095131 -1.258587 -1.340418   \n",
       "4      14296  2018.0      0  0.249443  0.016353 -0.020653  0.519260  0.449600   \n",
       "...      ...     ...    ...       ...       ...       ...       ...       ...   \n",
       "2238   14532  2020.0     -1 -0.080498 -0.064369 -0.050658  0.064593  0.028561   \n",
       "2239   41355  2020.0      0  0.015946  0.043579  0.147297  0.082415  0.064142   \n",
       "2240   13983  2020.0      0  0.112274  0.003504 -0.219388 -0.345708 -0.429181   \n",
       "2241   47619  2020.0      0 -0.057480  0.007940 -0.033335 -0.010621 -0.123947   \n",
       "2242   80362  2020.0      0  0.057283  0.014109  0.062698  0.115561  0.070647   \n",
       "\n",
       "            ni        op  ...   grltnoa  conv  operprof   capxint      chpm  \\\n",
       "0     0.095118  0.323725  ...  0.123393   0.0  0.467402  0.089973 -0.016902   \n",
       "1     0.012251  0.126233  ...  0.024414   0.0  0.140084  0.033813 -0.049337   \n",
       "2     0.008571  0.155644  ...  0.114552   1.0  0.163587  0.091550 -0.066428   \n",
       "3     1.527239 -1.582117  ...  0.117086   0.0 -2.972048  0.020384  1.954118   \n",
       "4     0.854538 -0.183793  ...  0.329045   0.0  0.982905  0.014257  0.821456   \n",
       "...        ...       ...  ...       ...   ...       ...       ...       ...   \n",
       "2238  0.000000 -0.213658  ...  0.000831   0.0 -0.226537  0.000677  0.000912   \n",
       "2239 -0.030160  0.383195  ...  0.002844   0.0  0.402497  0.011861  0.031440   \n",
       "2240  0.004651  0.196534  ... -0.196021   0.0  0.116354  0.021174 -0.419594   \n",
       "2241  0.000000  0.096157  ... -0.016991   0.0  0.093207  0.034593 -0.014232   \n",
       "2242 -0.007255  0.144202  ...  0.030756   0.0  0.152382  0.003596  0.006251   \n",
       "\n",
       "           alm      hire      herf     bm_ia     me_ia  \n",
       "0     0.469687 -0.050260  0.553758  1.622149  0.017834  \n",
       "1     0.084011  0.215316  0.759126  0.318117  0.127617  \n",
       "2     0.144939 -0.035948  0.482831  2.015340  0.072533  \n",
       "3    -0.075192 -0.234043  0.114982  0.965078  0.001005  \n",
       "4     0.269721 -0.048576  0.223411  1.151220  0.039338  \n",
       "...        ...       ...       ...       ...       ...  \n",
       "2238  0.683570 -0.010638  0.383695  1.508840  0.011199  \n",
       "2239  0.238843 -0.027287  0.964868  0.199428  2.990846  \n",
       "2240  0.178488 -0.050000  0.505660  1.127711  0.139232  \n",
       "2241  0.726728 -0.134865  0.242645  3.577947  0.009458  \n",
       "2242  0.607385  0.038462  0.383695  1.894915  0.117197  \n",
       "\n",
       "[2243 rows x 56 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Keep only the labels between -1 and 3. \n",
    "Split the data into Train-Validation-Test:\n",
    "- Training data should contain features in 2018, do not forget to remove ‘label’\n",
    "- Training labels should only contain ‘label’ in 2018\n",
    "- Validation data should contain features in 2019, do not forget to remove ‘label’ • Validation labels should only contain ‘label’ in 2019\n",
    "- Test data should contain features in 2020, do not forget to remove ‘label’\n",
    "- Test labels should only contain ‘label’ in 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter label between -1 and 3\n",
    "dat_1 = dat.loc[(dat['label'] >= -1) & (dat['label'] < 3)]\n",
    "\n",
    "# Training data\n",
    "train_feature = dat_1.loc[dat_1['fyear'] == 2018]\n",
    "train_label = train_feature['label']\n",
    "train_feature = train_feature.drop(columns=['label', 'permno', 'fyear'])\n",
    "\n",
    "# validation data\n",
    "valid_feature = dat_1.loc[dat_1['fyear'] == 2019]\n",
    "valid_label = valid_feature['label']\n",
    "valid_feature = valid_feature.drop(columns=['label', 'permno', 'fyear'])\n",
    "\n",
    "# test data\n",
    "test_feature = dat_1.loc[dat_1['fyear'] == 2020]\n",
    "test_label = test_feature['label']\n",
    "test_feature = test_feature.drop(columns=['label', 'permno', 'fyear'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Compute and report the prior probabilities $\\pi_j$ for all labels in the Training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "-1    0.070159\n",
       " 0    0.820393\n",
       " 1    0.088868\n",
       " 2    0.020580\n",
       "Name: prior, dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_label = pd.DataFrame(train_label).groupby('label').size()\n",
    "all_label = pd.DataFrame(all_label, columns=['count'], index=all_label.index)\n",
    "all_label['prior'] = all_label['count'] / sum(all_label['count'])\n",
    "\n",
    "pi = all_label['prior']\n",
    "pi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Using the Training set, calculate the likelihood for feature ‘ret’ to be 0.1 conditional on each value of the label $P_j = P(ret = 0.1|y = j)$ (Use the Normal PDF found in the following link: [Scipy](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html])). Report the likelihood for each value of the label. You need to code this by hand, 0 points will be given if you use the pre-coded scipy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ret</th>\n",
       "      <th>acc</th>\n",
       "      <th>agr</th>\n",
       "      <th>cfp</th>\n",
       "      <th>ep</th>\n",
       "      <th>ni</th>\n",
       "      <th>op</th>\n",
       "      <th>rsup</th>\n",
       "      <th>cash</th>\n",
       "      <th>chcsho</th>\n",
       "      <th>...</th>\n",
       "      <th>grltnoa</th>\n",
       "      <th>conv</th>\n",
       "      <th>operprof</th>\n",
       "      <th>capxint</th>\n",
       "      <th>chpm</th>\n",
       "      <th>alm</th>\n",
       "      <th>hire</th>\n",
       "      <th>herf</th>\n",
       "      <th>bm_ia</th>\n",
       "      <th>me_ia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.102609</td>\n",
       "      <td>0.053313</td>\n",
       "      <td>0.252472</td>\n",
       "      <td>0.253973</td>\n",
       "      <td>0.148027</td>\n",
       "      <td>0.095118</td>\n",
       "      <td>0.323725</td>\n",
       "      <td>0.907821</td>\n",
       "      <td>0.170630</td>\n",
       "      <td>0.099789</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123393</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.467402</td>\n",
       "      <td>0.089973</td>\n",
       "      <td>-0.016902</td>\n",
       "      <td>0.469687</td>\n",
       "      <td>-0.050260</td>\n",
       "      <td>0.553758</td>\n",
       "      <td>1.622149</td>\n",
       "      <td>0.017834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.067922</td>\n",
       "      <td>-0.001165</td>\n",
       "      <td>0.215518</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.002326</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.126233</td>\n",
       "      <td>0.033752</td>\n",
       "      <td>0.242426</td>\n",
       "      <td>0.012327</td>\n",
       "      <td>...</td>\n",
       "      <td>0.024414</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.140084</td>\n",
       "      <td>0.033813</td>\n",
       "      <td>-0.049337</td>\n",
       "      <td>0.084011</td>\n",
       "      <td>0.215316</td>\n",
       "      <td>0.759126</td>\n",
       "      <td>0.318117</td>\n",
       "      <td>0.127617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.071429</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.032913</td>\n",
       "      <td>0.184267</td>\n",
       "      <td>0.024266</td>\n",
       "      <td>0.008571</td>\n",
       "      <td>0.155644</td>\n",
       "      <td>-0.060579</td>\n",
       "      <td>0.301746</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114552</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.163587</td>\n",
       "      <td>0.091550</td>\n",
       "      <td>-0.066428</td>\n",
       "      <td>0.144939</td>\n",
       "      <td>-0.035948</td>\n",
       "      <td>0.482831</td>\n",
       "      <td>2.015340</td>\n",
       "      <td>0.072533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.326014</td>\n",
       "      <td>0.045752</td>\n",
       "      <td>-0.095131</td>\n",
       "      <td>-1.258587</td>\n",
       "      <td>-1.340418</td>\n",
       "      <td>1.527239</td>\n",
       "      <td>-1.582117</td>\n",
       "      <td>0.020306</td>\n",
       "      <td>0.050378</td>\n",
       "      <td>-0.539456</td>\n",
       "      <td>...</td>\n",
       "      <td>0.117086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2.972048</td>\n",
       "      <td>0.020384</td>\n",
       "      <td>1.954118</td>\n",
       "      <td>-0.075192</td>\n",
       "      <td>-0.234043</td>\n",
       "      <td>0.114982</td>\n",
       "      <td>0.965078</td>\n",
       "      <td>0.001005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.249443</td>\n",
       "      <td>0.016353</td>\n",
       "      <td>-0.020653</td>\n",
       "      <td>0.519260</td>\n",
       "      <td>0.449600</td>\n",
       "      <td>0.854538</td>\n",
       "      <td>-0.183793</td>\n",
       "      <td>0.033630</td>\n",
       "      <td>0.022905</td>\n",
       "      <td>-0.764971</td>\n",
       "      <td>...</td>\n",
       "      <td>0.329045</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.982905</td>\n",
       "      <td>0.014257</td>\n",
       "      <td>0.821456</td>\n",
       "      <td>0.269721</td>\n",
       "      <td>-0.048576</td>\n",
       "      <td>0.223411</td>\n",
       "      <td>1.151220</td>\n",
       "      <td>0.039338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1080</th>\n",
       "      <td>0.054105</td>\n",
       "      <td>-0.001807</td>\n",
       "      <td>0.089017</td>\n",
       "      <td>0.107095</td>\n",
       "      <td>0.041666</td>\n",
       "      <td>0.013848</td>\n",
       "      <td>0.142874</td>\n",
       "      <td>0.040472</td>\n",
       "      <td>0.012117</td>\n",
       "      <td>0.013944</td>\n",
       "      <td>...</td>\n",
       "      <td>0.109585</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.201967</td>\n",
       "      <td>0.032356</td>\n",
       "      <td>0.036884</td>\n",
       "      <td>0.011584</td>\n",
       "      <td>0.148718</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1081</th>\n",
       "      <td>0.079388</td>\n",
       "      <td>0.000527</td>\n",
       "      <td>0.015823</td>\n",
       "      <td>0.047288</td>\n",
       "      <td>0.031791</td>\n",
       "      <td>0.008216</td>\n",
       "      <td>0.201411</td>\n",
       "      <td>0.014678</td>\n",
       "      <td>0.041941</td>\n",
       "      <td>0.008250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.051059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.257229</td>\n",
       "      <td>0.028789</td>\n",
       "      <td>0.050255</td>\n",
       "      <td>0.096617</td>\n",
       "      <td>0.099057</td>\n",
       "      <td>0.100118</td>\n",
       "      <td>0.677892</td>\n",
       "      <td>0.613586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1082</th>\n",
       "      <td>-0.148534</td>\n",
       "      <td>-0.010133</td>\n",
       "      <td>-0.236462</td>\n",
       "      <td>-1.358676</td>\n",
       "      <td>-1.638366</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.039809</td>\n",
       "      <td>-0.200483</td>\n",
       "      <td>0.046370</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.145097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.029605</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>-0.869378</td>\n",
       "      <td>-0.559189</td>\n",
       "      <td>-0.231169</td>\n",
       "      <td>0.100118</td>\n",
       "      <td>10.746082</td>\n",
       "      <td>0.002548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1083</th>\n",
       "      <td>0.056180</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.120499</td>\n",
       "      <td>0.080167</td>\n",
       "      <td>-0.018595</td>\n",
       "      <td>0.011872</td>\n",
       "      <td>0.108467</td>\n",
       "      <td>-0.441751</td>\n",
       "      <td>0.069532</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002094</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.112288</td>\n",
       "      <td>0.002194</td>\n",
       "      <td>0.082869</td>\n",
       "      <td>0.483436</td>\n",
       "      <td>-0.086294</td>\n",
       "      <td>0.091165</td>\n",
       "      <td>2.094959</td>\n",
       "      <td>0.032947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1084</th>\n",
       "      <td>-0.003247</td>\n",
       "      <td>-0.019258</td>\n",
       "      <td>-0.003814</td>\n",
       "      <td>-0.090586</td>\n",
       "      <td>-0.199959</td>\n",
       "      <td>0.005494</td>\n",
       "      <td>0.096817</td>\n",
       "      <td>1.109166</td>\n",
       "      <td>0.117443</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058704</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.123943</td>\n",
       "      <td>0.013193</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.360488</td>\n",
       "      <td>-0.080000</td>\n",
       "      <td>0.121375</td>\n",
       "      <td>1.920705</td>\n",
       "      <td>0.426664</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1069 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           ret       acc       agr       cfp        ep        ni        op  \\\n",
       "0     0.102609  0.053313  0.252472  0.253973  0.148027  0.095118  0.323725   \n",
       "1     0.067922 -0.001165  0.215518  0.005945  0.002326  0.012251  0.126233   \n",
       "2    -0.071429  0.000948  0.032913  0.184267  0.024266  0.008571  0.155644   \n",
       "3    -0.326014  0.045752 -0.095131 -1.258587 -1.340418  1.527239 -1.582117   \n",
       "4     0.249443  0.016353 -0.020653  0.519260  0.449600  0.854538 -0.183793   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "1080  0.054105 -0.001807  0.089017  0.107095  0.041666  0.013848  0.142874   \n",
       "1081  0.079388  0.000527  0.015823  0.047288  0.031791  0.008216  0.201411   \n",
       "1082 -0.148534 -0.010133 -0.236462 -1.358676 -1.638366  0.000000 -0.039809   \n",
       "1083  0.056180 -0.000269 -0.120499  0.080167 -0.018595  0.011872  0.108467   \n",
       "1084 -0.003247 -0.019258 -0.003814 -0.090586 -0.199959  0.005494  0.096817   \n",
       "\n",
       "          rsup      cash    chcsho  ...   grltnoa  conv  operprof   capxint  \\\n",
       "0     0.907821  0.170630  0.099789  ...  0.123393   0.0  0.467402  0.089973   \n",
       "1     0.033752  0.242426  0.012327  ...  0.024414   0.0  0.140084  0.033813   \n",
       "2    -0.060579  0.301746  0.008608  ...  0.114552   1.0  0.163587  0.091550   \n",
       "3     0.020306  0.050378 -0.539456  ...  0.117086   0.0 -2.972048  0.020384   \n",
       "4     0.033630  0.022905 -0.764971  ...  0.329045   0.0  0.982905  0.014257   \n",
       "...        ...       ...       ...  ...       ...   ...       ...       ...   \n",
       "1080  0.040472  0.012117  0.013944  ...  0.109585   0.0  0.201967  0.032356   \n",
       "1081  0.014678  0.041941  0.008250  ...  0.051059   0.0  0.257229  0.028789   \n",
       "1082 -0.200483  0.046370  0.000000  ... -0.145097   0.0 -0.029605  0.001979   \n",
       "1083 -0.441751  0.069532  0.011943  ... -0.002094   0.0  0.112288  0.002194   \n",
       "1084  1.109166  0.117443  0.005509  ...  0.058704   0.0  0.123943  0.013193   \n",
       "\n",
       "          chpm       alm      hire      herf      bm_ia     me_ia  \n",
       "0    -0.016902  0.469687 -0.050260  0.553758   1.622149  0.017834  \n",
       "1    -0.049337  0.084011  0.215316  0.759126   0.318117  0.127617  \n",
       "2    -0.066428  0.144939 -0.035948  0.482831   2.015340  0.072533  \n",
       "3     1.954118 -0.075192 -0.234043  0.114982   0.965078  0.001005  \n",
       "4     0.821456  0.269721 -0.048576  0.223411   1.151220  0.039338  \n",
       "...        ...       ...       ...       ...        ...       ...  \n",
       "1080  0.036884  0.011584  0.148718  1.000000   1.000000  1.000000  \n",
       "1081  0.050255  0.096617  0.099057  0.100118   0.677892  0.613586  \n",
       "1082 -0.869378 -0.559189 -0.231169  0.100118  10.746082  0.002548  \n",
       "1083  0.082869  0.483436 -0.086294  0.091165   2.094959  0.032947  \n",
       "1084  0.002711  0.360488 -0.080000  0.121375   1.920705  0.426664  \n",
       "\n",
       "[1069 rows x 53 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>var</th>\n",
       "      <th>likelihood</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>-1</th>\n",
       "      <td>0.014262</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.301106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.010048</td>\n",
       "      <td>0.010146</td>\n",
       "      <td>0.267751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.027497</td>\n",
       "      <td>0.012392</td>\n",
       "      <td>0.322697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.010655</td>\n",
       "      <td>0.008859</td>\n",
       "      <td>0.199889</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           mean       var  likelihood\n",
       "label                                \n",
       "-1     0.014262  0.013064    0.301106\n",
       " 0     0.010048  0.010146    0.267751\n",
       " 1     0.027497  0.012392    0.322697\n",
       " 2    -0.010655  0.008859    0.199889"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = dat_1.loc[dat_1['fyear'] == 2018]\n",
    "ret_mean = train.groupby(['label']).mean()['ret']\n",
    "ret_var = train.groupby(['label']).var()['ret']\n",
    "\n",
    "P = pd.DataFrame()\n",
    "P['mean'] = ret_mean\n",
    "P['var'] = ret_var\n",
    "P['likelihood'] = norm.pdf((0.1 - P['mean']) / np.sqrt(P['var']))\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Use Guassian naive bayes from the scikit-learn library (found here:[scikitlearnfunction](https://scikit-learn.org/stable/modules/naive_bayes.html#gaussian-naive-bayes)) to classify the test data. Report the accuracy. You need to use the train+validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy is 0.2784\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(\n",
    "    pd.concat([train_feature, valid_feature], ignore_index=True), \n",
    "    pd.concat([train_label, valid_label], ignore_index=True)\n",
    "    ).predict(test_feature)\n",
    "\n",
    "test_accuracy = accuracy_score(test_label, y_pred)\n",
    "\n",
    "print(f'test accuracy is {round(test_accuracy, ndigits=4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Compute the confusion matrix (as shown in the lectures) and report the top 2 pairs with most (absolute number) incorrect classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 11,  24,   3,  37],\n",
       "       [ 16, 215,  29, 599],\n",
       "       [  0,  10,  19,  59],\n",
       "       [  0,   0,   4,  19]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = confusion_matrix(valid_label, y_pred, labels=[i for i in range(-1, 3)])\n",
    "mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 3], [2, 3]]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = confusion_matrix(valid_label, y_pred, labels=[i for i in range(-1, 3)])\n",
    "\n",
    "def find_n_max(matrix, n):\n",
    "\n",
    "    pairs = []\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        max_incorrect = 0\n",
    "        max_row = 0\n",
    "        max_col = 0\n",
    "\n",
    "        for row in range(len(matrix)):\n",
    "            \n",
    "            for column in range(len(matrix[0])):\n",
    "                \n",
    "                val = matrix[row, column]\n",
    "                \n",
    "                if (val > max_incorrect) & (row != column):\n",
    "\n",
    "                    max_incorrect = val\n",
    "                    max_row = row\n",
    "                    max_col = column\n",
    "                \n",
    "        matrix[max_row, max_col] = 0 # change the biggest value to 0 and loop again\n",
    "        pairs.append([max_row, max_col]) # row and column\n",
    "        \n",
    "    return pairs\n",
    "\n",
    "find_n_max(mat, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data given by `pairs`, we know that the top 2 pairs with most incorrect are [2, 0] and [2, 1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Implement Gaussian Mixture model on the data as shown in class. \n",
    "Tune the covariance type parameter on the validation data. \n",
    "\n",
    "Use the selected value to compute the test accuracy. As always, train the model on train+validation data to compute the test accuracy. Train the model twice, the first model should use the covariance type that yielded the highest accuracy in the validation stage. The second model should use the covariance type that yielded the second highest accuracy in the validation stage. \n",
    "\n",
    "Comment on the accuracy on the test set of the the models you ran. \n",
    "\n",
    "Hint: Use ‘n components=3, init params=”kmeans”, random state=34’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for covariance type full = 0.8220095693779904\n",
      "Validation accuracy for covariance type tied = 0.7655502392344498\n",
      "Validation accuracy for covariance type diag = 0.15502392344497606\n",
      "Validation accuracy for covariance type spherical = 0.4296650717703349\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for cov in ['full', 'tied', 'diag', 'spherical']:\n",
    "\n",
    "    clf = GaussianMixture(n_components=4, covariance_type=cov, init_params='kmeans', random_state=34)\n",
    "    clf.means_init = np.array([train_feature[train_label == i].mean(axis=0) for i in range(-1, 3)])\n",
    "    clf.fit(train_feature, train_label)\n",
    "    pred = clf.predict(valid_feature)\n",
    "\n",
    "    print ('Validation accuracy for covariance type '+ cov + ' = ' + str(accuracy_score(valid_label, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above results, we can see that the first model is `full` and the second is `tied`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for covariance type full is 0.13402061855670103\n",
      "Test accuracy for covariance type tied is 0.7628865979381443\n"
     ]
    }
   ],
   "source": [
    "X = pd.concat([train_feature, valid_feature], ignore_index=True)\n",
    "y = pd.concat([train_label, valid_label], ignore_index=True)\n",
    "\n",
    "# first model\n",
    "clf1 = GaussianMixture(n_components=4, covariance_type='full', init_params='kmeans', random_state=34)\n",
    "clf1.means_init = np.array([X[y == i].mean(axis=0) for i in range(-1, 3)])\n",
    "clf1.fit(X, y)\n",
    "pred1 = clf1.predict(test_feature)\n",
    "\n",
    "print(f'Test accuracy for covariance type full is {accuracy_score(test_label, pred1)}')\n",
    "\n",
    "# second model\n",
    "clf2 = GaussianMixture(n_components=4, covariance_type='tied', init_params='kmeans', random_state=34)\n",
    "clf2.means_init = np.array([X[y == i].mean(axis=0) for i in range(-1, 3)])\n",
    "clf2.fit(X, y)\n",
    "pred2 = clf2.predict(test_feature)\n",
    "\n",
    "print(f'Test accuracy for covariance type tied is {accuracy_score(test_label, pred2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the test accuracy decreases sharply after we rerun the models using train+vaild data. \n",
    "\n",
    "We should be awared that the dataset was separated into three sets with same length. And there can be some similar pattern within this year thus causing overfit. \n",
    "\n",
    "After we use two-year data as the training data, the model could perform very bad."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Bonus Question: Apply Linear Discriminant Analysis model on the train+validation data and report the accuracy obtained on test data. Report the transformation matrix (w) along with the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy for discriminant analysis is 0.7731958762886598\n"
     ]
    }
   ],
   "source": [
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X, y)\n",
    "y_pred = clf.predict(test_feature)\n",
    "print(f'Test accuracy for discriminant analysis is {accuracy_score(test_label, y_pred)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple computation of mean for the features in each class\n",
    "mean_vectors = []\n",
    "for cl in range(-1, 3):\n",
    "    \n",
    "    mean_vectors.append(np.mean(X[y == cl], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "within-class Scatter Matrix:\n",
      " [[ 3.37090280e+01  2.26961249e+00  2.08486945e+00 ... -2.87292510e+00\n",
      "  -3.06798859e+01 -1.92177013e+01]\n",
      " [ 2.26961249e+00  8.20671455e+02  7.08876880e+00 ... -1.85227247e+00\n",
      "   3.62100656e+01 -6.39508675e+01]\n",
      " [ 2.08486945e+00  7.08876880e+00  2.56472760e+02 ... -4.29710349e+00\n",
      "  -4.74012464e+01 -9.26123489e+01]\n",
      " ...\n",
      " [-2.87292510e+00 -1.85227247e+00 -4.29710349e+00 ...  1.62216372e+02\n",
      "   2.37906197e+01 -1.36474141e+02]\n",
      " [-3.06798859e+01  3.62100656e+01 -4.74012464e+01 ...  2.37906197e+01\n",
      "   1.46418988e+03 -1.07294057e+03]\n",
      " [-1.92177013e+01 -6.39508675e+01 -9.26123489e+01 ... -1.36474141e+02\n",
      "  -1.07294057e+03  3.90703411e+04]]\n"
     ]
    }
   ],
   "source": [
    "# within class scatter matrix S_W \n",
    "\n",
    "S_W = np.zeros((53, 53)) # within class and between class \n",
    "for cl, mv in zip(range(-1, 3), mean_vectors):\n",
    "\n",
    "    class_sc_mat = np.zeros((53, 53))                  # scatter matrix for every class\n",
    "\n",
    "    for row in np.array(X[y == cl]):\n",
    "\n",
    "        row, mv = row.reshape(53, 1), np.array(mv).reshape(53, 1) # make column vectors\n",
    "        class_sc_mat += (row - mv).dot((row - mv).T)\n",
    "\n",
    "    S_W += class_sc_mat                             # sum class scatter matrices\n",
    "\n",
    "print('within-class Scatter Matrix:\\n', S_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "between-class Scatter Matrix:\n",
      " [[ 1.60678891e-01  1.08073875e-01  1.04489939e-01 ... -9.76099627e-02\n",
      "   1.47898062e+00  1.08505095e+00]\n",
      " [ 1.08073875e-01  1.18967756e-01  1.72230009e-01 ...  1.27061446e-01\n",
      "  -1.03087175e+00  3.22165415e+00]\n",
      " [ 1.04489939e-01  1.72230009e-01  4.44357839e-01 ...  4.78574905e-01\n",
      "  -4.21202614e+00  7.76959298e+00]\n",
      " ...\n",
      " [-9.76099627e-02  1.27061446e-01  4.78574905e-01 ...  9.52773411e-01\n",
      "  -9.88454415e+00  1.09363717e+01]\n",
      " [ 1.47898062e+00 -1.03087175e+00 -4.21202614e+00 ... -9.88454415e+00\n",
      "   1.05613427e+02 -1.06462894e+02]\n",
      " [ 1.08505095e+00  3.22165415e+00  7.76959298e+00 ...  1.09363717e+01\n",
      "  -1.06462894e+02  1.57831711e+02]]\n"
     ]
    }
   ],
   "source": [
    "# between-class scatter matrix S_B\n",
    "\n",
    "overall_mean = np.mean(X, axis=0)\n",
    "\n",
    "S_B = np.zeros((53, 53))\n",
    "for i, mean_vec in enumerate(mean_vectors):  \n",
    "\n",
    "    n = np.array(X[y == i - 1]).shape[0]\n",
    "    mean_vec = np.array(mean_vec).reshape(53, 1) # make column vector\n",
    "    overall_mean = np.array(overall_mean).reshape(53, 1) # make column vector\n",
    "    S_B += n * (mean_vec - overall_mean).dot((mean_vec - overall_mean).T)\n",
    "\n",
    "print('between-class Scatter Matrix:\\n', S_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.30980460e-02+0.j, -1.87114021e-02+0.j],\n",
       "       [-1.51556576e-03+0.j, -1.47933131e-03+0.j],\n",
       "       [-2.96345734e-02+0.j, -2.29568808e-02+0.j],\n",
       "       [-1.11506276e-01+0.j,  4.98448069e-02+0.j],\n",
       "       [ 1.25225281e-01+0.j, -4.85266274e-02+0.j],\n",
       "       [ 6.08224241e-02+0.j,  4.64768781e-02+0.j],\n",
       "       [-1.97884662e-03+0.j,  2.81086455e-03+0.j],\n",
       "       [ 6.82809213e-03+0.j, -4.93035745e-03+0.j],\n",
       "       [-4.93522368e-02+0.j,  5.74399164e-02+0.j],\n",
       "       [-9.91386841e-03+0.j,  3.31033711e-03+0.j],\n",
       "       [-1.79712796e-02+0.j, -1.71002894e-02+0.j],\n",
       "       [ 1.52886473e-06+0.j,  7.05213261e-07+0.j],\n",
       "       [ 8.58350842e-03+0.j,  5.59601876e-03+0.j],\n",
       "       [ 5.49628505e-03+0.j,  6.74053740e-04+0.j],\n",
       "       [ 2.36901594e-04+0.j, -6.27171761e-04+0.j],\n",
       "       [ 4.21602112e-04+0.j,  9.87152123e-03+0.j],\n",
       "       [ 4.54243613e-03+0.j,  6.86732317e-03+0.j],\n",
       "       [ 1.18260316e-02+0.j,  8.56657543e-03+0.j],\n",
       "       [-9.74268625e-01+0.j, -9.80222225e-01+0.j],\n",
       "       [-4.28345799e-03+0.j, -3.44262562e-04+0.j],\n",
       "       [ 2.31666206e-03+0.j,  2.17154971e-03+0.j],\n",
       "       [-7.80269670e-03+0.j, -1.13597723e-02+0.j],\n",
       "       [ 2.00267257e-02+0.j, -7.29519345e-03+0.j],\n",
       "       [-3.79791653e-08+0.j, -5.45130204e-08+0.j],\n",
       "       [ 2.04131015e-02+0.j,  3.01970062e-02+0.j],\n",
       "       [-1.23240972e-02+0.j,  1.85320074e-02+0.j],\n",
       "       [ 6.76119941e-02+0.j,  5.91897374e-02+0.j],\n",
       "       [-3.39479199e-02+0.j, -2.91609724e-02+0.j],\n",
       "       [-1.58316926e-02+0.j, -1.23822283e-01+0.j],\n",
       "       [-3.92757678e-04+0.j,  6.47702323e-05+0.j],\n",
       "       [ 1.85998437e-03+0.j,  9.04843361e-04+0.j],\n",
       "       [-2.81424730e-05+0.j, -2.21579064e-05+0.j],\n",
       "       [ 5.57912391e-03+0.j,  3.81244839e-03+0.j],\n",
       "       [ 2.53994666e-04+0.j, -1.26525629e-03+0.j],\n",
       "       [ 3.44139690e-04+0.j,  2.37130149e-04+0.j],\n",
       "       [ 1.73238751e-04+0.j,  3.60900533e-05+0.j],\n",
       "       [-8.48592269e-04+0.j,  2.25981439e-03+0.j],\n",
       "       [-4.90361226e-03+0.j,  7.26409371e-03+0.j],\n",
       "       [ 1.13310764e-02+0.j, -5.49287835e-03+0.j],\n",
       "       [ 1.16772981e-06+0.j,  1.24496638e-06+0.j],\n",
       "       [-3.48898525e-05+0.j,  8.95992984e-05+0.j],\n",
       "       [-2.56279513e-06+0.j,  6.01460543e-06+0.j],\n",
       "       [ 1.53938621e-03+0.j, -1.80843779e-03+0.j],\n",
       "       [ 6.40838560e-03+0.j, -3.68867284e-02+0.j],\n",
       "       [ 1.13128600e-02+0.j, -9.89664168e-03+0.j],\n",
       "       [-2.33445510e-03+0.j, -5.07527171e-03+0.j],\n",
       "       [-5.60146062e-02+0.j, -3.09854587e-02+0.j],\n",
       "       [ 7.41411253e-04+0.j,  3.15911034e-03+0.j],\n",
       "       [-5.86232854e-02+0.j, -6.07412206e-02+0.j],\n",
       "       [ 9.39660496e-03+0.j, -3.56673013e-03+0.j],\n",
       "       [-8.69675648e-03+0.j, -2.47293196e-03+0.j],\n",
       "       [ 1.63551899e-02+0.j,  1.13721279e-03+0.j],\n",
       "       [-4.27016377e-04+0.j, -1.52202858e-04+0.j]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eig_vals, eig_vecs = np.linalg.eig(np.linalg.inv(S_W).dot(S_B))\n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigvec_sc = eig_vecs[:,i].reshape(53, 1)  \n",
    "\n",
    "for i in range(len(eig_vals)):\n",
    "    eigv = eig_vecs[:,i].reshape(53, 1)\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs = sorted(eig_pairs, key=lambda k: k[0], reverse=True)\n",
    "\n",
    "W = np.hstack((eig_pairs[0][1].reshape(53, 1), eig_pairs[1][1].reshape(53, 1)))\n",
    "\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
