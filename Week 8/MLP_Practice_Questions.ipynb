{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLTgmm2fjIbp"
      },
      "source": [
        "# Multi-Layer Perceptron - Excess Return Forecasts - Practice Question Solutions\n",
        "\n",
        "**Objectives**\n",
        "\n",
        "* Train Feed-Forward Networks\n",
        "  * Early Stopping\n",
        "  * Activation Functions\n",
        "  * Loss Functions\n",
        "  * Optimizers\n",
        "* Forecast Excess Returns\n",
        "* Test the newest model proposed by academia\n",
        "\n",
        "As we have done during the hands on session we are going to use the Goyal Welch dataset combined wuth Fred-MD to predict the SP500 excess returns.\n",
        "\n",
        "Here is the task:\n",
        "* Divide the dataset into three parts:\n",
        "  * Train: 1959-03-31 : 2018-12-31\n",
        "  * Validation : 2019-01-31 : 2019-12-31\n",
        "  * Test : 2020-01-31 - 2020-12-31\n",
        "\n",
        "* MinMaxScaler(feature_range=(-1,1)):\n",
        "  * Fit and transform the train data using fit transform\n",
        "  * Transform the validation set using information until the train data\n",
        "  * Fit and transorm the train + validation set\n",
        "  * Tranform the test set using the information available until 2019-31-31\n",
        "\n",
        "* Build a 4 Layer Neural Net to test the latest academia tested model\n",
        "  * 500 Epochs\n",
        "  * Use Huber loss\n",
        "  * Use the ReLU activation function\n",
        "  * Use the Adam optimizer\n",
        "  * First layer 68 neurons\n",
        "  * Second layer 34 neurons\n",
        "  * Third layer 12 neurons\n",
        "  * Fourth layer 6 neurons\n",
        "  * Run the above structure twice:\n",
        "    * No early stopping\n",
        "    * With early stopping (patience=200)\n",
        "  * Show the in sample evaluation metrics for both models\n",
        "  * Show the out of sample evaluation metrics for both models\n",
        "  * Graph the forecasts produced against the actual.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP__utrhjH4g"
      },
      "source": [
        "## Data Import - Goyal Welch Excess Return Predictors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqcvNtDowuiO",
        "outputId": "e2ee6997-6e11-4acb-f92c-68c1d5f21687"
      },
      "outputs": [],
      "source": [
        "# !gdown --id 1OMWYlG7JXHN-PeBvQ1UBmfmlIuZP80fO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pffIaIyYXAuK"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "rz8YSTDgW7rG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Normalization\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "GmvSymg-w1yi"
      },
      "outputs": [],
      "source": [
        "df_1 = pd.read_csv('merged_data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "LWJPKnOAxI4y"
      },
      "outputs": [],
      "source": [
        "df_1.set_index('Date',inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRQot0XsXZd0"
      },
      "source": [
        "## Data Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "grvVKc__xyWC"
      },
      "outputs": [],
      "source": [
        "y = df_1['excess_returns']\n",
        "X = df_1.drop('excess_returns', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1kgAKueZ2Bl",
        "outputId": "c4f8cbaf-f67f-4d58-d4a6-44a112b26d13"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "1959-03-01    0.001152\n",
              "1959-04-01    0.038054\n",
              "1959-05-01    0.021431\n",
              "1959-06-01   -0.003225\n",
              "1959-07-01    0.032004\n",
              "Name: excess_returns, dtype: float64"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "0tjxwnA3aIRl",
        "outputId": "4fd2d0a9-95c8-4b25-d05d-838f1bcb3693"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>RPI</th>\n",
              "      <th>W875RX1</th>\n",
              "      <th>DPCERA3M086SBEA</th>\n",
              "      <th>CMRMTSPLx</th>\n",
              "      <th>RETAILx</th>\n",
              "      <th>INDPRO</th>\n",
              "      <th>IPFPNSS</th>\n",
              "      <th>IPFINAL</th>\n",
              "      <th>IPCONGD</th>\n",
              "      <th>...</th>\n",
              "      <th>DTCOLNVHFNM</th>\n",
              "      <th>DTCTHFNM</th>\n",
              "      <th>INVEST</th>\n",
              "      <th>D12</th>\n",
              "      <th>E12</th>\n",
              "      <th>b/m</th>\n",
              "      <th>lty</th>\n",
              "      <th>ntis</th>\n",
              "      <th>ltr</th>\n",
              "      <th>svar</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1959-03-01</th>\n",
              "      <td>-0.000690</td>\n",
              "      <td>0.006430</td>\n",
              "      <td>0.007359</td>\n",
              "      <td>0.009410</td>\n",
              "      <td>-0.003423</td>\n",
              "      <td>0.008321</td>\n",
              "      <td>0.014303</td>\n",
              "      <td>0.006036</td>\n",
              "      <td>0.004899</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.004929</td>\n",
              "      <td>0.004138</td>\n",
              "      <td>-0.014792</td>\n",
              "      <td>1.77000</td>\n",
              "      <td>3.11000</td>\n",
              "      <td>0.516860</td>\n",
              "      <td>0.0403</td>\n",
              "      <td>0.024984</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.000492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959-04-01</th>\n",
              "      <td>0.001380</td>\n",
              "      <td>0.006494</td>\n",
              "      <td>0.007049</td>\n",
              "      <td>-0.003639</td>\n",
              "      <td>0.019922</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.021077</td>\n",
              "      <td>0.014338</td>\n",
              "      <td>0.014542</td>\n",
              "      <td>0.015653</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012134</td>\n",
              "      <td>0.006734</td>\n",
              "      <td>0.024929</td>\n",
              "      <td>1.77667</td>\n",
              "      <td>3.20667</td>\n",
              "      <td>0.498597</td>\n",
              "      <td>0.0414</td>\n",
              "      <td>0.024361</td>\n",
              "      <td>-0.0117</td>\n",
              "      <td>0.000493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959-05-01</th>\n",
              "      <td>0.001723</td>\n",
              "      <td>0.005763</td>\n",
              "      <td>0.006616</td>\n",
              "      <td>0.012005</td>\n",
              "      <td>0.006797</td>\n",
              "      <td>0.007803</td>\n",
              "      <td>0.014950</td>\n",
              "      <td>0.008269</td>\n",
              "      <td>0.009583</td>\n",
              "      <td>0.004768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002828</td>\n",
              "      <td>0.002020</td>\n",
              "      <td>-0.015342</td>\n",
              "      <td>1.78333</td>\n",
              "      <td>3.30333</td>\n",
              "      <td>0.483077</td>\n",
              "      <td>0.0417</td>\n",
              "      <td>0.025887</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>0.000428</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959-06-01</th>\n",
              "      <td>0.000339</td>\n",
              "      <td>0.003102</td>\n",
              "      <td>0.002974</td>\n",
              "      <td>0.003708</td>\n",
              "      <td>-0.000051</td>\n",
              "      <td>0.009064</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>0.007126</td>\n",
              "      <td>-0.004768</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009726</td>\n",
              "      <td>0.009007</td>\n",
              "      <td>-0.012252</td>\n",
              "      <td>1.79000</td>\n",
              "      <td>3.40000</td>\n",
              "      <td>0.483219</td>\n",
              "      <td>0.0419</td>\n",
              "      <td>0.026882</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.000915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1959-07-01</th>\n",
              "      <td>-0.001034</td>\n",
              "      <td>-0.000589</td>\n",
              "      <td>-0.000764</td>\n",
              "      <td>-0.003427</td>\n",
              "      <td>0.012110</td>\n",
              "      <td>-0.000330</td>\n",
              "      <td>-0.024238</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.008247</td>\n",
              "      <td>0.013056</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004631</td>\n",
              "      <td>-0.001000</td>\n",
              "      <td>0.029341</td>\n",
              "      <td>1.79667</td>\n",
              "      <td>3.41000</td>\n",
              "      <td>0.460823</td>\n",
              "      <td>0.0417</td>\n",
              "      <td>0.027141</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.000488</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 119 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            CPIAUCSL       RPI   W875RX1  DPCERA3M086SBEA  CMRMTSPLx  \\\n",
              "Date                                                                   \n",
              "1959-03-01 -0.000690  0.006430  0.007359         0.009410  -0.003423   \n",
              "1959-04-01  0.001380  0.006494  0.007049        -0.003639   0.019922   \n",
              "1959-05-01  0.001723  0.005763  0.006616         0.012005   0.006797   \n",
              "1959-06-01  0.000339  0.003102  0.002974         0.003708  -0.000051   \n",
              "1959-07-01 -0.001034 -0.000589 -0.000764        -0.003427   0.012110   \n",
              "\n",
              "             RETAILx    INDPRO   IPFPNSS   IPFINAL   IPCONGD  ...  \\\n",
              "Date                                                          ...   \n",
              "1959-03-01  0.008321  0.014303  0.006036  0.004899  0.000000  ...   \n",
              "1959-04-01  0.000616  0.021077  0.014338  0.014542  0.015653  ...   \n",
              "1959-05-01  0.007803  0.014950  0.008269  0.009583  0.004768  ...   \n",
              "1959-06-01  0.009064  0.001144  0.007034  0.007126 -0.004768  ...   \n",
              "1959-07-01 -0.000330 -0.024238  0.001167  0.008247  0.013056  ...   \n",
              "\n",
              "            DTCOLNVHFNM  DTCTHFNM    INVEST      D12      E12       b/m  \\\n",
              "Date                                                                      \n",
              "1959-03-01     0.004929  0.004138 -0.014792  1.77000  3.11000  0.516860   \n",
              "1959-04-01     0.012134  0.006734  0.024929  1.77667  3.20667  0.498597   \n",
              "1959-05-01     0.002828  0.002020 -0.015342  1.78333  3.30333  0.483077   \n",
              "1959-06-01     0.009726  0.009007 -0.012252  1.79000  3.40000  0.483219   \n",
              "1959-07-01    -0.004631 -0.001000  0.029341  1.79667  3.41000  0.460823   \n",
              "\n",
              "               lty      ntis     ltr      svar  \n",
              "Date                                            \n",
              "1959-03-01  0.0403  0.024984  0.0017  0.000492  \n",
              "1959-04-01  0.0414  0.024361 -0.0117  0.000493  \n",
              "1959-05-01  0.0417  0.025887 -0.0005  0.000428  \n",
              "1959-06-01  0.0419  0.026882  0.0010  0.000915  \n",
              "1959-07-01  0.0417  0.027141  0.0060  0.000488  \n",
              "\n",
              "[5 rows x 119 columns]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHvnTo2lc0lP"
      },
      "source": [
        "Generating the splits- We are going to use Min Max Scaler and use a very specific strategy in scaling our variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrT76zi-yLkL",
        "outputId": "ec98684d-7042-47ce-9da8-2f386b52b80a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Date\n",
              "1959-03-01    0.001152\n",
              "1959-04-01    0.038054\n",
              "1959-05-01    0.021431\n",
              "1959-06-01   -0.003225\n",
              "1959-07-01    0.032004\n",
              "                ...   \n",
              "2020-08-01    0.071968\n",
              "2020-09-01   -0.038251\n",
              "2020-10-01   -0.026509\n",
              "2020-11-01    0.109303\n",
              "2020-12-01    0.041473\n",
              "Name: excess_returns, Length: 742, dtype: float64"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "UJiR0Q4Sdtas"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "TnPqZZsFciDd"
      },
      "outputs": [],
      "source": [
        "X_train =scaler.fit_transform(X.loc[:'2018'])\n",
        "X_valid = scaler.transform(X.loc['2019-01-01':'2019-12-01'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "vNPl4H9LeZr1"
      },
      "outputs": [],
      "source": [
        "X_test_0 = scaler.fit_transform(X.loc[:'2019'])\n",
        "X_test = scaler.transform(X.loc['2020-01-01':'2020-12-01'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "lEH4s6Zxi_CO"
      },
      "outputs": [],
      "source": [
        "y_train = y.loc[:'2018']\n",
        "y_valid = y.loc['2019-01-01':'2019-12-01']\n",
        "y_test = y.loc['2020-01-01':'2020-12-01']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kic7InGhhfx"
      },
      "source": [
        "# Building the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOCX_j0ohmZ6"
      },
      "source": [
        "## Some Universal Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "S2uGloeEdTVV"
      },
      "outputs": [],
      "source": [
        "# Set the Patience Level of Early Stopping\n",
        "# Iterations increase with patience\n",
        "patience = 200\n",
        "\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=patience)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "c326Z_YHh5jn"
      },
      "outputs": [],
      "source": [
        "### Select Activation Function by Uncommenting ###\n",
        "\n",
        "act_fn = 'relu'      # Rectified Linear Unit\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "bTxRvCOqiaaF"
      },
      "outputs": [],
      "source": [
        "### Select Loss Function by Uncommenting ###\n",
        "\n",
        "loss = 'huber' #Huber Loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AOIbYUszie57"
      },
      "outputs": [],
      "source": [
        "### Select Optimizer by Uncommenting ###\n",
        "\n",
        "optimizer = 'adam' # Adam\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_KPLfwLePw-q"
      },
      "source": [
        "## Neural Net with 4 Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            " 9/23 [==========>...................] - ETA: 0s - loss: 0.0214"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 19:57:36.457142: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 1s 18ms/step - loss: 0.0095 - val_loss: 0.0013\n",
            "Epoch 2/500\n",
            "19/23 [=======================>......] - ETA: 0s - loss: 9.3223e-04"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 19:57:36.936636: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 6ms/step - loss: 9.1874e-04 - val_loss: 9.2142e-04\n",
            "Epoch 3/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8599e-04 - val_loss: 8.4571e-04\n",
            "Epoch 4/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.6973e-04 - val_loss: 8.8308e-04\n",
            "Epoch 5/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5328e-04 - val_loss: 9.0829e-04\n",
            "Epoch 6/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.4459e-04 - val_loss: 9.2950e-04\n",
            "Epoch 7/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3466e-04 - val_loss: 9.1441e-04\n",
            "Epoch 8/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1156e-04 - val_loss: 9.6432e-04\n",
            "Epoch 9/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0427e-04 - val_loss: 8.9615e-04\n",
            "Epoch 10/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9622e-04 - val_loss: 9.5304e-04\n",
            "Epoch 11/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9069e-04 - val_loss: 8.9883e-04\n",
            "Epoch 12/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8409e-04 - val_loss: 9.3505e-04\n",
            "Epoch 13/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7959e-04 - val_loss: 9.5664e-04\n",
            "Epoch 14/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7294e-04 - val_loss: 9.2273e-04\n",
            "Epoch 15/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6330e-04 - val_loss: 9.6805e-04\n",
            "Epoch 16/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5596e-04 - val_loss: 9.1711e-04\n",
            "Epoch 17/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4441e-04 - val_loss: 9.2064e-04\n",
            "Epoch 18/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3983e-04 - val_loss: 7.7484e-04\n",
            "Epoch 19/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.2537e-04 - val_loss: 8.5990e-04\n",
            "Epoch 20/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1465e-04 - val_loss: 0.0011\n",
            "Epoch 21/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9536e-04 - val_loss: 9.3963e-04\n",
            "Epoch 22/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7993e-04 - val_loss: 0.0011\n",
            "Epoch 23/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.6098e-04 - val_loss: 0.0012\n",
            "Epoch 24/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4317e-04 - val_loss: 9.2781e-04\n",
            "Epoch 25/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2986e-04 - val_loss: 0.0017\n",
            "Epoch 26/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3005e-04 - val_loss: 0.0016\n",
            "Epoch 27/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2520e-04 - val_loss: 0.0010\n",
            "Epoch 28/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8771e-04 - val_loss: 0.0013\n",
            "Epoch 29/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6106e-04 - val_loss: 0.0011\n",
            "Epoch 30/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2709e-04 - val_loss: 0.0015\n",
            "Epoch 31/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8963e-04 - val_loss: 0.0014\n",
            "Epoch 32/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6116e-04 - val_loss: 0.0012\n",
            "Epoch 33/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2832e-04 - val_loss: 0.0012\n",
            "Epoch 34/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9417e-04 - val_loss: 0.0013\n",
            "Epoch 35/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7211e-04 - val_loss: 0.0017\n",
            "Epoch 36/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7525e-04 - val_loss: 0.0016\n",
            "Epoch 37/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4131e-04 - val_loss: 0.0022\n",
            "Epoch 38/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1520e-04 - val_loss: 0.0019\n",
            "Epoch 39/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0563e-04 - val_loss: 0.0015\n",
            "Epoch 40/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7390e-04 - val_loss: 0.0018\n",
            "Epoch 41/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.5597e-04 - val_loss: 0.0016\n",
            "Epoch 42/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.6798e-04 - val_loss: 0.0014\n",
            "Epoch 43/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5249e-04 - val_loss: 0.0014\n",
            "Epoch 44/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3198e-04 - val_loss: 0.0014\n",
            "Epoch 45/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2859e-04 - val_loss: 0.0012\n",
            "Epoch 46/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0413e-04 - val_loss: 0.0014\n",
            "Epoch 47/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9775e-04 - val_loss: 0.0015\n",
            "Epoch 48/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1493e-04 - val_loss: 0.0013\n",
            "Epoch 49/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0741e-04 - val_loss: 0.0011\n",
            "Epoch 50/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7128e-04 - val_loss: 0.0016\n",
            "Epoch 51/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6075e-04 - val_loss: 0.0012\n",
            "Epoch 52/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.9463e-04 - val_loss: 0.0015\n",
            "Epoch 53/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7444e-04 - val_loss: 0.0014\n",
            "Epoch 54/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8288e-04 - val_loss: 0.0015\n",
            "Epoch 55/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6580e-04 - val_loss: 0.0012\n",
            "Epoch 56/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.4639e-04 - val_loss: 0.0015\n",
            "Epoch 57/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3308e-04 - val_loss: 0.0014\n",
            "Epoch 58/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1991e-04 - val_loss: 0.0012\n",
            "Epoch 59/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.2022e-04 - val_loss: 0.0013\n",
            "Epoch 60/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0602e-04 - val_loss: 0.0014\n",
            "Epoch 61/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0124e-04 - val_loss: 0.0017\n",
            "Epoch 62/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 9.6783e-05 - val_loss: 0.0016\n",
            "Epoch 63/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.9313e-05 - val_loss: 0.0015\n",
            "Epoch 64/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.5901e-05 - val_loss: 0.0015\n",
            "Epoch 65/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.5014e-05 - val_loss: 0.0018\n",
            "Epoch 66/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1441e-05 - val_loss: 0.0019\n",
            "Epoch 67/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4293e-05 - val_loss: 0.0017\n",
            "Epoch 68/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.5622e-05 - val_loss: 0.0018\n",
            "Epoch 69/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0436e-05 - val_loss: 0.0017\n",
            "Epoch 70/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1061e-05 - val_loss: 0.0017\n",
            "Epoch 71/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4063e-05 - val_loss: 0.0019\n",
            "Epoch 72/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2185e-05 - val_loss: 0.0019\n",
            "Epoch 73/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9593e-05 - val_loss: 0.0021\n",
            "Epoch 74/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4728e-04 - val_loss: 0.0023\n",
            "Epoch 75/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.2017e-05 - val_loss: 0.0018\n",
            "Epoch 76/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.2382e-05 - val_loss: 0.0018\n",
            "Epoch 77/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.8512e-05 - val_loss: 0.0015\n",
            "Epoch 78/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0242e-04 - val_loss: 0.0015\n",
            "Epoch 79/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0405e-04 - val_loss: 0.0014\n",
            "Epoch 80/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6416e-05 - val_loss: 0.0018\n",
            "Epoch 81/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7532e-05 - val_loss: 0.0014\n",
            "Epoch 82/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7228e-05 - val_loss: 0.0016\n",
            "Epoch 83/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7099e-05 - val_loss: 0.0017\n",
            "Epoch 84/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6874e-05 - val_loss: 0.0016\n",
            "Epoch 85/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0176e-05 - val_loss: 0.0015\n",
            "Epoch 86/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3506e-05 - val_loss: 0.0017\n",
            "Epoch 87/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2981e-05 - val_loss: 0.0015\n",
            "Epoch 88/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9311e-05 - val_loss: 0.0015\n",
            "Epoch 89/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5594e-05 - val_loss: 0.0018\n",
            "Epoch 90/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2399e-05 - val_loss: 0.0015\n",
            "Epoch 91/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3767e-05 - val_loss: 0.0016\n",
            "Epoch 92/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8950e-05 - val_loss: 0.0015\n",
            "Epoch 93/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9145e-05 - val_loss: 0.0017\n",
            "Epoch 94/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7982e-05 - val_loss: 0.0018\n",
            "Epoch 95/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3243e-05 - val_loss: 0.0015\n",
            "Epoch 96/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9224e-05 - val_loss: 0.0017\n",
            "Epoch 97/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0633e-05 - val_loss: 0.0016\n",
            "Epoch 98/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7687e-05 - val_loss: 0.0017\n",
            "Epoch 99/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9563e-05 - val_loss: 0.0016\n",
            "Epoch 100/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.0236e-05 - val_loss: 0.0017\n",
            "Epoch 101/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6146e-05 - val_loss: 0.0017\n",
            "Epoch 102/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4013e-05 - val_loss: 0.0016\n",
            "Epoch 103/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0213e-05 - val_loss: 0.0016\n",
            "Epoch 104/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0425e-05 - val_loss: 0.0016\n",
            "Epoch 105/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4443e-05 - val_loss: 0.0016\n",
            "Epoch 106/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1857e-05 - val_loss: 0.0015\n",
            "Epoch 107/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7049e-05 - val_loss: 0.0017\n",
            "Epoch 108/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5845e-05 - val_loss: 0.0016\n",
            "Epoch 109/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4785e-05 - val_loss: 0.0017\n",
            "Epoch 110/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3847e-05 - val_loss: 0.0015\n",
            "Epoch 111/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4962e-05 - val_loss: 0.0015\n",
            "Epoch 112/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4597e-05 - val_loss: 0.0017\n",
            "Epoch 113/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4923e-05 - val_loss: 0.0018\n",
            "Epoch 114/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6944e-05 - val_loss: 0.0016\n",
            "Epoch 115/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3363e-05 - val_loss: 0.0017\n",
            "Epoch 116/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3352e-05 - val_loss: 0.0016\n",
            "Epoch 117/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.5111e-05 - val_loss: 0.0017\n",
            "Epoch 118/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9356e-05 - val_loss: 0.0017\n",
            "Epoch 119/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8608e-05 - val_loss: 0.0016\n",
            "Epoch 120/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6378e-05 - val_loss: 0.0015\n",
            "Epoch 121/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8316e-05 - val_loss: 0.0015\n",
            "Epoch 122/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.5150e-05 - val_loss: 0.0015\n",
            "Epoch 123/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2205e-05 - val_loss: 0.0016\n",
            "Epoch 124/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5570e-05 - val_loss: 0.0015\n",
            "Epoch 125/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8570e-05 - val_loss: 0.0013\n",
            "Epoch 126/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5903e-05 - val_loss: 0.0016\n",
            "Epoch 127/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7063e-05 - val_loss: 0.0019\n",
            "Epoch 128/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.5107e-05 - val_loss: 0.0014\n",
            "Epoch 129/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8220e-05 - val_loss: 0.0015\n",
            "Epoch 130/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2477e-05 - val_loss: 0.0015\n",
            "Epoch 131/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6471e-05 - val_loss: 0.0014\n",
            "Epoch 132/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7273e-05 - val_loss: 0.0016\n",
            "Epoch 133/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7327e-05 - val_loss: 0.0014\n",
            "Epoch 134/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4503e-05 - val_loss: 0.0015\n",
            "Epoch 135/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2468e-05 - val_loss: 0.0015\n",
            "Epoch 136/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 9.7251e-06 - val_loss: 0.0015\n",
            "Epoch 137/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8152e-06 - val_loss: 0.0015\n",
            "Epoch 138/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7241e-06 - val_loss: 0.0015\n",
            "Epoch 139/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4605e-06 - val_loss: 0.0015\n",
            "Epoch 140/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7615e-06 - val_loss: 0.0015\n",
            "Epoch 141/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2473e-06 - val_loss: 0.0015\n",
            "Epoch 142/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7654e-06 - val_loss: 0.0015\n",
            "Epoch 143/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.7061e-06 - val_loss: 0.0015\n",
            "Epoch 144/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6230e-06 - val_loss: 0.0014\n",
            "Epoch 145/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3635e-06 - val_loss: 0.0016\n",
            "Epoch 146/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9645e-06 - val_loss: 0.0015\n",
            "Epoch 147/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2216e-05 - val_loss: 0.0013\n",
            "Epoch 148/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1387e-05 - val_loss: 0.0014\n",
            "Epoch 149/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0596e-05 - val_loss: 0.0015\n",
            "Epoch 150/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9625e-06 - val_loss: 0.0014\n",
            "Epoch 151/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2140e-05 - val_loss: 0.0015\n",
            "Epoch 152/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5911e-05 - val_loss: 0.0013\n",
            "Epoch 153/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1227e-05 - val_loss: 0.0015\n",
            "Epoch 154/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1144e-05 - val_loss: 0.0014\n",
            "Epoch 155/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9473e-06 - val_loss: 0.0015\n",
            "Epoch 156/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.6106e-06 - val_loss: 0.0014\n",
            "Epoch 157/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1498e-05 - val_loss: 0.0013\n",
            "Epoch 158/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2143e-05 - val_loss: 0.0016\n",
            "Epoch 159/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1095e-05 - val_loss: 0.0014\n",
            "Epoch 160/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9986e-06 - val_loss: 0.0015\n",
            "Epoch 161/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8437e-05 - val_loss: 0.0014\n",
            "Epoch 162/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9384e-05 - val_loss: 0.0013\n",
            "Epoch 163/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5802e-05 - val_loss: 0.0016\n",
            "Epoch 164/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0635e-05 - val_loss: 0.0015\n",
            "Epoch 165/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3794e-05 - val_loss: 0.0014\n",
            "Epoch 166/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6059e-05 - val_loss: 0.0012\n",
            "Epoch 167/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0957e-05 - val_loss: 0.0013\n",
            "Epoch 168/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4726e-05 - val_loss: 0.0013\n",
            "Epoch 169/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5863e-05 - val_loss: 0.0014\n",
            "Epoch 170/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8349e-05 - val_loss: 0.0015\n",
            "Epoch 171/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6455e-05 - val_loss: 0.0014\n",
            "Epoch 172/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5545e-05 - val_loss: 0.0013\n",
            "Epoch 173/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6098e-05 - val_loss: 0.0012\n",
            "Epoch 174/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4996e-05 - val_loss: 0.0016\n",
            "Epoch 175/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5861e-05 - val_loss: 0.0016\n",
            "Epoch 176/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7024e-05 - val_loss: 0.0015\n",
            "Epoch 177/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1180e-05 - val_loss: 0.0015\n",
            "Epoch 178/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1191e-04 - val_loss: 0.0014\n",
            "Epoch 179/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3609e-04 - val_loss: 0.0016\n",
            "Epoch 180/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.7252e-05 - val_loss: 0.0012\n",
            "Epoch 181/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0685e-05 - val_loss: 0.0017\n",
            "Epoch 182/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7022e-05 - val_loss: 0.0015\n",
            "Epoch 183/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0517e-05 - val_loss: 0.0014\n",
            "Epoch 184/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.5479e-05 - val_loss: 0.0012\n",
            "Epoch 185/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9875e-05 - val_loss: 0.0013\n",
            "Epoch 186/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4984e-05 - val_loss: 0.0012\n",
            "Epoch 187/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5540e-05 - val_loss: 0.0013\n",
            "Epoch 188/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5474e-05 - val_loss: 0.0013\n",
            "Epoch 189/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1646e-05 - val_loss: 0.0013\n",
            "Epoch 190/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.5358e-06 - val_loss: 0.0013\n",
            "Epoch 191/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1565e-06 - val_loss: 0.0013\n",
            "Epoch 192/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8012e-06 - val_loss: 0.0013\n",
            "Epoch 193/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1291e-06 - val_loss: 0.0014\n",
            "Epoch 194/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3025e-06 - val_loss: 0.0013\n",
            "Epoch 195/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.0320e-06 - val_loss: 0.0013\n",
            "Epoch 196/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0555e-06 - val_loss: 0.0013\n",
            "Epoch 197/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4487e-06 - val_loss: 0.0013\n",
            "Epoch 198/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4578e-06 - val_loss: 0.0013\n",
            "Epoch 199/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9480e-06 - val_loss: 0.0013\n",
            "Epoch 200/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.7226e-06 - val_loss: 0.0013\n",
            "Epoch 201/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8611e-06 - val_loss: 0.0013\n",
            "Epoch 202/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6304e-06 - val_loss: 0.0014\n",
            "Epoch 203/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2454e-06 - val_loss: 0.0013\n",
            "Epoch 204/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4613e-06 - val_loss: 0.0013\n",
            "Epoch 205/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0797e-06 - val_loss: 0.0013\n",
            "Epoch 206/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0811e-06 - val_loss: 0.0013\n",
            "Epoch 207/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1059e-06 - val_loss: 0.0013\n",
            "Epoch 208/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3230e-06 - val_loss: 0.0013\n",
            "Epoch 209/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2307e-06 - val_loss: 0.0013\n",
            "Epoch 210/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7313e-06 - val_loss: 0.0014\n",
            "Epoch 211/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2380e-06 - val_loss: 0.0014\n",
            "Epoch 212/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1302e-06 - val_loss: 0.0013\n",
            "Epoch 213/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.5185e-06 - val_loss: 0.0013\n",
            "Epoch 214/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4733e-06 - val_loss: 0.0014\n",
            "Epoch 215/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2802e-06 - val_loss: 0.0013\n",
            "Epoch 216/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6397e-06 - val_loss: 0.0013\n",
            "Epoch 217/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2670e-06 - val_loss: 0.0013\n",
            "Epoch 218/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8565e-06 - val_loss: 0.0014\n",
            "Epoch 219/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0945e-06 - val_loss: 0.0013\n",
            "Epoch 220/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1897e-06 - val_loss: 0.0014\n",
            "Epoch 221/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0550e-06 - val_loss: 0.0013\n",
            "Epoch 222/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6758e-05 - val_loss: 0.0014\n",
            "Epoch 223/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5860e-05 - val_loss: 0.0015\n",
            "Epoch 224/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2955e-05 - val_loss: 0.0013\n",
            "Epoch 225/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8896e-05 - val_loss: 0.0015\n",
            "Epoch 226/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2592e-05 - val_loss: 0.0014\n",
            "Epoch 227/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1904e-05 - val_loss: 0.0012\n",
            "Epoch 228/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7240e-05 - val_loss: 0.0017\n",
            "Epoch 229/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6136e-05 - val_loss: 0.0013\n",
            "Epoch 230/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4875e-05 - val_loss: 0.0015\n",
            "Epoch 231/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2943e-05 - val_loss: 0.0014\n",
            "Epoch 232/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7264e-05 - val_loss: 0.0012\n",
            "Epoch 233/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5845e-05 - val_loss: 0.0013\n",
            "Epoch 234/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0710e-05 - val_loss: 0.0012\n",
            "Epoch 235/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5019e-05 - val_loss: 0.0013\n",
            "Epoch 236/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0256e-05 - val_loss: 0.0011\n",
            "Epoch 237/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1109e-05 - val_loss: 0.0011\n",
            "Epoch 238/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7645e-05 - val_loss: 0.0013\n",
            "Epoch 239/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2031e-05 - val_loss: 0.0012\n",
            "Epoch 240/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9768e-06 - val_loss: 0.0013\n",
            "Epoch 241/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9031e-06 - val_loss: 0.0013\n",
            "Epoch 242/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.1379e-06 - val_loss: 0.0012\n",
            "Epoch 243/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3714e-06 - val_loss: 0.0013\n",
            "Epoch 244/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4671e-06 - val_loss: 0.0013\n",
            "Epoch 245/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8036e-06 - val_loss: 0.0012\n",
            "Epoch 246/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2890e-06 - val_loss: 0.0013\n",
            "Epoch 247/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4011e-06 - val_loss: 0.0012\n",
            "Epoch 248/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8262e-06 - val_loss: 0.0012\n",
            "Epoch 249/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7960e-06 - val_loss: 0.0013\n",
            "Epoch 250/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0339e-05 - val_loss: 0.0012\n",
            "Epoch 251/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2578e-05 - val_loss: 0.0012\n",
            "Epoch 252/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7834e-05 - val_loss: 0.0013\n",
            "Epoch 253/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1012e-05 - val_loss: 0.0012\n",
            "Epoch 254/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6261e-05 - val_loss: 0.0011\n",
            "Epoch 255/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3138e-05 - val_loss: 0.0012\n",
            "Epoch 256/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8212e-05 - val_loss: 0.0012\n",
            "Epoch 257/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4529e-05 - val_loss: 0.0012\n",
            "Epoch 258/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9063e-05 - val_loss: 0.0011\n",
            "Epoch 259/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8741e-05 - val_loss: 0.0011\n",
            "Epoch 260/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5314e-05 - val_loss: 0.0012\n",
            "Epoch 261/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3547e-05 - val_loss: 0.0013\n",
            "Epoch 262/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8942e-05 - val_loss: 0.0012\n",
            "Epoch 263/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5641e-05 - val_loss: 0.0011\n",
            "Epoch 264/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6597e-05 - val_loss: 0.0013\n",
            "Epoch 265/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7705e-05 - val_loss: 0.0010\n",
            "Epoch 266/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3937e-05 - val_loss: 0.0013\n",
            "Epoch 267/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4787e-05 - val_loss: 0.0011\n",
            "Epoch 268/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7964e-05 - val_loss: 0.0013\n",
            "Epoch 269/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0797e-05 - val_loss: 0.0010\n",
            "Epoch 270/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5254e-05 - val_loss: 0.0013\n",
            "Epoch 271/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1680e-05 - val_loss: 0.0012\n",
            "Epoch 272/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2298e-05 - val_loss: 0.0012\n",
            "Epoch 273/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2279e-05 - val_loss: 0.0012\n",
            "Epoch 274/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9829e-05 - val_loss: 0.0011\n",
            "Epoch 275/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6035e-05 - val_loss: 0.0014\n",
            "Epoch 276/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8872e-05 - val_loss: 0.0011\n",
            "Epoch 277/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7563e-05 - val_loss: 0.0010\n",
            "Epoch 278/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6145e-05 - val_loss: 0.0012\n",
            "Epoch 279/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3055e-05 - val_loss: 0.0011\n",
            "Epoch 280/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3607e-05 - val_loss: 0.0011\n",
            "Epoch 281/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1162e-05 - val_loss: 0.0011\n",
            "Epoch 282/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1787e-06 - val_loss: 0.0011\n",
            "Epoch 283/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5084e-05 - val_loss: 0.0010\n",
            "Epoch 284/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.4065e-06 - val_loss: 0.0010\n",
            "Epoch 285/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7215e-06 - val_loss: 0.0011\n",
            "Epoch 286/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0722e-06 - val_loss: 0.0011\n",
            "Epoch 287/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4470e-06 - val_loss: 0.0010\n",
            "Epoch 288/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.8756e-06 - val_loss: 0.0012\n",
            "Epoch 289/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9730e-06 - val_loss: 0.0011\n",
            "Epoch 290/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2829e-06 - val_loss: 0.0010\n",
            "Epoch 291/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9599e-06 - val_loss: 0.0010\n",
            "Epoch 292/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7703e-06 - val_loss: 0.0011\n",
            "Epoch 293/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3573e-06 - val_loss: 0.0011\n",
            "Epoch 294/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6178e-06 - val_loss: 0.0010\n",
            "Epoch 295/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6028e-06 - val_loss: 0.0010\n",
            "Epoch 296/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2959e-06 - val_loss: 0.0010\n",
            "Epoch 297/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8022e-06 - val_loss: 0.0010\n",
            "Epoch 298/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8293e-06 - val_loss: 0.0010\n",
            "Epoch 299/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7089e-06 - val_loss: 0.0011\n",
            "Epoch 300/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7207e-06 - val_loss: 0.0011\n",
            "Epoch 301/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.5343e-06 - val_loss: 0.0011\n",
            "Epoch 302/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9364e-06 - val_loss: 0.0011\n",
            "Epoch 303/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6504e-06 - val_loss: 0.0012\n",
            "Epoch 304/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4645e-06 - val_loss: 0.0010\n",
            "Epoch 305/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5038e-06 - val_loss: 9.9674e-04\n",
            "Epoch 306/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6102e-06 - val_loss: 9.9145e-04\n",
            "Epoch 307/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9674e-06 - val_loss: 9.9408e-04\n",
            "Epoch 308/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8513e-06 - val_loss: 9.8223e-04\n",
            "Epoch 309/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7368e-06 - val_loss: 0.0011\n",
            "Epoch 310/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2570e-06 - val_loss: 0.0010\n",
            "Epoch 311/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4024e-06 - val_loss: 0.0011\n",
            "Epoch 312/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3764e-06 - val_loss: 0.0011\n",
            "Epoch 313/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0520e-06 - val_loss: 0.0010\n",
            "Epoch 314/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4567e-06 - val_loss: 0.0010\n",
            "Epoch 315/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1033e-06 - val_loss: 0.0011\n",
            "Epoch 316/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9865e-06 - val_loss: 9.9768e-04\n",
            "Epoch 317/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3061e-06 - val_loss: 9.9840e-04\n",
            "Epoch 318/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0107e-06 - val_loss: 9.9015e-04\n",
            "Epoch 319/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2597e-06 - val_loss: 0.0010\n",
            "Epoch 320/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9200e-06 - val_loss: 0.0011\n",
            "Epoch 321/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6915e-06 - val_loss: 0.0010\n",
            "Epoch 322/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3067e-05 - val_loss: 0.0010\n",
            "Epoch 323/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.3927e-05 - val_loss: 0.0010\n",
            "Epoch 324/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6723e-05 - val_loss: 0.0011\n",
            "Epoch 325/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7049e-05 - val_loss: 8.9472e-04\n",
            "Epoch 326/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2397e-05 - val_loss: 0.0010\n",
            "Epoch 327/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5910e-05 - val_loss: 9.4375e-04\n",
            "Epoch 328/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6948e-05 - val_loss: 0.0010\n",
            "Epoch 329/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4133e-05 - val_loss: 9.3806e-04\n",
            "Epoch 330/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0908e-05 - val_loss: 9.4700e-04\n",
            "Epoch 331/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3829e-05 - val_loss: 0.0010\n",
            "Epoch 332/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4993e-05 - val_loss: 8.0908e-04\n",
            "Epoch 333/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7098e-05 - val_loss: 7.8250e-04\n",
            "Epoch 334/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2467e-05 - val_loss: 9.8352e-04\n",
            "Epoch 335/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6536e-05 - val_loss: 0.0010\n",
            "Epoch 336/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6503e-05 - val_loss: 9.5390e-04\n",
            "Epoch 337/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0445e-05 - val_loss: 9.9943e-04\n",
            "Epoch 338/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1356e-05 - val_loss: 7.8265e-04\n",
            "Epoch 339/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6419e-05 - val_loss: 0.0010\n",
            "Epoch 340/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6165e-05 - val_loss: 7.6229e-04\n",
            "Epoch 341/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3535e-05 - val_loss: 9.0721e-04\n",
            "Epoch 342/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0147e-05 - val_loss: 8.6780e-04\n",
            "Epoch 343/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5238e-05 - val_loss: 7.9081e-04\n",
            "Epoch 344/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4488e-05 - val_loss: 7.8598e-04\n",
            "Epoch 345/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9838e-05 - val_loss: 7.5443e-04\n",
            "Epoch 346/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6682e-05 - val_loss: 9.8415e-04\n",
            "Epoch 347/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4882e-05 - val_loss: 8.0809e-04\n",
            "Epoch 348/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2024e-05 - val_loss: 9.7361e-04\n",
            "Epoch 349/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0691e-04 - val_loss: 8.6267e-04\n",
            "Epoch 350/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0262e-04 - val_loss: 9.5696e-04\n",
            "Epoch 351/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5639e-05 - val_loss: 6.6042e-04\n",
            "Epoch 352/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0082e-05 - val_loss: 9.9686e-04\n",
            "Epoch 353/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6831e-05 - val_loss: 7.1937e-04\n",
            "Epoch 354/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0192e-05 - val_loss: 8.4436e-04\n",
            "Epoch 355/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8803e-05 - val_loss: 7.7094e-04\n",
            "Epoch 356/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8955e-05 - val_loss: 8.6575e-04\n",
            "Epoch 357/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1036e-05 - val_loss: 8.6702e-04\n",
            "Epoch 358/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8272e-06 - val_loss: 7.8210e-04\n",
            "Epoch 359/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4924e-05 - val_loss: 0.0011\n",
            "Epoch 360/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9237e-05 - val_loss: 8.5576e-04\n",
            "Epoch 361/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1573e-05 - val_loss: 6.9771e-04\n",
            "Epoch 362/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1782e-05 - val_loss: 8.2320e-04\n",
            "Epoch 363/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8293e-05 - val_loss: 7.3675e-04\n",
            "Epoch 364/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9420e-05 - val_loss: 8.8395e-04\n",
            "Epoch 365/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0675e-05 - val_loss: 9.3979e-04\n",
            "Epoch 366/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7482e-05 - val_loss: 8.3497e-04\n",
            "Epoch 367/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3386e-05 - val_loss: 0.0010\n",
            "Epoch 368/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9281e-05 - val_loss: 7.3943e-04\n",
            "Epoch 369/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1055e-05 - val_loss: 7.6577e-04\n",
            "Epoch 370/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.2491e-06 - val_loss: 7.3263e-04\n",
            "Epoch 371/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1257e-06 - val_loss: 7.7891e-04\n",
            "Epoch 372/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0894e-06 - val_loss: 7.8263e-04\n",
            "Epoch 373/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4609e-06 - val_loss: 8.0987e-04\n",
            "Epoch 374/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0650e-06 - val_loss: 8.1448e-04\n",
            "Epoch 375/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8221e-06 - val_loss: 7.2703e-04\n",
            "Epoch 376/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4338e-06 - val_loss: 7.8160e-04\n",
            "Epoch 377/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9664e-06 - val_loss: 7.7939e-04\n",
            "Epoch 378/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5784e-06 - val_loss: 7.7590e-04\n",
            "Epoch 379/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4165e-06 - val_loss: 7.8989e-04\n",
            "Epoch 380/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3861e-06 - val_loss: 7.8883e-04\n",
            "Epoch 381/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0548e-06 - val_loss: 7.7180e-04\n",
            "Epoch 382/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.3798e-07 - val_loss: 7.7876e-04\n",
            "Epoch 383/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8195e-07 - val_loss: 7.8588e-04\n",
            "Epoch 384/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9076e-07 - val_loss: 7.9268e-04\n",
            "Epoch 385/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.8539e-07 - val_loss: 7.8213e-04\n",
            "Epoch 386/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1078e-07 - val_loss: 7.6334e-04\n",
            "Epoch 387/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8125e-07 - val_loss: 7.9741e-04\n",
            "Epoch 388/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6124e-07 - val_loss: 7.7425e-04\n",
            "Epoch 389/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6108e-07 - val_loss: 8.0094e-04\n",
            "Epoch 390/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2449e-06 - val_loss: 7.7662e-04\n",
            "Epoch 391/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.8922e-07 - val_loss: 7.9773e-04\n",
            "Epoch 392/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1166e-06 - val_loss: 7.9013e-04\n",
            "Epoch 393/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0903e-06 - val_loss: 7.7778e-04\n",
            "Epoch 394/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4298e-06 - val_loss: 7.8592e-04\n",
            "Epoch 395/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5441e-06 - val_loss: 7.5718e-04\n",
            "Epoch 396/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0583e-06 - val_loss: 8.7874e-04\n",
            "Epoch 397/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8190e-05 - val_loss: 8.3017e-04\n",
            "Epoch 398/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2634e-05 - val_loss: 8.6207e-04\n",
            "Epoch 399/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3604e-05 - val_loss: 8.9199e-04\n",
            "Epoch 400/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2222e-05 - val_loss: 9.2648e-04\n",
            "Epoch 401/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2535e-05 - val_loss: 7.2844e-04\n",
            "Epoch 402/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0595e-05 - val_loss: 9.5584e-04\n",
            "Epoch 403/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9215e-05 - val_loss: 8.1939e-04\n",
            "Epoch 404/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6491e-05 - val_loss: 9.1882e-04\n",
            "Epoch 405/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0856e-05 - val_loss: 7.9388e-04\n",
            "Epoch 406/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5188e-06 - val_loss: 8.4014e-04\n",
            "Epoch 407/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3310e-06 - val_loss: 8.3470e-04\n",
            "Epoch 408/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0232e-05 - val_loss: 8.5014e-04\n",
            "Epoch 409/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0538e-05 - val_loss: 8.6360e-04\n",
            "Epoch 410/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2341e-05 - val_loss: 8.4700e-04\n",
            "Epoch 411/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2271e-05 - val_loss: 8.7542e-04\n",
            "Epoch 412/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0199e-05 - val_loss: 8.7747e-04\n",
            "Epoch 413/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.6208e-06 - val_loss: 8.1235e-04\n",
            "Epoch 414/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.5484e-06 - val_loss: 8.6287e-04\n",
            "Epoch 415/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2149e-06 - val_loss: 8.3733e-04\n",
            "Epoch 416/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5052e-06 - val_loss: 8.1979e-04\n",
            "Epoch 417/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7177e-06 - val_loss: 8.1163e-04\n",
            "Epoch 418/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5044e-06 - val_loss: 8.0275e-04\n",
            "Epoch 419/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2418e-06 - val_loss: 7.8383e-04\n",
            "Epoch 420/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9394e-06 - val_loss: 8.0752e-04\n",
            "Epoch 421/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0677e-06 - val_loss: 8.4440e-04\n",
            "Epoch 422/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0741e-06 - val_loss: 8.8484e-04\n",
            "Epoch 423/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3959e-06 - val_loss: 7.9408e-04\n",
            "Epoch 424/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4160e-06 - val_loss: 7.5058e-04\n",
            "Epoch 425/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6221e-06 - val_loss: 7.6330e-04\n",
            "Epoch 426/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0039e-06 - val_loss: 8.0456e-04\n",
            "Epoch 427/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5462e-06 - val_loss: 7.6701e-04\n",
            "Epoch 428/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9523e-06 - val_loss: 7.7884e-04\n",
            "Epoch 429/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8682e-06 - val_loss: 8.0442e-04\n",
            "Epoch 430/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2968e-06 - val_loss: 8.1147e-04\n",
            "Epoch 431/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1365e-06 - val_loss: 7.9207e-04\n",
            "Epoch 432/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1055e-06 - val_loss: 8.0580e-04\n",
            "Epoch 433/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0490e-06 - val_loss: 7.9671e-04\n",
            "Epoch 434/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0533e-07 - val_loss: 7.9510e-04\n",
            "Epoch 435/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.3399e-07 - val_loss: 8.0203e-04\n",
            "Epoch 436/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2893e-06 - val_loss: 7.7063e-04\n",
            "Epoch 437/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4165e-06 - val_loss: 7.8239e-04\n",
            "Epoch 438/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7023e-06 - val_loss: 7.5036e-04\n",
            "Epoch 439/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3126e-06 - val_loss: 8.2794e-04\n",
            "Epoch 440/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9345e-06 - val_loss: 8.1182e-04\n",
            "Epoch 441/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6331e-06 - val_loss: 8.0723e-04\n",
            "Epoch 442/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8412e-06 - val_loss: 7.4502e-04\n",
            "Epoch 443/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8474e-06 - val_loss: 7.4021e-04\n",
            "Epoch 444/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2523e-06 - val_loss: 8.2423e-04\n",
            "Epoch 445/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0528e-06 - val_loss: 7.6824e-04\n",
            "Epoch 446/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0410e-06 - val_loss: 8.1494e-04\n",
            "Epoch 447/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0427e-06 - val_loss: 7.6666e-04\n",
            "Epoch 448/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3632e-06 - val_loss: 8.6479e-04\n",
            "Epoch 449/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7708e-06 - val_loss: 7.4459e-04\n",
            "Epoch 450/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1962e-06 - val_loss: 8.6575e-04\n",
            "Epoch 451/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9369e-06 - val_loss: 8.1361e-04\n",
            "Epoch 452/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2384e-05 - val_loss: 7.6263e-04\n",
            "Epoch 453/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8772e-05 - val_loss: 8.4690e-04\n",
            "Epoch 454/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5469e-05 - val_loss: 7.8678e-04\n",
            "Epoch 455/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1599e-05 - val_loss: 6.6314e-04\n",
            "Epoch 456/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0442e-05 - val_loss: 8.0375e-04\n",
            "Epoch 457/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3941e-05 - val_loss: 7.5101e-04\n",
            "Epoch 458/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0613e-05 - val_loss: 7.0475e-04\n",
            "Epoch 459/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1369e-05 - val_loss: 7.2879e-04\n",
            "Epoch 460/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.1413e-05 - val_loss: 7.0665e-04\n",
            "Epoch 461/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0238e-05 - val_loss: 6.4113e-04\n",
            "Epoch 462/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9724e-05 - val_loss: 8.3081e-04\n",
            "Epoch 463/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7515e-05 - val_loss: 8.3625e-04\n",
            "Epoch 464/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8547e-05 - val_loss: 6.6293e-04\n",
            "Epoch 465/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9480e-05 - val_loss: 7.9207e-04\n",
            "Epoch 466/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5184e-05 - val_loss: 8.1355e-04\n",
            "Epoch 467/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3151e-06 - val_loss: 7.3209e-04\n",
            "Epoch 468/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1036e-05 - val_loss: 7.4401e-04\n",
            "Epoch 469/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2142e-05 - val_loss: 7.4917e-04\n",
            "Epoch 470/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2816e-05 - val_loss: 7.5897e-04\n",
            "Epoch 471/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3728e-05 - val_loss: 7.8947e-04\n",
            "Epoch 472/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0319e-05 - val_loss: 7.7706e-04\n",
            "Epoch 473/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7044e-05 - val_loss: 7.2003e-04\n",
            "Epoch 474/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2930e-05 - val_loss: 7.8127e-04\n",
            "Epoch 475/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0350e-05 - val_loss: 7.4434e-04\n",
            "Epoch 476/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1858e-05 - val_loss: 7.7520e-04\n",
            "Epoch 477/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0138e-05 - val_loss: 7.8456e-04\n",
            "Epoch 478/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1844e-06 - val_loss: 8.4758e-04\n",
            "Epoch 479/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7619e-06 - val_loss: 8.2827e-04\n",
            "Epoch 480/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2199e-05 - val_loss: 7.9877e-04\n",
            "Epoch 481/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2003e-05 - val_loss: 7.9187e-04\n",
            "Epoch 482/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2042e-05 - val_loss: 8.1933e-04\n",
            "Epoch 483/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5220e-06 - val_loss: 7.5939e-04\n",
            "Epoch 484/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1209e-06 - val_loss: 7.6919e-04\n",
            "Epoch 485/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0342e-05 - val_loss: 9.1295e-04\n",
            "Epoch 486/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0919e-06 - val_loss: 7.9384e-04\n",
            "Epoch 487/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6339e-06 - val_loss: 8.1789e-04\n",
            "Epoch 488/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9565e-06 - val_loss: 8.0604e-04\n",
            "Epoch 489/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2584e-06 - val_loss: 8.0489e-04\n",
            "Epoch 490/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5720e-06 - val_loss: 8.2200e-04\n",
            "Epoch 491/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3196e-06 - val_loss: 7.5405e-04\n",
            "Epoch 492/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6417e-06 - val_loss: 7.8587e-04\n",
            "Epoch 493/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1862e-06 - val_loss: 7.9194e-04\n",
            "Epoch 494/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9730e-06 - val_loss: 8.4328e-04\n",
            "Epoch 495/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9701e-06 - val_loss: 7.4975e-04\n",
            "Epoch 496/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.2951e-06 - val_loss: 8.1769e-04\n",
            "Epoch 497/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7618e-05 - val_loss: 7.8434e-04\n",
            "Epoch 498/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4584e-05 - val_loss: 9.0271e-04\n",
            "Epoch 499/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0910e-05 - val_loss: 8.2364e-04\n",
            "Epoch 500/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.2005e-06 - val_loss: 9.0739e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x29a933ee0>"
            ]
          },
          "execution_count": 96,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 500\n",
        "normalizer = Normalization(axis=-1)\n",
        "model_dnn4 = Sequential(normalizer) # scale the input variables\n",
        "model_dnn4.add(Dense(68, activation=act_fn)) \n",
        "model_dnn4.add(Dense(34, activation=act_fn))\n",
        "model_dnn4.add(Dense(12, activation=act_fn))\n",
        "model_dnn4.add(Dense(6, activation=act_fn))\n",
        "model_dnn4.add(Dense(1)) # output layer\n",
        "model_dnn4.compile(loss=loss, optimizer=optimizer) # define the loss and optimizer\n",
        "model_dnn4.fit(X_train,y_train,validation_data = (X_valid,y_valid),epochs=epochs) #fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "rVXEkt9GoBWX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            " 9/23 [==========>...................] - ETA: 0s - loss: 0.0185"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 19:59:17.130776: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 1s 11ms/step - loss: 0.0083 - val_loss: 9.8936e-04\n",
            "Epoch 2/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0146e-04 - val_loss: 7.7221e-04\n",
            "Epoch 3/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 19:59:17.463674: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7553e-04 - val_loss: 8.1353e-04\n",
            "Epoch 4/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6373e-04 - val_loss: 8.4398e-04\n",
            "Epoch 5/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5473e-04 - val_loss: 8.5110e-04\n",
            "Epoch 6/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.4845e-04 - val_loss: 8.1135e-04\n",
            "Epoch 7/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.4188e-04 - val_loss: 7.8002e-04\n",
            "Epoch 8/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.3493e-04 - val_loss: 7.4685e-04\n",
            "Epoch 9/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.2327e-04 - val_loss: 7.9372e-04\n",
            "Epoch 10/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1098e-04 - val_loss: 8.0440e-04\n",
            "Epoch 11/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0208e-04 - val_loss: 7.6386e-04\n",
            "Epoch 12/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9864e-04 - val_loss: 7.6967e-04\n",
            "Epoch 13/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9572e-04 - val_loss: 7.8570e-04\n",
            "Epoch 14/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9259e-04 - val_loss: 7.8256e-04\n",
            "Epoch 15/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8356e-04 - val_loss: 9.1384e-04\n",
            "Epoch 16/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6927e-04 - val_loss: 8.6839e-04\n",
            "Epoch 17/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5495e-04 - val_loss: 8.6685e-04\n",
            "Epoch 18/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.4313e-04 - val_loss: 8.9146e-04\n",
            "Epoch 19/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4690e-04 - val_loss: 9.6526e-04\n",
            "Epoch 20/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1406e-04 - val_loss: 0.0010\n",
            "Epoch 21/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.8124e-04 - val_loss: 0.0013\n",
            "Epoch 22/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5597e-04 - val_loss: 0.0013\n",
            "Epoch 23/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2907e-04 - val_loss: 0.0014\n",
            "Epoch 24/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1065e-04 - val_loss: 0.0014\n",
            "Epoch 25/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0509e-04 - val_loss: 0.0013\n",
            "Epoch 26/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8201e-04 - val_loss: 0.0011\n",
            "Epoch 27/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4881e-04 - val_loss: 0.0011\n",
            "Epoch 28/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3090e-04 - val_loss: 0.0011\n",
            "Epoch 29/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9440e-04 - val_loss: 9.7885e-04\n",
            "Epoch 30/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5890e-04 - val_loss: 8.6977e-04\n",
            "Epoch 31/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2749e-04 - val_loss: 0.0013\n",
            "Epoch 32/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0532e-04 - val_loss: 0.0012\n",
            "Epoch 33/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9067e-04 - val_loss: 0.0012\n",
            "Epoch 34/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6875e-04 - val_loss: 9.6776e-04\n",
            "Epoch 35/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6742e-04 - val_loss: 0.0010\n",
            "Epoch 36/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7742e-04 - val_loss: 6.1523e-04\n",
            "Epoch 37/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4793e-04 - val_loss: 9.3456e-04\n",
            "Epoch 38/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4831e-04 - val_loss: 0.0011\n",
            "Epoch 39/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3189e-04 - val_loss: 0.0013\n",
            "Epoch 40/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0272e-04 - val_loss: 0.0016\n",
            "Epoch 41/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8859e-04 - val_loss: 9.0395e-04\n",
            "Epoch 42/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9177e-04 - val_loss: 6.6340e-04\n",
            "Epoch 43/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9507e-04 - val_loss: 8.4127e-04\n",
            "Epoch 44/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5770e-04 - val_loss: 9.7770e-04\n",
            "Epoch 45/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4101e-04 - val_loss: 0.0014\n",
            "Epoch 46/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2344e-04 - val_loss: 9.7260e-04\n",
            "Epoch 47/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1068e-04 - val_loss: 0.0011\n",
            "Epoch 48/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0053e-04 - val_loss: 9.1638e-04\n",
            "Epoch 49/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8774e-04 - val_loss: 9.9627e-04\n",
            "Epoch 50/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9639e-04 - val_loss: 9.8564e-04\n",
            "Epoch 51/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7766e-04 - val_loss: 9.6675e-04\n",
            "Epoch 52/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6609e-04 - val_loss: 9.3214e-04\n",
            "Epoch 53/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6468e-04 - val_loss: 7.9890e-04\n",
            "Epoch 54/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5833e-04 - val_loss: 8.0603e-04\n",
            "Epoch 55/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6514e-04 - val_loss: 8.5850e-04\n",
            "Epoch 56/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4986e-04 - val_loss: 9.4225e-04\n",
            "Epoch 57/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3566e-04 - val_loss: 0.0012\n",
            "Epoch 58/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5526e-04 - val_loss: 8.1060e-04\n",
            "Epoch 59/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3902e-04 - val_loss: 8.7421e-04\n",
            "Epoch 60/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2578e-04 - val_loss: 8.1920e-04\n",
            "Epoch 61/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2348e-04 - val_loss: 9.6818e-04\n",
            "Epoch 62/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2672e-04 - val_loss: 8.8418e-04\n",
            "Epoch 63/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1147e-04 - val_loss: 0.0011\n",
            "Epoch 64/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0011e-04 - val_loss: 9.6791e-04\n",
            "Epoch 65/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0257e-04 - val_loss: 8.5008e-04\n",
            "Epoch 66/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0959e-04 - val_loss: 9.9431e-04\n",
            "Epoch 67/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.5368e-05 - val_loss: 8.2702e-04\n",
            "Epoch 68/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8072e-05 - val_loss: 0.0010\n",
            "Epoch 69/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3548e-05 - val_loss: 0.0010\n",
            "Epoch 70/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.1428e-05 - val_loss: 7.8133e-04\n",
            "Epoch 71/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8736e-05 - val_loss: 9.6409e-04\n",
            "Epoch 72/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.4360e-05 - val_loss: 8.7619e-04\n",
            "Epoch 73/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.6278e-05 - val_loss: 7.6301e-04\n",
            "Epoch 74/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3342e-05 - val_loss: 6.3255e-04\n",
            "Epoch 75/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0462e-05 - val_loss: 7.3755e-04\n",
            "Epoch 76/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.2458e-05 - val_loss: 8.8448e-04\n",
            "Epoch 77/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.2269e-05 - val_loss: 8.9986e-04\n",
            "Epoch 78/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0272e-05 - val_loss: 8.8345e-04\n",
            "Epoch 79/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0933e-05 - val_loss: 8.8957e-04\n",
            "Epoch 80/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.9285e-05 - val_loss: 9.2833e-04\n",
            "Epoch 81/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9341e-05 - val_loss: 8.1444e-04\n",
            "Epoch 82/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.4912e-05 - val_loss: 8.1988e-04\n",
            "Epoch 83/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1551e-05 - val_loss: 0.0011\n",
            "Epoch 84/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.0055e-04 - val_loss: 0.0011\n",
            "Epoch 85/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2611e-04 - val_loss: 9.8110e-04\n",
            "Epoch 86/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5145e-05 - val_loss: 0.0010\n",
            "Epoch 87/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7778e-05 - val_loss: 9.9195e-04\n",
            "Epoch 88/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8675e-05 - val_loss: 0.0010\n",
            "Epoch 89/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7232e-05 - val_loss: 0.0011\n",
            "Epoch 90/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1425e-05 - val_loss: 9.4564e-04\n",
            "Epoch 91/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5700e-05 - val_loss: 9.8203e-04\n",
            "Epoch 92/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7855e-05 - val_loss: 9.4329e-04\n",
            "Epoch 93/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3297e-05 - val_loss: 9.5798e-04\n",
            "Epoch 94/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2033e-05 - val_loss: 9.8181e-04\n",
            "Epoch 95/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5116e-05 - val_loss: 0.0010\n",
            "Epoch 96/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2283e-05 - val_loss: 0.0010\n",
            "Epoch 97/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7572e-05 - val_loss: 0.0010\n",
            "Epoch 98/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1419e-05 - val_loss: 9.4595e-04\n",
            "Epoch 99/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5405e-05 - val_loss: 9.4977e-04\n",
            "Epoch 100/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1577e-05 - val_loss: 9.7009e-04\n",
            "Epoch 101/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0300e-05 - val_loss: 9.6494e-04\n",
            "Epoch 102/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8640e-05 - val_loss: 9.9755e-04\n",
            "Epoch 103/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1676e-05 - val_loss: 0.0010\n",
            "Epoch 104/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6818e-05 - val_loss: 9.5656e-04\n",
            "Epoch 105/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1917e-05 - val_loss: 9.6591e-04\n",
            "Epoch 106/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3500e-05 - val_loss: 9.7286e-04\n",
            "Epoch 107/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3128e-05 - val_loss: 9.7318e-04\n",
            "Epoch 108/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8680e-05 - val_loss: 9.8703e-04\n",
            "Epoch 109/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7764e-05 - val_loss: 0.0010\n",
            "Epoch 110/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5319e-05 - val_loss: 9.6117e-04\n",
            "Epoch 111/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5887e-05 - val_loss: 9.7274e-04\n",
            "Epoch 112/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4597e-05 - val_loss: 9.5793e-04\n",
            "Epoch 113/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3661e-05 - val_loss: 9.5482e-04\n",
            "Epoch 114/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9432e-05 - val_loss: 9.5394e-04\n",
            "Epoch 115/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3615e-05 - val_loss: 9.9129e-04\n",
            "Epoch 116/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.9264e-05 - val_loss: 9.7703e-04\n",
            "Epoch 117/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.4471e-05 - val_loss: 9.3146e-04\n",
            "Epoch 118/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3095e-05 - val_loss: 9.4462e-04\n",
            "Epoch 119/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2525e-05 - val_loss: 9.1597e-04\n",
            "Epoch 120/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.3671e-05 - val_loss: 9.9914e-04\n",
            "Epoch 121/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.8208e-05 - val_loss: 9.6138e-04\n",
            "Epoch 122/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.6625e-05 - val_loss: 9.6389e-04\n",
            "Epoch 123/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.5338e-05 - val_loss: 9.2252e-04\n",
            "Epoch 124/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6404e-05 - val_loss: 9.3639e-04\n",
            "Epoch 125/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4205e-05 - val_loss: 9.2331e-04\n",
            "Epoch 126/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2527e-05 - val_loss: 9.2040e-04\n",
            "Epoch 127/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2880e-05 - val_loss: 9.4071e-04\n",
            "Epoch 128/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1025e-05 - val_loss: 9.7274e-04\n",
            "Epoch 129/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0255e-05 - val_loss: 9.3947e-04\n",
            "Epoch 130/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6664e-05 - val_loss: 8.9388e-04\n",
            "Epoch 131/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9507e-05 - val_loss: 9.2953e-04\n",
            "Epoch 132/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3428e-05 - val_loss: 0.0012\n",
            "Epoch 133/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8682e-05 - val_loss: 0.0010\n",
            "Epoch 134/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2635e-05 - val_loss: 0.0012\n",
            "Epoch 135/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9469e-05 - val_loss: 0.0010\n",
            "Epoch 136/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4840e-05 - val_loss: 9.9107e-04\n",
            "Epoch 137/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6573e-05 - val_loss: 0.0010\n",
            "Epoch 138/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2464e-05 - val_loss: 9.9940e-04\n",
            "Epoch 139/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4014e-05 - val_loss: 9.9923e-04\n",
            "Epoch 140/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4311e-05 - val_loss: 0.0010\n",
            "Epoch 141/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3673e-05 - val_loss: 9.4141e-04\n",
            "Epoch 142/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0585e-05 - val_loss: 0.0010\n",
            "Epoch 143/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4702e-05 - val_loss: 9.4677e-04\n",
            "Epoch 144/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3404e-05 - val_loss: 9.6143e-04\n",
            "Epoch 145/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8688e-05 - val_loss: 9.3328e-04\n",
            "Epoch 146/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4511e-05 - val_loss: 8.9554e-04\n",
            "Epoch 147/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0639e-05 - val_loss: 9.8962e-04\n",
            "Epoch 148/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.6495e-05 - val_loss: 7.2945e-04\n",
            "Epoch 149/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5392e-05 - val_loss: 9.5566e-04\n",
            "Epoch 150/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4883e-05 - val_loss: 8.4197e-04\n",
            "Epoch 151/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9764e-05 - val_loss: 0.0010\n",
            "Epoch 152/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5297e-05 - val_loss: 9.0210e-04\n",
            "Epoch 153/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5247e-05 - val_loss: 9.9962e-04\n",
            "Epoch 154/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2449e-05 - val_loss: 9.7919e-04\n",
            "Epoch 155/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0495e-05 - val_loss: 9.7710e-04\n",
            "Epoch 156/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5288e-05 - val_loss: 9.6242e-04\n",
            "Epoch 157/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1389e-05 - val_loss: 9.6634e-04\n",
            "Epoch 158/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5605e-05 - val_loss: 0.0010\n",
            "Epoch 159/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4619e-05 - val_loss: 9.7674e-04\n",
            "Epoch 160/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0550e-05 - val_loss: 9.4022e-04\n",
            "Epoch 161/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7712e-06 - val_loss: 8.9751e-04\n",
            "Epoch 162/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4486e-06 - val_loss: 9.3958e-04\n",
            "Epoch 163/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2602e-06 - val_loss: 9.4349e-04\n",
            "Epoch 164/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9652e-06 - val_loss: 9.2981e-04\n",
            "Epoch 165/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9577e-06 - val_loss: 9.3073e-04\n",
            "Epoch 166/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9168e-06 - val_loss: 9.5448e-04\n",
            "Epoch 167/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9713e-06 - val_loss: 9.4802e-04\n",
            "Epoch 168/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7980e-06 - val_loss: 9.4612e-04\n",
            "Epoch 169/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0305e-06 - val_loss: 9.3674e-04\n",
            "Epoch 170/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3111e-06 - val_loss: 9.4501e-04\n",
            "Epoch 171/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4765e-05 - val_loss: 9.1248e-04\n",
            "Epoch 172/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3304e-05 - val_loss: 9.1563e-04\n",
            "Epoch 173/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5492e-05 - val_loss: 8.7632e-04\n",
            "Epoch 174/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2832e-05 - val_loss: 9.2412e-04\n",
            "Epoch 175/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0331e-05 - val_loss: 9.3145e-04\n",
            "Epoch 176/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4156e-05 - val_loss: 9.1903e-04\n",
            "Epoch 177/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4833e-05 - val_loss: 9.1584e-04\n",
            "Epoch 178/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4302e-05 - val_loss: 8.9738e-04\n",
            "Epoch 179/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3942e-05 - val_loss: 7.4954e-04\n",
            "Epoch 180/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9988e-05 - val_loss: 7.9179e-04\n",
            "Epoch 181/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.2125e-04 - val_loss: 0.0013\n",
            "Epoch 182/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3712e-04 - val_loss: 0.0011\n",
            "Epoch 183/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1714e-04 - val_loss: 8.8197e-04\n",
            "Epoch 184/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6165e-04 - val_loss: 0.0010\n",
            "Epoch 185/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5089e-04 - val_loss: 0.0012\n",
            "Epoch 186/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0933e-04 - val_loss: 0.0013\n",
            "Epoch 187/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3573e-04 - val_loss: 8.5957e-04\n",
            "Epoch 188/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.4676e-05 - val_loss: 0.0010\n",
            "Epoch 189/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7510e-05 - val_loss: 0.0010\n",
            "Epoch 190/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5603e-05 - val_loss: 9.6264e-04\n",
            "Epoch 191/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7351e-05 - val_loss: 0.0010\n",
            "Epoch 192/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6687e-05 - val_loss: 9.6690e-04\n",
            "Epoch 193/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4423e-05 - val_loss: 9.5769e-04\n",
            "Epoch 194/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6940e-05 - val_loss: 9.4657e-04\n",
            "Epoch 195/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8853e-05 - val_loss: 9.6211e-04\n",
            "Epoch 196/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4220e-05 - val_loss: 9.8541e-04\n",
            "Epoch 197/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1705e-05 - val_loss: 9.5954e-04\n",
            "Epoch 198/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.8090e-06 - val_loss: 9.4305e-04\n",
            "Epoch 199/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5469e-06 - val_loss: 9.4944e-04\n",
            "Epoch 200/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5277e-06 - val_loss: 9.7090e-04\n",
            "Epoch 201/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9212e-06 - val_loss: 0.0010\n",
            "Epoch 202/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5992e-06 - val_loss: 9.9300e-04\n",
            "Epoch 203/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0378e-05 - val_loss: 0.0010\n",
            "Epoch 204/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8401e-06 - val_loss: 0.0010\n",
            "Epoch 205/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5005e-06 - val_loss: 0.0010\n",
            "Epoch 206/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9543e-06 - val_loss: 0.0010\n",
            "Epoch 207/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1459e-06 - val_loss: 0.0010\n",
            "Epoch 208/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9335e-06 - val_loss: 9.9491e-04\n",
            "Epoch 209/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6791e-06 - val_loss: 0.0010\n",
            "Epoch 210/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6661e-06 - val_loss: 0.0010\n",
            "Epoch 211/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4967e-06 - val_loss: 0.0010\n",
            "Epoch 212/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5609e-06 - val_loss: 0.0010\n",
            "Epoch 213/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5747e-06 - val_loss: 0.0010\n",
            "Epoch 214/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4894e-06 - val_loss: 0.0010\n",
            "Epoch 215/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3470e-06 - val_loss: 0.0010\n",
            "Epoch 216/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9322e-06 - val_loss: 0.0010\n",
            "Epoch 217/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7774e-06 - val_loss: 0.0010\n",
            "Epoch 218/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8484e-06 - val_loss: 0.0010\n",
            "Epoch 219/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1097e-06 - val_loss: 0.0010\n",
            "Epoch 220/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7241e-06 - val_loss: 0.0010\n",
            "Epoch 221/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7534e-06 - val_loss: 9.9112e-04\n",
            "Epoch 222/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6554e-06 - val_loss: 0.0011\n",
            "Epoch 223/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3814e-06 - val_loss: 9.9396e-04\n",
            "Epoch 224/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6254e-06 - val_loss: 0.0010\n",
            "Epoch 225/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3582e-06 - val_loss: 0.0010\n",
            "Epoch 226/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7590e-06 - val_loss: 0.0011\n",
            "Epoch 227/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8136e-06 - val_loss: 0.0010\n",
            "Epoch 228/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5590e-05 - val_loss: 9.9424e-04\n",
            "Epoch 229/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2348e-05 - val_loss: 0.0010\n",
            "Epoch 230/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.8098e-06 - val_loss: 9.5803e-04\n",
            "Epoch 231/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6218e-06 - val_loss: 0.0011\n",
            "Epoch 232/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9955e-06 - val_loss: 0.0010\n",
            "Epoch 233/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9588e-06 - val_loss: 0.0010\n",
            "Epoch 234/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0507e-06 - val_loss: 9.9490e-04\n",
            "Epoch 235/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.4420e-06 - val_loss: 0.0010\n",
            "Epoch 236/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3743e-05 - val_loss: 9.9682e-04\n",
            "Epoch 237/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6304e-05 - val_loss: 0.0010\n",
            "Epoch 238/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9515e-05 - val_loss: 8.6170e-04\n",
            "Epoch 239/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9133e-05 - val_loss: 0.0010\n",
            "Epoch 240/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1224e-05 - val_loss: 9.5860e-04\n",
            "Epoch 241/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4942e-05 - val_loss: 0.0011\n",
            "Epoch 242/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3283e-05 - val_loss: 8.7246e-04\n",
            "Epoch 243/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1408e-05 - val_loss: 9.8947e-04\n",
            "Epoch 244/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7091e-05 - val_loss: 9.3814e-04\n",
            "Epoch 245/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3854e-05 - val_loss: 9.7968e-04\n",
            "Epoch 246/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7791e-05 - val_loss: 9.6944e-04\n",
            "Epoch 247/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1413e-05 - val_loss: 0.0010\n",
            "Epoch 248/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0022e-05 - val_loss: 9.5781e-04\n",
            "Epoch 249/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1117e-05 - val_loss: 0.0012\n",
            "Epoch 250/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4471e-05 - val_loss: 0.0010\n",
            "Epoch 251/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3767e-05 - val_loss: 0.0010\n",
            "Epoch 252/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9417e-05 - val_loss: 9.7884e-04\n",
            "Epoch 253/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5841e-05 - val_loss: 9.7813e-04\n",
            "Epoch 254/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9335e-05 - val_loss: 0.0011\n",
            "Epoch 255/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2216e-05 - val_loss: 0.0011\n",
            "Epoch 256/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0109e-05 - val_loss: 0.0011\n",
            "Epoch 257/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8282e-05 - val_loss: 9.9434e-04\n",
            "Epoch 258/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5254e-05 - val_loss: 0.0010\n",
            "Epoch 259/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7585e-05 - val_loss: 0.0012\n",
            "Epoch 260/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6924e-05 - val_loss: 0.0011\n",
            "Epoch 261/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6712e-05 - val_loss: 9.4477e-04\n",
            "Epoch 262/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0086e-05 - val_loss: 0.0010\n",
            "Epoch 263/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7998e-05 - val_loss: 0.0010\n",
            "Epoch 264/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4656e-05 - val_loss: 9.9522e-04\n",
            "Epoch 265/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.4819e-06 - val_loss: 9.6163e-04\n",
            "Epoch 266/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.2670e-06 - val_loss: 9.6902e-04\n",
            "Epoch 267/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2293e-06 - val_loss: 9.7872e-04\n",
            "Epoch 268/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9914e-06 - val_loss: 9.4111e-04\n",
            "Epoch 269/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7844e-06 - val_loss: 9.5148e-04\n",
            "Epoch 270/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0373e-06 - val_loss: 9.3616e-04\n",
            "Epoch 271/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1134e-06 - val_loss: 9.4426e-04\n",
            "Epoch 272/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1966e-06 - val_loss: 9.5791e-04\n",
            "Epoch 273/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7475e-06 - val_loss: 9.2231e-04\n",
            "Epoch 274/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4682e-06 - val_loss: 8.9729e-04\n",
            "Epoch 275/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3410e-06 - val_loss: 8.7225e-04\n",
            "Epoch 276/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8259e-06 - val_loss: 9.2891e-04\n",
            "Epoch 277/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5596e-06 - val_loss: 9.2974e-04\n",
            "Epoch 278/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6647e-06 - val_loss: 9.1782e-04\n",
            "Epoch 279/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1796e-06 - val_loss: 9.3068e-04\n",
            "Epoch 280/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4058e-06 - val_loss: 9.2636e-04\n",
            "Epoch 281/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2195e-06 - val_loss: 9.2005e-04\n",
            "Epoch 282/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6383e-06 - val_loss: 9.2420e-04\n",
            "Epoch 283/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.3108e-06 - val_loss: 8.8633e-04\n",
            "Epoch 284/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5911e-06 - val_loss: 9.0190e-04\n",
            "Epoch 285/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9317e-06 - val_loss: 8.9610e-04\n",
            "Epoch 286/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9147e-06 - val_loss: 9.2577e-04\n",
            "Epoch 287/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7238e-06 - val_loss: 9.0298e-04\n",
            "Epoch 288/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3332e-06 - val_loss: 9.0424e-04\n",
            "Epoch 289/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1398e-06 - val_loss: 9.1039e-04\n",
            "Epoch 290/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9668e-06 - val_loss: 9.4503e-04\n",
            "Epoch 291/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0426e-05 - val_loss: 9.5405e-04\n",
            "Epoch 292/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6468e-05 - val_loss: 0.0010\n",
            "Epoch 293/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1962e-04 - val_loss: 9.6349e-04\n",
            "Epoch 294/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0461e-05 - val_loss: 7.9499e-04\n",
            "Epoch 295/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5883e-05 - val_loss: 9.3909e-04\n",
            "Epoch 296/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8046e-05 - val_loss: 7.3913e-04\n",
            "Epoch 297/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2288e-05 - val_loss: 8.6774e-04\n",
            "Epoch 298/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2319e-05 - val_loss: 8.2968e-04\n",
            "Epoch 299/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1015e-05 - val_loss: 9.1039e-04\n",
            "Epoch 300/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4890e-05 - val_loss: 9.1106e-04\n",
            "Epoch 301/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9375e-05 - val_loss: 8.4954e-04\n",
            "Epoch 302/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0222e-05 - val_loss: 8.4754e-04\n",
            "Epoch 303/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2936e-05 - val_loss: 8.4930e-04\n",
            "Epoch 304/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6496e-05 - val_loss: 8.0873e-04\n",
            "Epoch 305/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4133e-05 - val_loss: 9.4935e-04\n",
            "Epoch 306/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7085e-05 - val_loss: 9.8360e-04\n",
            "Epoch 307/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5098e-05 - val_loss: 0.0010\n",
            "Epoch 308/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1927e-05 - val_loss: 9.1631e-04\n",
            "Epoch 309/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0909e-05 - val_loss: 9.5138e-04\n",
            "Epoch 310/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0169e-06 - val_loss: 9.3244e-04\n",
            "Epoch 311/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2429e-06 - val_loss: 9.1170e-04\n",
            "Epoch 312/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2725e-06 - val_loss: 9.2128e-04\n",
            "Epoch 313/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9676e-06 - val_loss: 0.0010\n",
            "Epoch 314/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1605e-06 - val_loss: 9.8480e-04\n",
            "Epoch 315/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7022e-06 - val_loss: 9.4807e-04\n",
            "Epoch 316/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3731e-06 - val_loss: 9.2890e-04\n",
            "Epoch 317/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9333e-06 - val_loss: 9.6160e-04\n",
            "Epoch 318/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6537e-06 - val_loss: 9.8424e-04\n",
            "Epoch 319/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2338e-06 - val_loss: 9.8491e-04\n",
            "Epoch 320/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9462e-06 - val_loss: 9.9593e-04\n",
            "Epoch 321/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1078e-06 - val_loss: 9.6163e-04\n",
            "Epoch 322/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8814e-06 - val_loss: 0.0010\n",
            "Epoch 323/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9460e-06 - val_loss: 9.4667e-04\n",
            "Epoch 324/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5671e-06 - val_loss: 9.8786e-04\n",
            "Epoch 325/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9181e-06 - val_loss: 9.3001e-04\n",
            "Epoch 326/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9653e-06 - val_loss: 0.0010\n",
            "Epoch 327/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1659e-06 - val_loss: 9.5859e-04\n",
            "Epoch 328/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6044e-06 - val_loss: 0.0010\n",
            "Epoch 329/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5527e-06 - val_loss: 9.4224e-04\n",
            "Epoch 330/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1145e-06 - val_loss: 0.0010\n",
            "Epoch 331/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6980e-06 - val_loss: 9.7160e-04\n",
            "Epoch 332/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8117e-06 - val_loss: 9.6816e-04\n",
            "Epoch 333/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1957e-06 - val_loss: 9.6719e-04\n",
            "Epoch 334/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3117e-06 - val_loss: 9.2265e-04\n",
            "Epoch 335/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9772e-06 - val_loss: 0.0010\n",
            "Epoch 336/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9356e-06 - val_loss: 9.7736e-04\n",
            "Epoch 337/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6725e-05 - val_loss: 9.6190e-04\n",
            "Epoch 338/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5845e-05 - val_loss: 9.5596e-04\n",
            "Epoch 339/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7467e-05 - val_loss: 0.0010\n",
            "Epoch 340/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5161e-05 - val_loss: 0.0010\n",
            "Epoch 341/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1110e-05 - val_loss: 9.5999e-04\n",
            "Epoch 342/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3867e-05 - val_loss: 9.7674e-04\n",
            "Epoch 343/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1423e-05 - val_loss: 0.0011\n",
            "Epoch 344/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9684e-05 - val_loss: 0.0011\n",
            "Epoch 345/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3332e-05 - val_loss: 0.0010\n",
            "Epoch 346/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6656e-05 - val_loss: 0.0011\n",
            "Epoch 347/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4992e-05 - val_loss: 0.0010\n",
            "Epoch 348/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1888e-05 - val_loss: 0.0010\n",
            "Epoch 349/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3192e-06 - val_loss: 0.0011\n",
            "Epoch 350/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6579e-06 - val_loss: 0.0010\n",
            "Epoch 351/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3864e-06 - val_loss: 0.0010\n",
            "Epoch 352/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0481e-06 - val_loss: 0.0010\n",
            "Epoch 353/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7991e-06 - val_loss: 0.0011\n",
            "Epoch 354/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1433e-06 - val_loss: 0.0010\n",
            "Epoch 355/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6752e-06 - val_loss: 0.0011\n",
            "Epoch 356/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1835e-06 - val_loss: 0.0010\n",
            "Epoch 357/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9478e-06 - val_loss: 0.0011\n",
            "Epoch 358/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5658e-06 - val_loss: 0.0011\n",
            "Epoch 359/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9696e-06 - val_loss: 0.0011\n",
            "Epoch 360/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9657e-06 - val_loss: 0.0011\n",
            "Epoch 361/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5779e-06 - val_loss: 0.0010\n",
            "Epoch 362/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.6358e-06 - val_loss: 0.0011\n",
            "Epoch 363/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0801e-06 - val_loss: 0.0010\n",
            "Epoch 364/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2881e-05 - val_loss: 9.4359e-04\n",
            "Epoch 365/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3015e-05 - val_loss: 0.0011\n",
            "Epoch 366/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0188e-05 - val_loss: 0.0010\n",
            "Epoch 367/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5999e-05 - val_loss: 0.0011\n",
            "Epoch 368/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2872e-05 - val_loss: 9.5216e-04\n",
            "Epoch 369/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1282e-04 - val_loss: 0.0011\n",
            "Epoch 370/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6602e-05 - val_loss: 0.0010\n",
            "Epoch 371/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0477e-05 - val_loss: 9.5565e-04\n",
            "Epoch 372/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2483e-05 - val_loss: 8.8781e-04\n",
            "Epoch 373/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8610e-05 - val_loss: 9.9574e-04\n",
            "Epoch 374/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4721e-05 - val_loss: 8.5318e-04\n",
            "Epoch 375/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3939e-05 - val_loss: 9.2512e-04\n",
            "Epoch 376/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8197e-05 - val_loss: 0.0011\n",
            "Epoch 377/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0439e-05 - val_loss: 9.3431e-04\n",
            "Epoch 378/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0881e-05 - val_loss: 0.0010\n",
            "Epoch 379/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5089e-05 - val_loss: 9.2072e-04\n",
            "Epoch 380/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3614e-05 - val_loss: 9.0959e-04\n",
            "Epoch 381/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.5820e-06 - val_loss: 9.0241e-04\n",
            "Epoch 382/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6037e-06 - val_loss: 9.4749e-04\n",
            "Epoch 383/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.6937e-06 - val_loss: 9.5556e-04\n",
            "Epoch 384/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0971e-06 - val_loss: 9.0736e-04\n",
            "Epoch 385/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6705e-06 - val_loss: 8.9548e-04\n",
            "Epoch 386/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7360e-06 - val_loss: 8.6475e-04\n",
            "Epoch 387/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9002e-06 - val_loss: 9.0846e-04\n",
            "Epoch 388/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2266e-06 - val_loss: 9.1587e-04\n",
            "Epoch 389/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6088e-06 - val_loss: 9.2703e-04\n",
            "Epoch 390/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1306e-05 - val_loss: 9.6117e-04\n",
            "Epoch 391/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7429e-06 - val_loss: 8.8583e-04\n",
            "Epoch 392/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8195e-06 - val_loss: 9.1359e-04\n",
            "Epoch 393/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7957e-06 - val_loss: 8.9187e-04\n",
            "Epoch 394/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1877e-06 - val_loss: 9.1969e-04\n",
            "Epoch 395/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7086e-05 - val_loss: 8.8147e-04\n",
            "Epoch 396/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2550e-05 - val_loss: 9.2847e-04\n",
            "Epoch 397/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3542e-05 - val_loss: 8.7066e-04\n",
            "Epoch 398/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.3949e-05 - val_loss: 0.0011\n",
            "Epoch 399/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6718e-05 - val_loss: 9.0186e-04\n",
            "Epoch 400/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0459e-05 - val_loss: 0.0011\n",
            "Epoch 401/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3498e-05 - val_loss: 0.0011\n",
            "Epoch 402/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2948e-05 - val_loss: 0.0010\n",
            "Epoch 403/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5049e-05 - val_loss: 9.8630e-04\n",
            "Epoch 404/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.7013e-06 - val_loss: 9.8814e-04\n",
            "Epoch 405/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0691e-05 - val_loss: 0.0011\n",
            "Epoch 406/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9788e-05 - val_loss: 9.5640e-04\n",
            "Epoch 407/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1438e-05 - val_loss: 0.0011\n",
            "Epoch 408/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4747e-05 - val_loss: 9.7516e-04\n",
            "Epoch 409/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3333e-06 - val_loss: 0.0010\n",
            "Epoch 410/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.2679e-06 - val_loss: 9.8512e-04\n",
            "Epoch 411/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5754e-06 - val_loss: 0.0010\n",
            "Epoch 412/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.7336e-06 - val_loss: 9.6616e-04\n",
            "Epoch 413/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0762e-06 - val_loss: 0.0010\n",
            "Epoch 414/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0816e-06 - val_loss: 9.8628e-04\n",
            "Epoch 415/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1134e-06 - val_loss: 9.8381e-04\n",
            "Epoch 416/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9609e-05 - val_loss: 0.0011\n",
            "Epoch 417/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1225e-05 - val_loss: 0.0010\n",
            "Epoch 418/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5219e-06 - val_loss: 9.8699e-04\n",
            "Epoch 419/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7043e-06 - val_loss: 0.0010\n",
            "Epoch 420/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9007e-06 - val_loss: 0.0010\n",
            "Epoch 421/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.6444e-06 - val_loss: 0.0010\n",
            "Epoch 422/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4451e-06 - val_loss: 0.0010\n",
            "Epoch 423/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9619e-06 - val_loss: 0.0010\n",
            "Epoch 424/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8622e-06 - val_loss: 0.0010\n",
            "Epoch 425/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6010e-06 - val_loss: 0.0010\n",
            "Epoch 426/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5365e-06 - val_loss: 0.0010\n",
            "Epoch 427/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5031e-06 - val_loss: 0.0010\n",
            "Epoch 428/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5854e-06 - val_loss: 0.0010\n",
            "Epoch 429/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1211e-06 - val_loss: 0.0010\n",
            "Epoch 430/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5661e-06 - val_loss: 0.0011\n",
            "Epoch 431/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7817e-06 - val_loss: 0.0010\n",
            "Epoch 432/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4217e-06 - val_loss: 0.0010\n",
            "Epoch 433/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2945e-06 - val_loss: 0.0010\n",
            "Epoch 434/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3685e-06 - val_loss: 0.0010\n",
            "Epoch 435/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3101e-06 - val_loss: 0.0010\n",
            "Epoch 436/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5794e-06 - val_loss: 0.0010\n",
            "Epoch 437/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6898e-06 - val_loss: 0.0010\n",
            "Epoch 438/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9007e-06 - val_loss: 0.0010\n",
            "Epoch 439/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4663e-06 - val_loss: 0.0010\n",
            "Epoch 440/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2038e-06 - val_loss: 0.0010\n",
            "Epoch 441/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9976e-06 - val_loss: 0.0010\n",
            "Epoch 442/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1776e-06 - val_loss: 0.0010\n",
            "Epoch 443/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3288e-06 - val_loss: 0.0010\n",
            "Epoch 444/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1567e-06 - val_loss: 0.0010\n",
            "Epoch 445/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2023e-06 - val_loss: 0.0010\n",
            "Epoch 446/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2612e-06 - val_loss: 9.9782e-04\n",
            "Epoch 447/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9421e-06 - val_loss: 0.0011\n",
            "Epoch 448/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3865e-06 - val_loss: 0.0010\n",
            "Epoch 449/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1731e-06 - val_loss: 0.0010\n",
            "Epoch 450/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.4041e-06 - val_loss: 9.9616e-04\n",
            "Epoch 451/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8389e-06 - val_loss: 0.0010\n",
            "Epoch 452/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1100e-06 - val_loss: 0.0010\n",
            "Epoch 453/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8382e-06 - val_loss: 0.0010\n",
            "Epoch 454/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0331e-05 - val_loss: 0.0011\n",
            "Epoch 455/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7019e-06 - val_loss: 0.0010\n",
            "Epoch 456/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6266e-06 - val_loss: 0.0010\n",
            "Epoch 457/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8878e-06 - val_loss: 9.7885e-04\n",
            "Epoch 458/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6086e-05 - val_loss: 7.7052e-04\n",
            "Epoch 459/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5499e-05 - val_loss: 8.6628e-04\n",
            "Epoch 460/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1340e-05 - val_loss: 0.0011\n",
            "Epoch 461/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.0625e-05 - val_loss: 0.0010\n",
            "Epoch 462/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4075e-05 - val_loss: 0.0010\n",
            "Epoch 463/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4073e-05 - val_loss: 7.1719e-04\n",
            "Epoch 464/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9338e-05 - val_loss: 0.0011\n",
            "Epoch 465/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2203e-05 - val_loss: 9.5947e-04\n",
            "Epoch 466/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3641e-05 - val_loss: 0.0010\n",
            "Epoch 467/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2645e-05 - val_loss: 9.4586e-04\n",
            "Epoch 468/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7180e-06 - val_loss: 0.0011\n",
            "Epoch 469/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2626e-05 - val_loss: 0.0010\n",
            "Epoch 470/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5784e-05 - val_loss: 9.6820e-04\n",
            "Epoch 471/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.2613e-06 - val_loss: 0.0010\n",
            "Epoch 472/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.6662e-06 - val_loss: 0.0010\n",
            "Epoch 473/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2065e-05 - val_loss: 0.0011\n",
            "Epoch 474/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6721e-06 - val_loss: 9.7992e-04\n",
            "Epoch 475/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0421e-05 - val_loss: 0.0010\n",
            "Epoch 476/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0497e-05 - val_loss: 9.6268e-04\n",
            "Epoch 477/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4726e-06 - val_loss: 9.5376e-04\n",
            "Epoch 478/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4710e-06 - val_loss: 9.7449e-04\n",
            "Epoch 479/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5445e-05 - val_loss: 0.0010\n",
            "Epoch 480/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1550e-05 - val_loss: 9.2020e-04\n",
            "Epoch 481/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3218e-06 - val_loss: 0.0011\n",
            "Epoch 482/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8984e-06 - val_loss: 9.6789e-04\n",
            "Epoch 483/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0826e-06 - val_loss: 0.0010\n",
            "Epoch 484/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7153e-06 - val_loss: 9.6922e-04\n",
            "Epoch 485/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0199e-06 - val_loss: 9.8568e-04\n",
            "Epoch 486/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6771e-06 - val_loss: 9.7940e-04\n",
            "Epoch 487/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2968e-06 - val_loss: 9.7917e-04\n",
            "Epoch 488/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2036e-06 - val_loss: 9.7071e-04\n",
            "Epoch 489/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0869e-07 - val_loss: 9.7406e-04\n",
            "Epoch 490/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.6176e-07 - val_loss: 9.7956e-04\n",
            "Epoch 491/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4317e-07 - val_loss: 9.8602e-04\n",
            "Epoch 492/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0241e-07 - val_loss: 9.7651e-04\n",
            "Epoch 493/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4134e-07 - val_loss: 9.8954e-04\n",
            "Epoch 494/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4943e-07 - val_loss: 9.7487e-04\n",
            "Epoch 495/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1170e-07 - val_loss: 9.7288e-04\n",
            "Epoch 496/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9072e-07 - val_loss: 9.8772e-04\n",
            "Epoch 497/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7387e-07 - val_loss: 9.6850e-04\n",
            "Epoch 498/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.9762e-07 - val_loss: 9.8058e-04\n",
            "Epoch 499/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7524e-07 - val_loss: 9.6736e-04\n",
            "Epoch 500/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9743e-07 - val_loss: 9.9362e-04\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2a20c5a60>"
            ]
          },
          "execution_count": 101,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 500\n",
        "normalizer = Normalization(axis=-1)\n",
        "model_dnn4 = Sequential(normalizer) # scale the input variables\n",
        "model_dnn4.add(Dense(68, activation=act_fn)) \n",
        "model_dnn4.add(Dense(34, activation=act_fn))\n",
        "model_dnn4.add(Dense(12, activation=act_fn))\n",
        "model_dnn4.add(Dense(6, activation=act_fn))\n",
        "model_dnn4.add(Dense(1)) # output layer\n",
        "model_dnn4.compile(loss=loss, optimizer=optimizer) # define the loss and optimizer\n",
        "model_dnn4.fit(X_train, y_train, validation_data = (X_valid, y_valid), epochs=epochs) #fit the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "KsRdQSV5QofA"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/500\n",
            " 9/23 [==========>...................] - ETA: 0s - loss: 0.0175"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 17:15:39.043019: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 1s 12ms/step - loss: 0.0078 - val_loss: 0.0012\n",
            "Epoch 2/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 9.0249e-04 - val_loss: 0.0011\n",
            "Epoch 3/500\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 17:15:39.365355: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "23/23 [==============================] - 0s 6ms/step - loss: 8.7908e-04 - val_loss: 0.0011\n",
            "Epoch 4/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6685e-04 - val_loss: 0.0011\n",
            "Epoch 5/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5322e-04 - val_loss: 0.0010\n",
            "Epoch 6/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.4480e-04 - val_loss: 0.0011\n",
            "Epoch 7/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.3328e-04 - val_loss: 0.0011\n",
            "Epoch 8/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.2147e-04 - val_loss: 0.0010\n",
            "Epoch 9/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1283e-04 - val_loss: 0.0010\n",
            "Epoch 10/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0342e-04 - val_loss: 0.0010\n",
            "Epoch 11/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9634e-04 - val_loss: 0.0011\n",
            "Epoch 12/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.9258e-04 - val_loss: 0.0011\n",
            "Epoch 13/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.7794e-04 - val_loss: 0.0012\n",
            "Epoch 14/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.6771e-04 - val_loss: 0.0011\n",
            "Epoch 15/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.5059e-04 - val_loss: 0.0012\n",
            "Epoch 16/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.2994e-04 - val_loss: 0.0013\n",
            "Epoch 17/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8587e-04 - val_loss: 0.0013\n",
            "Epoch 18/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.8380e-04 - val_loss: 0.0013\n",
            "Epoch 19/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.6234e-04 - val_loss: 0.0013\n",
            "Epoch 20/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.3278e-04 - val_loss: 0.0014\n",
            "Epoch 21/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.2710e-04 - val_loss: 0.0015\n",
            "Epoch 22/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8106e-04 - val_loss: 0.0014\n",
            "Epoch 23/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.4837e-04 - val_loss: 0.0013\n",
            "Epoch 24/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1580e-04 - val_loss: 0.0014\n",
            "Epoch 25/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.8672e-04 - val_loss: 0.0012\n",
            "Epoch 26/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.6270e-04 - val_loss: 0.0013\n",
            "Epoch 27/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.4066e-04 - val_loss: 0.0012\n",
            "Epoch 28/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.1396e-04 - val_loss: 0.0012\n",
            "Epoch 29/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8739e-04 - val_loss: 0.0011\n",
            "Epoch 30/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6167e-04 - val_loss: 0.0011\n",
            "Epoch 31/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6206e-04 - val_loss: 9.9277e-04\n",
            "Epoch 32/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.6670e-04 - val_loss: 0.0011\n",
            "Epoch 33/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2868e-04 - val_loss: 9.2806e-04\n",
            "Epoch 34/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9718e-04 - val_loss: 0.0011\n",
            "Epoch 35/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.8034e-04 - val_loss: 0.0011\n",
            "Epoch 36/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.6203e-04 - val_loss: 0.0011\n",
            "Epoch 37/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5882e-04 - val_loss: 0.0011\n",
            "Epoch 38/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.4540e-04 - val_loss: 0.0012\n",
            "Epoch 39/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4593e-04 - val_loss: 0.0012\n",
            "Epoch 40/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1949e-04 - val_loss: 0.0012\n",
            "Epoch 41/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1065e-04 - val_loss: 0.0014\n",
            "Epoch 42/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.0207e-04 - val_loss: 0.0011\n",
            "Epoch 43/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.9529e-04 - val_loss: 0.0014\n",
            "Epoch 44/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7976e-04 - val_loss: 0.0014\n",
            "Epoch 45/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.8212e-04 - val_loss: 0.0013\n",
            "Epoch 46/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9832e-04 - val_loss: 0.0013\n",
            "Epoch 47/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1377e-04 - val_loss: 0.0014\n",
            "Epoch 48/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.8514e-04 - val_loss: 0.0015\n",
            "Epoch 49/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6568e-04 - val_loss: 0.0013\n",
            "Epoch 50/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4413e-04 - val_loss: 0.0013\n",
            "Epoch 51/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3700e-04 - val_loss: 0.0014\n",
            "Epoch 52/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4168e-04 - val_loss: 0.0015\n",
            "Epoch 53/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3781e-04 - val_loss: 0.0015\n",
            "Epoch 54/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6059e-04 - val_loss: 0.0013\n",
            "Epoch 55/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.8236e-04 - val_loss: 0.0012\n",
            "Epoch 56/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4609e-04 - val_loss: 0.0011\n",
            "Epoch 57/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0153e-04 - val_loss: 0.0014\n",
            "Epoch 58/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7507e-04 - val_loss: 0.0015\n",
            "Epoch 59/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.5192e-04 - val_loss: 0.0014\n",
            "Epoch 60/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4179e-04 - val_loss: 0.0014\n",
            "Epoch 61/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3206e-04 - val_loss: 0.0014\n",
            "Epoch 62/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2329e-04 - val_loss: 0.0015\n",
            "Epoch 63/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0678e-04 - val_loss: 0.0014\n",
            "Epoch 64/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0143e-04 - val_loss: 0.0014\n",
            "Epoch 65/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.3410e-05 - val_loss: 0.0014\n",
            "Epoch 66/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.1522e-05 - val_loss: 0.0015\n",
            "Epoch 67/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9791e-05 - val_loss: 0.0016\n",
            "Epoch 68/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.8739e-05 - val_loss: 0.0016\n",
            "Epoch 69/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.0144e-05 - val_loss: 0.0016\n",
            "Epoch 70/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.0655e-05 - val_loss: 0.0016\n",
            "Epoch 71/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.4457e-05 - val_loss: 0.0016\n",
            "Epoch 72/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.4665e-05 - val_loss: 0.0016\n",
            "Epoch 73/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.1414e-05 - val_loss: 0.0017\n",
            "Epoch 74/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.5005e-05 - val_loss: 0.0016\n",
            "Epoch 75/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.9306e-05 - val_loss: 0.0017\n",
            "Epoch 76/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.2031e-05 - val_loss: 0.0016\n",
            "Epoch 77/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.2032e-05 - val_loss: 0.0018\n",
            "Epoch 78/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.0773e-05 - val_loss: 0.0017\n",
            "Epoch 79/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1363e-04 - val_loss: 0.0019\n",
            "Epoch 80/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.1528e-04 - val_loss: 0.0018\n",
            "Epoch 81/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7623e-05 - val_loss: 0.0021\n",
            "Epoch 82/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.9032e-05 - val_loss: 0.0017\n",
            "Epoch 83/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7237e-05 - val_loss: 0.0019\n",
            "Epoch 84/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4856e-05 - val_loss: 0.0019\n",
            "Epoch 85/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6016e-05 - val_loss: 0.0019\n",
            "Epoch 86/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 5.1559e-05 - val_loss: 0.0019\n",
            "Epoch 87/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3045e-05 - val_loss: 0.0018\n",
            "Epoch 88/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3620e-05 - val_loss: 0.0018\n",
            "Epoch 89/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 4.4117e-05 - val_loss: 0.0018\n",
            "Epoch 90/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5795e-05 - val_loss: 0.0019\n",
            "Epoch 91/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2746e-05 - val_loss: 0.0019\n",
            "Epoch 92/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.5011e-05 - val_loss: 0.0020\n",
            "Epoch 93/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.2035e-05 - val_loss: 0.0020\n",
            "Epoch 94/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.6391e-05 - val_loss: 0.0016\n",
            "Epoch 95/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0228e-04 - val_loss: 0.0018\n",
            "Epoch 96/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.5636e-05 - val_loss: 0.0017\n",
            "Epoch 97/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6583e-05 - val_loss: 0.0019\n",
            "Epoch 98/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2738e-05 - val_loss: 0.0018\n",
            "Epoch 99/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.1428e-05 - val_loss: 0.0018\n",
            "Epoch 100/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.0231e-05 - val_loss: 0.0018\n",
            "Epoch 101/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3512e-05 - val_loss: 0.0020\n",
            "Epoch 102/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1085e-05 - val_loss: 0.0019\n",
            "Epoch 103/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8797e-05 - val_loss: 0.0019\n",
            "Epoch 104/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2511e-05 - val_loss: 0.0019\n",
            "Epoch 105/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7566e-05 - val_loss: 0.0019\n",
            "Epoch 106/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5754e-05 - val_loss: 0.0019\n",
            "Epoch 107/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.4258e-05 - val_loss: 0.0019\n",
            "Epoch 108/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4240e-05 - val_loss: 0.0019\n",
            "Epoch 109/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3083e-05 - val_loss: 0.0019\n",
            "Epoch 110/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2736e-05 - val_loss: 0.0020\n",
            "Epoch 111/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1714e-05 - val_loss: 0.0020\n",
            "Epoch 112/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0728e-05 - val_loss: 0.0019\n",
            "Epoch 113/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.1256e-05 - val_loss: 0.0020\n",
            "Epoch 114/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2142e-05 - val_loss: 0.0020\n",
            "Epoch 115/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0832e-05 - val_loss: 0.0019\n",
            "Epoch 116/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0759e-05 - val_loss: 0.0020\n",
            "Epoch 117/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6549e-05 - val_loss: 0.0018\n",
            "Epoch 118/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.6073e-05 - val_loss: 0.0019\n",
            "Epoch 119/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9566e-05 - val_loss: 0.0020\n",
            "Epoch 120/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.1431e-05 - val_loss: 0.0016\n",
            "Epoch 121/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0308e-04 - val_loss: 0.0017\n",
            "Epoch 122/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.0770e-04 - val_loss: 0.0019\n",
            "Epoch 123/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6774e-05 - val_loss: 0.0016\n",
            "Epoch 124/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 3.0320e-05 - val_loss: 0.0016\n",
            "Epoch 125/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5258e-05 - val_loss: 0.0018\n",
            "Epoch 126/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4040e-05 - val_loss: 0.0018\n",
            "Epoch 127/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7101e-05 - val_loss: 0.0019\n",
            "Epoch 128/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4250e-05 - val_loss: 0.0018\n",
            "Epoch 129/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 1.2096e-05 - val_loss: 0.0019\n",
            "Epoch 130/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0025e-05 - val_loss: 0.0019\n",
            "Epoch 131/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.8234e-06 - val_loss: 0.0018\n",
            "Epoch 132/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5650e-06 - val_loss: 0.0019\n",
            "Epoch 133/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 8.7646e-06 - val_loss: 0.0020\n",
            "Epoch 134/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.3374e-06 - val_loss: 0.0020\n",
            "Epoch 135/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0337e-06 - val_loss: 0.0019\n",
            "Epoch 136/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3711e-06 - val_loss: 0.0019\n",
            "Epoch 137/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3010e-06 - val_loss: 0.0019\n",
            "Epoch 138/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 7.0171e-06 - val_loss: 0.0019\n",
            "Epoch 139/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.7071e-04 - val_loss: 0.0020\n",
            "Epoch 140/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0277e-04 - val_loss: 0.0021\n",
            "Epoch 141/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0133e-04 - val_loss: 0.0013\n",
            "Epoch 142/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5550e-05 - val_loss: 0.0016\n",
            "Epoch 143/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.8078e-05 - val_loss: 0.0014\n",
            "Epoch 144/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.7543e-05 - val_loss: 0.0014\n",
            "Epoch 145/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.5466e-05 - val_loss: 0.0014\n",
            "Epoch 146/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8503e-05 - val_loss: 0.0014\n",
            "Epoch 147/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.5271e-05 - val_loss: 0.0014\n",
            "Epoch 148/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3258e-05 - val_loss: 0.0015\n",
            "Epoch 149/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0117e-05 - val_loss: 0.0015\n",
            "Epoch 150/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3677e-05 - val_loss: 0.0015\n",
            "Epoch 151/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.2923e-05 - val_loss: 0.0016\n",
            "Epoch 152/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.1415e-05 - val_loss: 0.0017\n",
            "Epoch 153/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6306e-05 - val_loss: 0.0017\n",
            "Epoch 154/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0330e-05 - val_loss: 0.0017\n",
            "Epoch 155/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.9505e-06 - val_loss: 0.0015\n",
            "Epoch 156/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.3090e-06 - val_loss: 0.0017\n",
            "Epoch 157/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7291e-06 - val_loss: 0.0017\n",
            "Epoch 158/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5518e-06 - val_loss: 0.0017\n",
            "Epoch 159/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.5074e-06 - val_loss: 0.0017\n",
            "Epoch 160/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.0659e-06 - val_loss: 0.0017\n",
            "Epoch 161/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.9940e-06 - val_loss: 0.0017\n",
            "Epoch 162/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3994e-06 - val_loss: 0.0017\n",
            "Epoch 163/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4746e-06 - val_loss: 0.0017\n",
            "Epoch 164/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9822e-06 - val_loss: 0.0017\n",
            "Epoch 165/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.9709e-06 - val_loss: 0.0017\n",
            "Epoch 166/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4322e-06 - val_loss: 0.0017\n",
            "Epoch 167/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7055e-06 - val_loss: 0.0018\n",
            "Epoch 168/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5149e-06 - val_loss: 0.0017\n",
            "Epoch 169/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3655e-06 - val_loss: 0.0017\n",
            "Epoch 170/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6334e-06 - val_loss: 0.0017\n",
            "Epoch 171/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2041e-06 - val_loss: 0.0017\n",
            "Epoch 172/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.8675e-06 - val_loss: 0.0017\n",
            "Epoch 173/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0699e-06 - val_loss: 0.0017\n",
            "Epoch 174/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.8056e-06 - val_loss: 0.0018\n",
            "Epoch 175/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.6301e-06 - val_loss: 0.0017\n",
            "Epoch 176/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4522e-06 - val_loss: 0.0017\n",
            "Epoch 177/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2728e-05 - val_loss: 0.0014\n",
            "Epoch 178/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5726e-05 - val_loss: 0.0012\n",
            "Epoch 179/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.1264e-05 - val_loss: 0.0013\n",
            "Epoch 180/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0680e-05 - val_loss: 0.0013\n",
            "Epoch 181/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.4868e-05 - val_loss: 0.0012\n",
            "Epoch 182/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.7230e-05 - val_loss: 0.0014\n",
            "Epoch 183/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.3082e-05 - val_loss: 0.0012\n",
            "Epoch 184/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3526e-05 - val_loss: 0.0013\n",
            "Epoch 185/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8293e-05 - val_loss: 0.0013\n",
            "Epoch 186/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2118e-05 - val_loss: 0.0013\n",
            "Epoch 187/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.2846e-05 - val_loss: 0.0014\n",
            "Epoch 188/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9968e-05 - val_loss: 0.0012\n",
            "Epoch 189/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3130e-05 - val_loss: 0.0013\n",
            "Epoch 190/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.6249e-06 - val_loss: 0.0012\n",
            "Epoch 191/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.4752e-05 - val_loss: 0.0013\n",
            "Epoch 192/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.4325e-06 - val_loss: 0.0013\n",
            "Epoch 193/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.9119e-06 - val_loss: 0.0013\n",
            "Epoch 194/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.5624e-06 - val_loss: 0.0013\n",
            "Epoch 195/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.5322e-06 - val_loss: 0.0013\n",
            "Epoch 196/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.4944e-06 - val_loss: 0.0012\n",
            "Epoch 197/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 9.6890e-06 - val_loss: 0.0013\n",
            "Epoch 198/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0601e-05 - val_loss: 0.0012\n",
            "Epoch 199/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.3039e-05 - val_loss: 0.0012\n",
            "Epoch 200/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 8.9571e-06 - val_loss: 0.0012\n",
            "Epoch 201/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.7322e-06 - val_loss: 0.0012\n",
            "Epoch 202/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 6.1542e-06 - val_loss: 0.0012\n",
            "Epoch 203/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.2967e-05 - val_loss: 0.0011\n",
            "Epoch 204/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.3374e-05 - val_loss: 0.0011\n",
            "Epoch 205/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0546e-05 - val_loss: 0.0011\n",
            "Epoch 206/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.2387e-05 - val_loss: 0.0012\n",
            "Epoch 207/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8251e-05 - val_loss: 0.0012\n",
            "Epoch 208/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.0124e-05 - val_loss: 0.0012\n",
            "Epoch 209/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 7.1910e-06 - val_loss: 0.0012\n",
            "Epoch 210/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.2514e-06 - val_loss: 0.0013\n",
            "Epoch 211/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.5271e-06 - val_loss: 0.0012\n",
            "Epoch 212/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.0585e-06 - val_loss: 0.0012\n",
            "Epoch 213/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 6.4293e-06 - val_loss: 0.0012\n",
            "Epoch 214/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.8877e-06 - val_loss: 0.0012\n",
            "Epoch 215/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.2896e-06 - val_loss: 0.0012\n",
            "Epoch 216/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 5.7425e-06 - val_loss: 0.0012\n",
            "Epoch 217/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 4.0237e-06 - val_loss: 0.0012\n",
            "Epoch 218/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.3387e-06 - val_loss: 0.0012\n",
            "Epoch 219/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.7831e-06 - val_loss: 0.0012\n",
            "Epoch 220/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 3.0525e-06 - val_loss: 0.0012\n",
            "Epoch 221/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.9161e-06 - val_loss: 0.0012\n",
            "Epoch 222/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.1881e-06 - val_loss: 0.0012\n",
            "Epoch 223/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.4996e-06 - val_loss: 0.0012\n",
            "Epoch 224/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9821e-06 - val_loss: 0.0012\n",
            "Epoch 225/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8848e-06 - val_loss: 0.0012\n",
            "Epoch 226/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.6163e-06 - val_loss: 0.0012\n",
            "Epoch 227/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3150e-06 - val_loss: 0.0012\n",
            "Epoch 228/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.0946e-06 - val_loss: 0.0012\n",
            "Epoch 229/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 2.3280e-06 - val_loss: 0.0012\n",
            "Epoch 230/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9384e-06 - val_loss: 0.0012\n",
            "Epoch 231/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.8187e-06 - val_loss: 0.0012\n",
            "Epoch 232/500\n",
            "23/23 [==============================] - 0s 6ms/step - loss: 1.9573e-06 - val_loss: 0.0012\n",
            "Epoch 233/500\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 2.3427e-06 - val_loss: 0.0012\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x2a6706a30>"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "epochs = 500\n",
        "normalizer = Normalization(axis=-1)\n",
        "model_dnn4_es = Sequential(normalizer) # scale the input variables\n",
        "model_dnn4_es.add(Dense(68, activation=act_fn)) \n",
        "model_dnn4_es.add(Dense(34, activation=act_fn))\n",
        "model_dnn4_es.add(Dense(12, activation=act_fn))\n",
        "model_dnn4_es.add(Dense(6, activation=act_fn))\n",
        "model_dnn4_es.add(Dense(1)) # output layer\n",
        "model_dnn4_es.compile(loss=loss, optimizer=optimizer) # define the loss and optimizer\n",
        "model_dnn4_es.fit(X_train, y_train, validation_data = (X_valid,y_valid), epochs=epochs, callbacks=[early_stop]) #fit the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWcaf9omSXNJ",
        "outputId": "469db7ac-81da-45b2-917e-d5a051198655"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0673\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.067254438996315"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dnn4.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPTnlUNISdEJ",
        "outputId": "a569484f-8fc5-4250-a6e7-f9794d98bed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0645\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.06450894474983215"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_dnn4_es.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "ky8J8MMV0PLN"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 20:00:54.726264: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
          ]
        }
      ],
      "source": [
        "y_pred_nn4 = model_dnn4.predict(X_test)\n",
        "y_pred_nn4_es = model_dnn4_es.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0.06175392],\n",
              "       [-0.04586355],\n",
              "       [-0.25055888],\n",
              "       [ 0.00509609],\n",
              "       [-0.42551202],\n",
              "       [-0.49169683],\n",
              "       [-0.0125751 ],\n",
              "       [ 0.08308136],\n",
              "       [ 0.0407354 ],\n",
              "       [-0.00069477],\n",
              "       [ 0.00719424],\n",
              "       [ 0.03542385]], dtype=float32)"
            ]
          },
          "execution_count": 103,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_nn4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "OUiuHJ780RrI",
        "outputId": "87e5a908-1e58-475f-87f0-dd1789c58834"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'NeuralNet Test data')"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAEICAYAAABcVE8dAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAABTS0lEQVR4nO3dd3hUVfrA8e+Z9N4TUkhPqIFI7x2lF0WFta2ube1dLLuKq2vv7aeCKzYsKIJU6U3pRVqYVEiAFNJ7mTm/P+4QKQGSzCSTcj7Pk2favee+E8I7d8459z1CSomiKIrS9umsHYCiKIrSPFTCVxRFaSdUwlcURWknVMJXFEVpJ1TCVxRFaSdUwlcURWknVMJX2iwhxPNCiK+tHUdzEUKMEEJkWDsOpeVSCV+xGCFEmhAiWwjhctZztwshNlgxrDNxhAshpBBi+XnPfy2EeL6ebaQJIcZc5LUbhBAlpp9yIYTxrMclZsRr29B969n+34UQW5qibaXlUglfsTQb4MGmPogZibC/EGKQRYMBpJTfSCldpZSuwHjg5JnHpucUxepUwlcs7XXgMSGEZ10vCiE6CyFWCyHyhBBHhRDXnfXaBiHE7Wc9Pucs1HTGe68QIhFIND33rhAiXQhRJITYLYQYepn4XgNeutiLQohJQoh9QogCIcTvQogepue/AkKBX01n7U9c9jfxV5tBQoifhBA5QohUIcQDZ73WTwixyxR/lhDiLdNLm0y3BabjDayjXSchxBdCiHwhxGGg73mvzxZCJAshioUQh4UQ003PdwH+DxhoarvA9PxEIcReUyzp9f3mo7QeKuErlrYL2AA8dv4Lpq6e1cC3gD8wE/hICNG1Ae1PA/oDZ/bZCcQD3qZ2fxRCOF5i/4+A2Lq6ZoQQVwCfA3cBPsAnwBIhhIOU8ibgODDZdNb+Wn2CFULogF+B/UAwMBp4SAhxlWmTd4F3pZTuQBTwg+n5YaZbT9Px/qij+edM+0QBVwG3nPd6MjAU8ADmAF8LIQKllEeAu4E/TG17mrYvBW4GPIGJwD+FENPq8z6V1kElfKUp/Bu4Xwjhd97zk4A0KeX/pJQ1Usq9wE/AtQ1o+2UpZZ6UshxASvm1lDLX1N6bgAPQ6RL7l6Od4b9Yx2t3Ap9IKbdLKQ1SyvlAJTCgAfGdry/gJ6V8QUpZJaVMAT5D+7ADqAaihRC+UsoSKeW2BrR9HfCS6feRDrx39otSyh+llCellEYp5fdo34r6XawxKeUGKeUB0/Z/AguA4Q2IR2nhVMJXLE5KeRBYCsw+76UwtD70gjM/wA1AhwY0n372AyHEY0KII0KIQlN7HoDvZdqYCwQIISbXEd+j58XXEQhqQHznCwOCzmvzaSDA9Po/gFggQQixUwgxqQFtB3Hu7+PY2S8KIW4+q3uqAOjOJX43Qoj+Qoj1pq6nQrRvAZf7XSqtSJPMAFAUtO6GPcCbZz2XDmyUUo69yD6lgPNZj+v6IKgt72rqr38CrZvkkJTSKITIB8SlApNSVgkh5gD/AQ6dF99LUsqL9fE3prRsOpAqpYy5SCyJwCxT18/VwEIhhE89j3UK7QPpzHsIPfOCECIM7ZvEaLSuG4MQYh9//W7qav9b4ANgvJSyQgjxDirhtynqDF9pElLKJOB74IGznl6K1n9+kxDCzvTT1zSICLAPuFoI4SyEiEY7+70UN6AGyAFshRD/BtzrGeJXgCMw7qznPgPuNp3pCiGEi2kg0830ehYQWc/2z9gBFAshnjQNstoIIboLIfoCCCFuFEL4SSmNQIFpH6PpPRkvc7wfgKeEEF5CiBDg/rNec0FL6jmm49yKdoZ/RhYQIoSwP+s5NyDPlOz7AX9r4HtVWjiV8JWm9AJa4gFASlkMXInWf30SyAReRet3B3gbqEJLRvOBby7T/ipgJaBH686o4Lwun4uRUhrQxhq8z3puF3AH2lluPpAE/P2s3V4GnjV1kVwwKH2J40xCG1hOBU6jdSl5mDYZBxwS2lz9d4GZUspyKWUZ2ljDVtPx6hpHmIP2vlOB39A+xM4c9zDat6s/0H6fccDWs/Zdh/bNIFMIcdr03D3AC0KIYtPv5geUNkWoBVAURVHaB3WGryiK0k6ohK8oitJOqISvKIrSTqiEryiK0k602Hn4vr6+Mjw83NphKIqitCq7d+8+LaU8/yp3oAUn/PDwcHbt2mXtMBRFUVoVIcSxi72munQURVHaCZXwFUVR2gmV8BVFUdoJlfAVRVHaCZXwFUVR2gmV8BVFUdoJlfAVRVHaCZXwlYuSUvLDrnSOZhZbOxRFUSygxV54pVjfkv0neWLhnwgB1/QK4ZGxsQR5Olk7LEVRGkmd4St1Kq2s4b/LjxAX7MEdQyNZsu8kI9/YwCsrEigsr7Z2eIqiNII6w1fq9P66JLKKKvm/G3tzRagXNw8M463f9HyyKZnvdh7nvpHR3DQwDAdbG2uHqihKPakzfOUCyTklzNuSwrW9Q7gi1AuAEC9n3ro+nmX3D6VHiCcvLjvCqDc2smhvBkajWjVNabt+2XuCye9v4fMtqZRW1lg7HLO02CUO+/TpI1XxtOYnpeSW/+1k7/F81j06Aj83hzq325p0mpdXHOHgiSK6Brrz1ITODI2ps0CforRan21K4aXlR/B1deB0SSXujrbcOCCMvw8Kx9/d0drh1UkIsVtK2aeu19QZvnKO1Yez2KTP4eExsRdN9gCDo31Zcu8Q3p0ZT1FFNTfN28FN87Zz8ERhM0arKE1DSsnLy4/w0vIjTOwRyNbZI/n5nkEMjvbl443JDH51HY//uB99VuuawabO8JVaFdUGxry1ERd7W5Y9MARbm/qdD1TWGPh623E+WJdIflk10+KDePTKTnT0dm7iiBXF8qoNRmb/dICf9mRw88AwnpvcDRudqH39WG4p87ak8sOudCqqjYzs5McdwyIZGOmDEOISLTePS53hq4Sv1HpnjZ531iSy4I4BDIzyafD+RRXV/N+GZOZtSUVKuGlgGPeNjMbLxb4JolUUyyuvMnDvt3tYl5DNI2NjuX9U9EWTeH5pFV9vO8b8P9I4XVJF92B37hwWxYTuHep9stQU2l3CTytMI8w9rEV82rYW6XlljHlrI2O7BvDB33qZ1dapwnLeXq1n4e4MXBxsuWdENLcODsfRTs3oaRbl+fDbs3B4CYT0gegxED0WfGNA/Z+4qIKyKm77Yif70gv4z7Tu3NA/rF77VVQbWLT3BJ9tTiElp5RgTyduGxLB9X074urQ/BMh21XCTy9KZ9riaQwOHsyzA57F39m/CaJre+76aheb9KdZ99hwAj0sc3HV0cxiXluZwNqEbAI9HHl4bCzX9Ao55+uxYkFSwuFfYPkTUJYLXadA1iE4rdde9wz9K/lHDAMHV6uG25KcKizn5nk7OJZbxnuz4hnXPbDBbRiNkrUJ2Xy2KYUdaXm4O9pyg2mAN6AZB3jbVcKvMdbwzZFv+GDvB9jp7HikzyNcE3ONOtu/hE36HG7+fAePX9WJe0dGW7z9bSm5vLwigf3pBXQKcGP2+M6M6OSn/k0aSUpJUUUN2UUVZBVVklVUQUnOMfof+S+dC7eQZBvNi7p7SLKJ4PnJ3RgTWAFJayBpLaRuhKoS0NlB2EAt+UePAf8u7fbsPym7mJvn7aC4ooZPb+7TqO7M8+09ns9nm1NYeTATG51ganwwdwyNpFMHNwtEfGntKuEDkJdCuq0dz//xPDsyd9CvQz+eG/gcoe6hlg2yDaiqMTLu3U0YjZJVDw9rsguppJQsP5DJ66sSSMstY0CkN0+N70LPjp5NcrzWqqSyhqyiCrKKKsg2JfOsokqyiivIMd1mFVVQUW0EQGDkBpu1PGn7HbbCwHyHG9jqey1+7i4cPlVEQmYxtw2OYPb4ztjb6qCmCo7/YfoAWAPZh7UDuwdD9GjtAyByBDi6W++X0Iz2Hs/n1i92YqvTMf+2vnQL8rBo+8dyS/l8Syo/7MqgvNrAiE5+3Dk0koFRTTfA274Sfm4yfDwYukxGTniDn9PX8OauN6kyVnFv/L3c1PUmbHXqAuMzPtmYzMsrEvjfrX0Z2anpu7+qDUYW7DjOu2sSyS2tYmKPQJ64qhNhPi5NfmxrklJyqrCCjPzyvxJ6ceUFyb20ynDBvk52NnTwcMTfzYEAd0cC3LXbSE7Q98/ncMvZjSF8ODZT3gXviNr9KqoNvLz8CPP/OEaPEA/en3XFhb/nwgztzD9pNaRshMoi0NlCx/6m7p8x0CGuTZ79bziazT+/3oO/uwNf3tavSf8G6xrgvWNoJBPiArGz8ABv+0r4RgNsfhM2vAIeIXDNPLJ9wnhp20usS19HV5+uvDDoBTp5d7J80K1MVlEFo97YwMAoH+be0rdZj11SWcOnm1L4bFMK1QYjN/QP5f7RMfi6Xnzuf2sipST1dCk7UvPYnprH9pRcThZWnLONva1OS95ujgS4O+LvflZCd3PE33Tf1cH23LPBmirY+g5seh3snGHcy9Bz1kWT8sqDmTyxcD9GCS9fHcfknkF1B22ohvQdWvJPWgOZB7TnXTuYkv9oiBoJTl4W+A1Z1y97T/DYj/vp1MGNL27td8lrTqqN1WQUZxDkGoSDjXl/nxXVBn7Ze4JPzxrgvXVwODP7hVpsgLd9Jfwzjm+Hn26HohMw8mnk4IdYnb6Ol7a/RFFlEbd2v5W7et5l9j9ga/bQd3tZfjCT1Q8Ps9oZdnZRBe+sTeT7nek42dnw90HhDIj0oUugGz6tKPlLKUnMLmF7Sq6W4FPzyCmuBMDX1Z7+ET70i/AmwtelNql7ONk1/Gt9+k5Ycj/kHIHu18C4V8D18t/MMvLLeGDBXvYcL2BWv478e1I3nOwv031XnPnX2X/yOqgoBKGDkH7aB0DMGOjQE3St6/rNuZtTeHHZEQZF+fDJTb1xc7S7YBuD0cCe7D0sT13O6mOrKawsRCd0hLiGEOkZSZRHVO1thEcEznYNu+bEaJSsS8jm080p7EjNw83Rlhv6h3HrYPMHeNtnwgcoL4Blj8DBnyB8KEz/hEJHV17f+TqLkxcT7h7OnEFz6BVg3jTE1mhHah7XffIH94+K5tErrf9tJym7hNdXJbDqUFbtc/5uDnQJdDf9uNE10J0IXxerznE+w2CUJGQWsT0lj+2puexMyyevtAqADu6O9I/0pn+ED/0jvYn0dTG/v7ayBNb9B7Z/Au5BMPEt6DSuQU1UG4y8tVrPxxuSiQ1w5YO/9SI2oJ6DiIYaOLFbS/6Jq+HUPgCMLn6sD41nrq6YQgHDQkcyPGQ4fQL6YGdzYSK1Jiklr6xM4JONKUyI68Db18efM2YlpeTA6QOsSF3BqrRV5JTn4GTrxIiOIxgQOIDM0kySC5JJKUwhrSiNGuNfdXWCXIJqPwCiPLUPgUjPSNztLz8Wsi+9gM82pbDi4CmLDPC234QP2lS1fd/C8sfBxg6mfgBdJvP7id95YdsLnCg5wcxOM3mo90O42LXtfuQzagxGJr2/heKKGtY8MvzyZ3rNKK+0iiOnijhyqojDp4o4cqqYpOxiqg3a36mDrY5OHdzo0kH7EOgS6E7nQHc8nJo2udQYjBw6WcT21Fy2p+SxMy2PogrtP3xHb6faM/gBET509Hay7IBc4mpY+rDW397vDhj9b3Bo/GyPTfocHvlhHyWVNTw/uRvX9+3Y4HgNxZms3v0Rn6SvJMlYTmh1NWHVNexwcqJSgKuNI4OChzIidCRDg4fi6ejZ6HgtocZgZPbPB1i4O4MbB4QyZ0p3bHQCKSX6fD0r01ayInUFJ0pOYKezY2jwUMZHjGdYyLA6z96rjdWkF6eTUpBCSmFK7QdBamEqlYbK2u38nfy1DwLPKCI9Ion00O57OV7YLXY8t4zPt6SwbtcBOnvU8MmjNzbq76h9J/wzcpPhp3/Ayb3Q+1a46r+UCXh/7/t8c+QbAlwC+NeAfzEsZJjljtlCfflHGv9efIiPb+jF+LiGzzdublU1RpKyS2o/CI5kah8EZ86oAYI9negS6E7XIHe6mj4IOno5o2vknP+qGiN/ZhTUds/sTsurHVCN9HWpPYPvF+HddIvClJ6GlbPhwI/g2wmmvA+h/S3SdHZxBQ9/v4+tSblM7hnEf6d3r7Nr43w1xhpWpq3k0z8/JbUwlQiPCO7sfjvj7AOwTdlAefIatuUeZKOzIxudnTlto0OHIN67KyMirmJ4x+FEuEc065Tc8ioD9327h7UJ2Tw0JoYHR8dwvPg4K1JXsDJ1JcmFydgIGwYEDmBcxDhGh47Gzb5xH6gGo4GTJSdJLkyu/RBIKUghuTCZ8pry2u28Hb2J9IggysGXSKOOqPJiIvNO4Jt1GFGaQ5n/FTjfs6FRMaiEf0ZNFax/Eba+q/0HmjEPOsSxL3sfz//+PMmFyUyMnMiTfZ+s8xO4LcgtqWTkGxuIC/Hg63/0b7Vz4aWUZBdXmr4FFHH4pHaberqUM9WaXext6Bz41zeBLoHudO7ghrP9hYNjFdUG9h4vYHtqLjtS89hzPL926mNsgGtt90y/cO+mr5IoJfz5Pax8CiqLYdhjMORhsLXsmIbBKPl4QxJvrdbT0duZD2b1Ii6k7mmJ1cZqlqUsY+6BuRwrOka0ZzR39byLsaFjsdGd9w2xvABSN2JMXMOhY+vZYCxio7MTRx20Ehuhjr6MCBvLiPCxxPvHY6drum9nhWXV/GP+TnYfz+fxCQG4eB9ieepyDudq01F7+fdiQsQExoSNwcfJ/Pn3F2OsKiUrfSvJx7eQfPoQKSXppFQXkWxrQ/FZXZRu2BLl6EvvgF48NOLVRh1LJfzzJa+HRXdDeR6M/Q/0v4sqYzVzD8zlswOf4Wbnxux+sxkfMb7VJsSLeernP/lxVwYrHhxKTH37b1uR8ioD+qziv74NnNLuF5vqmAsBET4uteMCFdVGdqTmsS+9gCqDESGga6B77dl7vwhvvJuzFlB+mtZ9k7xOGxyd8p52UVQT2pmWxwML9nK6pJLZ47tw2+Dw2r/7akM1i5MXM/fAXE6UnKCzd2fu7nE3I0NHohP1GEuREk4nQvJaTiWuZGPufjY42LLDyZFqIXDT2TPEvw8jYqYwJGRovfq86yuzsIIb/reGE1XbiYlMIq30IADdfLoxPmI8V4VfRQeXDhY7Xq2yPMj8E079qc10yjygXe0sTVNuHdy1qa4deiADunPaJ5RkGx3Jxcdqu4j8nPx4bfhrjTq8Svh1KT0Ni+8D/QqIuRKmfgSufujz9Ty39TkO5h5keMhwnh3wbNP8UVjBnxkFTP1wK/8YHMGzk7paO5xmI6UkI7+89tvAmQ+C43ll2OgE3YM9GGBK7n3CvZt8PKBORgNs/z9Y96I2E2bM89DnH802Aya/tIrHF/7JmiNZjOniz4vTO7Ph5DLmHZxHZmkm3X26c3fPuxkWMsy8k6DqCjj+O6X6VfxxfB0bqk+z2dmJPBsbbBH0cg1jeOQERkRNbPSFkkVVRSw4uJyPd/1Ejb0eIYxEeUQxPmI84yPGW+4CTCmh4JiW0GuT+5/azMAz3INrkzsd4iCwB3iGNel1DU2e8IUQ44B3ARtgrpTylfNedwC+BHoDucD1Usq0S7XZLNUypYSdc2HVM+DoAdM/hugxGIwGvj7yNR/s/QAbnQ0P93qYaztdW78zmhbKaJRc/fHvZOSXs/6x4fXqr23riiuq0QmBixUKXJ0j86A21fLkHoi5Cia9pV1D0syklHy2+Shvbf8Se59NSJtC4v3iubvn3QwKGtQ033aLTmJIWsMB/a/a2b+9IMle+0YVYevGiMCBjOhyPT0Del/YdXSWsuoyNmVsYnnqcjZnbKFGVkOND9OiJ3BTj2nEesWaF2dNFZw+eu5Ze+YBqDSt/yB04Bt7bnLv0ANcmq6b6GKaNOELIWwAPTAWyAB2ArOklIfP2uYeoIeU8m4hxExgupTy+ku126zlkbMOwcJ/aHObB96nzYKwdSC9OJ05v89he+Z2evn3Ys6gOYR7hDdPTBb2w650nlj4J29d15Ore9UzmRiN2lln16ngEdy0AbZH1RWw6TVtTMnRE8a/qs2tt0I3Yll1GT/qf+R/B/9HbkUutlXRlGSN5IGB47hnZEzzFLwzGuDkPjISfmHj8XVsqMpml6MDNULgiQ1DPWIYHj2FwbHTcbV3pcpQxdYTW1mRuoINGRsorynHw86H/JyueBj68c1NM4jwdoSqYm1aa2WxVkeosuivx3U9V3XWa2cel+fDmWmYds4Q0O2s5N4DArqCXRMN3jdQUyf8gcDzUsqrTI+fApBSvnzWNqtM2/whhLAFMgE/eYmDm5Pw521JZXKPwIYNrlWXayVld87V/gFnfA6+MUgp+SXpF17f+TqVhkr+Gf9Pbul2S5MONFlaYXk1o97YQLivCwvvHlj/M7W9X8Pie6HvHTDxjaYNsr1J2wK/Pgi5SdDzb3DVS+Ds3exhlFaXsiBhAV8e+pL8ynwGBA7grh530cmzJ88sOsiS/ScZHO3D29fFN/+SfhWFlCSuYqt+ERtzD7LZ1kCBjQ22EnrYeZJoKKVYVuOJLVfq3BhYDP6ZhfjYVtLBsQabqhI4a2bMJdk6alNd7V21Wwd3rZromeecvExJvgf4RMElvm1YW1Mn/BnAOCnl7abHNwH9pZT3nbXNQdM2GabHyaZtTp/X1p3AnQChoaG9jx071uB4UnJKuPLtTdjb6rhjaCR3Dots2Ff2hOVakqup0M64rrgJhCCnLIf/bv8va46vobN3Z+YMmkNXn9bRDz7n10N88Xsav943hO7B9SwOVVEI7/eG0hxw8YNHEsBG1SAyW3kBrHkOdn+h9eVOfgeiRjV7GEVVRXx75Fu+OvwVRVVFDAkewl097iLeP752GyklP+xK57klh3B1sOXN6+IZHmuldYulxHD6KPsPfMuGjI1sr8wiurqG8VWC/sKZomoHkooEdk7uxEWFYOfkbkrWbqYE7nqRx6bbFnaRmDlaTcI/m1kLoJwu5fVVR1l24BS+rg48NCaGmX071v8KzaKTsOguSN0EXadp/ylN9UNWH1vNS9teoqCygFu63cI/e/4TR9uWuZgxaDXpJ7y3mVn9OvLitLj677jqGfjjQxj6iFab6MaftVoqSuMlLNdm4JRmw4B7YOTTYN+8F/sVVhby1eGv+ObIN5RUlzCi4wju7nE33Xy7XXQffVYx9327B31WCXcPj+LRK2MtXvCrwaQEoV049dqqo3y8IZlx3Trwzsz4dr/QTrvr0jljz/F8Xl5+hJ1p+UT6uTB7XGfGdg2oX5eG0Qi/v6vNmnALhKs/0+qHo/2neXPXmyxKWkSYexjPD3yePh3q/P1alZSSWZ9tIyGzmPWPjqj/UoM5evh4IMT/Dca/Dq9Ha4tpTPuoaQNuywrS4d0e4N9Vu4AquHnLeeRV5PHloS9ZkLCAspoyxoaN5c4ed9LZu3O99i+vMvDC0kMs2JFOr1BP3pt1BSFe1l2zuMZg5OlFB/hhVwZ/6x/Kf6Z2V4vrcOmEb4mP6Z1AjBAiQghhD8wElpy3zRLgFtP9GcC6SyV7S+kV6sUPdw3k05t6A3DnV7u5/pNt7D2ef/mddTrtYpd//KaVi/1iAqz/Lxhq8HDw4IXBL/Dp2E+pMdZwx293kF6U3sTvpuGW/nmKbSl5PH5Vp/oneym1qzvtXGDUv8HOEbpMgiO/aoOMSuMcXQ7SCNd92azJ/nT5ad7Y+QbjfhrH5wc/Z3jIcH6e8jNvjXir3skewMnehpev7sH7s65An1XChHc3s/JgZhNGfmkV1Qbu/noPP+zK4MHRMbw0TSX7+rDUtMwJwDto0zI/l1K+JIR4AdglpVwihHAEvgKuAPKAmVLKlEu1aelZOjUGI9/tTOedNXpOl1QxMS6QJ8bVsw57ZbFWi2f/Aq1O+NWfgZe23mVWaRbjfhrHDV1u4LG+j1ksXnOVVtYw+s2N+LrZs/jeIfX/z3B0BSyYCVe9DAPv0Z5LXAPfXAPXf6Mlf6Xh5k+G4iy4b0eTHqa8ppwTxSfIKMlg26ltLNQvpNpYzcSIidze43YiPSLNPsax3FLuX7CXPzMKuXlgGE9P6GLRbhSjUVJcUUNeWRV5pVXkl2q3eWV/3T9wopCjWcW8MKUbNw0Mt9ix2wJ14dVZSipr+GxTCp9uSqHGaOSG/mE8MDqmfldT/vmjVn0TAZPf1qbQAY9ueJRtp7ax5to1ONm2jKlZr61M4KMNyfz0z4H0Dqvn7I+aSviwP9jYwz+3/jWQZaiGNztp66Be+0WTxdxmleVp3WKDH4Qxz5nVlJSS3IpcMoozSC9OJ6M4g4ySv+7nlOfUbmsrbJkcNZnb4263+GpvVTVGXl2ZwLwtqXQNdOeDv11BpN+Fa+RKKSmvNpBbUkX+mQReVkVeabWWvE1JPNeU2PPLqsgvq8ZgrDsv2dvo8Haxx8fVnvtGRreKWlDN7VIJv91Nu3B1sOXhsbHc0D+Ut9fo+fKPNH7ancE/R0Zx2+CIS5+p9LgWOvbV6uwvvE2rFT7+NWZ1nsVvx35jReoKro65uvnezEWk5JTw2eYUrukVUv9kD9ogbX4q3LTo3FkLNnba4PW+b7W5ymrx64ZJ/E27rL5z/b4dVRmqyCjJ0JL5mcRuenyi5MQ5RbgEggCXAEJcQxgSPIQQtxBCXEPo6NaRUPdQPBwsu2TfGfa2Ov41qSuDonx47Mf9THp/C1PjgymprKk9Cz+T4CtrjHW2oRPg7WKPl7M9Xi72RPm54uVij7eLHV7OWlL3crav3cbbxR5ne5s2V+6kObW7M/zzJWYV8+rKBNYcySbQw5FHxsZyda+QS3eBGGpg46uw+Q3wikBe/RnX7HkJG2HDD5N+sOofpJSSW7/Yya60fNY9Nhx/t3rOICo6pU3DjBwBs7698PVjv8P/xmvdWT2us2jMbd73N0HGTnj4MOh0SCnJr8y/IKGfOUvPLstG8tf/SydbJ4Jdg+no1vGchB7iFmKRVZjMdaqwnCcW/sn+9AItObvY421K4j7nPT6TzL1d7HF3tGt0NVPl4lSXTj38kZzLyyuO8GdGIZ07uPHUhC6Xn3OcthV+vgNKc/jhqtn858gXfDX+q3PmMje3NYezuP3LXTw7sQu3D21Af+3Pd8KhX+De7eesi1rLaIR3umtXF/7te4vF2+ZVl1P5WhSbOo9ktW8wKYUpZJRkUFpdes5mfk5+fyX085K6j2PTLXittD0q4deT0ShZduAUr61KID2vnKExvswe3/nSK9mXnoZPhlFma88YXyeGhAzltWGNq3JnropqA1e+vQkHWx3LHxxa/7nSx7fD51fC0Ee1shIXs+oZbcWlx/RWuSq0NTEYDezI3MGyfZ+yNmsHJTod3o7edPftriVyVy2xd3TrSJBrUIsZ+1FaP9WHX086nWByzyCu7BbA19uO8/66RCa9v4XpVwTz6JWdCK5roQsXX7j6M5znT2Kqb1++O7aa0+Wn8XXybfb4P9uUwvG8Mr65vX/9k73RCCueALcgGPLIpbeNmwF/fABHlkDvv5sdb1sjpeRQ7iGWpSxjZdpKTpefxgUbxlRUM2HSXPoFD8JWp/7LKdbTess/NiEHWxv+MSSCjY+P5M5hkSz98xQj39jAyyuOUFhefeEO4YNh2BNcn7KbGmMNC/ULmz3mjPwyPtyQxMS4QAZHN+DDZt/X2vqkY1+4/GBsYDx4R8GB5n9/LdmxomN8tO8jJv8ymVnLZvH90e/p6deTN4e9zobsEl70G8KgjsNUslesTv0FXoKHkx1Pje/CzQPDeXPVUT7dlML3O9O5f1QMNw4IPWcBZIY9TnjqJgZVpPJjwnf8I+4fzVpg7aVlRwB4emIDFssoL4A1c6DjAO3s/XKE0KaibnodijPBrW2sE9AYOWU5rExbybKUZRzKPYRA0LdDX27rfhujQ0drs2OOb9NqEXWeaO1wFQVQZ/j1EuzpxFvXx2vFx4I8+M/Sw4x5ayO/7j9J7RiIjS1c8xmzyqrJrshlfdrqZotvS+JpVhzM5L6R0XV3O13MxtegLBcmvFb/krxxMwAJhxY1KtbWrLiqmEWJi7jjtzsYs3AMr+18DaM08lifx1g9YzXzrprH1TFX/zUVMmEp6Owgeqx1A1cUEzVo20BSSjYlnubl5UdIyCymZ4gHj1/VmYFRPtjoBIbDS5j4+5MEOgfwv79taPJ4qmqMjH93E9UGyW8PD6v/FY85R+HjQRB/g7aMXkN8PERbX/WOtQ0PuJWpNFSyOWMzy1OXszF9I1XGKkJcQ5gYOZEJEROI9LzITCgp4b0rwDsSbvq5eYNW2jU1aGtBQgiGx/oxJNqXn/dk8OZvem6ctx0PJzsGRfkwOLonU11i+agqhcR9XxITf3OTxjP/9zSSc0qZd0uf+if7s+vlXGpWzsXEXQNrnoe81LqncLZyBqOBXVm7WJayjDXH1lBcXYy3ozczYmcwMXIicb5xl58mmZOgXcQ26P7mCVpR6kEl/Eay0Qmu7dORST2CWH0kiy2JObVdKw421+Ea/QpfbXmJ4VU96d29C57Oll8IO7uognfXJjKqsz+juwTUf8ejy7VFsse9os0yaqhuV2sJ/9DP2lTONkBKyeG8wyxPWc7K1JVkl2fjbOvMmLAxTIyYSL/Afg0bdE1Ypt12mtA0AStKI6iEbyYnexum9AxiSs8gpJSknC5lS+JpfkiIZ6XLHoYs+ye9Fj1F92AvhkT7MiTGl95hXucO+DbSKysSqKox8u+GLEheXQGrnga/ztD39sYd2CsMQvrBgZ9afcI/XnScZanLWJ6ynLSiNGx1tgwNHsrjkY8zPGR44+fHJyyD4D7grmq9KC2HSvgWJIQgys+VKD9X+nSazcylM8nxOMaX3tt4p3win2xK4aMNyTja6egX4cNQ0wdA5w5uDb6ScldaHj/vPcG9I6MI923AIhp/fAD5aXDTL+at8hM3Q5u/n30E/BswM6gFeX/v+3z656cIBH069OGWbrcwNmys+fVnCk9oC5I3prtMUZqQSvhNpJtPN3r4xvGd0LM47f8Yctt0in3Hsi0lj61Jp9mcmMNLy7WplL6uDgyJ9mFwtC9DY/zo4HHp+jcGo+Tfiw8R6OHIvSOj6x9U4Qlt9arOkyBqpDlvTyumtnK2Nid/9L/Ma8tKlqUso3dAb14Z+godXCw4xfTocu22nsXSFKW5qITfhGZ2nsXTW55mm1cQgxbeitvdWxjbNYCxXbX+9pMF5WxJOs2WxNNsTjzNL/tOAhDt78qQaF+GxvjSP9IH1/PW5P12x3EOnyrig79dgbN9A/4J1zwHRoO2YLa53AIgfCgc/AlGPVv/aZ0tRElVCSdKTnBNzDWWTfagJXyfaPCNtWy7imImlfCb0FXhV/HGrjdY4BvKoF3L4NeHYMbntckxyNOJ6/p05Lo+HTEaJQmZxWxJymFz4mkW7DjOF7+nYasTXBHqyZBoP4bE+BLq7cybvx1lYKQPExtSC/z4NjjwIwx7HLzCLfMG42bAkvu17ovg3pZps5kkFSQBEOMVY9mGywu0tZAH3tvqPgSVtk8l/CZkb2PPNTHXMO/gPE4MeYDgzW9D1CjoddMF2+p0gq5B7nQNcufOYVFUVBvYfSy/9hvAO2v1vL1GX1u2ec7UbvXv9zcatBW73IO1ZRstpctkWPqINnjbyhK+Pl8PQKyXhc/Ck9aAsQY6qatrlZZHJfwmdl2n65h3cB4/eLjzcPhQbaCzYz/w63TJ/RztbBgc7cvgaF+eHAd5pVX8nnyarUm5dA1yJzbArf5B7P0KMv+Ea+aBfQMGeC/HyQtixmrTM6/8D+gst8xdU9Pn63G1cyXQxcKzaBKWgos/hLS8Re0VRZVWaGIdXDowsuNIfk5aROXUD8HOSVstq4ELgnu72DOpRxAvXx3HTQPC6r9jeT6sfQFCB9UuyWhR3a+B4lPaAimtSGJ+IrFesZatM19Tqa3/22l8q/rwU9oPlfCbwazOsyioLGBl7j6Y9jFkHYTVzTRlb8OrWtIf/2rT9Cl3Gg92ztrgbSshpSQxP9Hy/fepm6GqWM3OUVoslfCbQb8O/Yj0iOS7hO8g9ioYcA/s+AQSljftgbOPwI5PodctENijaY5h76Il/cOLtcXOW4HM0kyKq4st33+fsBTsXbXF3hWlBVIJvxkIIZjZeSYHcw9yIOcAjHkeOvSAxfdoc+ObgpSw4kmtxv2oJp4n330GlOdB8vqmPY6FNMmArdGoTceMHg129VxHWFGamUr4zWRK1BRc7FxYkLBAqzQ5439QU6WtJWs0WP6ACUshdSOMfAZcfCzf/tmiR4OjBxxsHQujJBYkAhDt2YCL1i7nxG4oyVLdOUqLphJ+M3Gxc2Fy5GRWpq0kryIPfKNh4htwbIt29aslVZeb6uV0gT7/sGzbdbF10KZoJizTjt3C6fP0BLsG42p/mRW+GuLoMtDZarOWFKWFUgm/Gc3qPItqYzU/J5rqo/ecBXHXwYaX4dgfljvQ7x9AwXFtoNammWbedp8BVSWgX9U8xzODPl9v+QHbhGUQPkSbqqooLZRK+M0o0jOS/h3688PRH6gx1mizZia+CZ5h8NPtUJZn/kEKM2DLW9BlCkQON7+9+ooYps0/b+HdOlWGKtKK0ojxtGDCz9HDab262Epp8VTCb2azOs/iVOkpNmZs1J5wdIcZ86AkE359QBtsNcfqf4M0wpUvmh9sQ+hsoNt00P8GFYXNe+wGSClMwSANxHpbcMD2qKn2fWdV+15p2VTCb2bDOw6ng0sHbYrmGcG9YfRzcORX2PV54xs/9rs2H37wg1rN+ubW/RowVDb9dFMzNMkMnYTlEBgPHiGWa1NRmoBZCV8I4S2EWC2ESDTdXtCBKYSIF0L8IYQ4JIT4UwhxvTnHbO1sdbZcF3sd205tI6Ug5a8XBt4HUaO1wdasww1v2GiA5U+AewgMfshi8TZIx37gEdqiu3US8xOx19kT6hZqmQaLMyFjp5qdo7QK5p7hzwbWSiljgLWmx+crA26WUnYDxgHvCCE8zTxuq3Z1zNXY6ez47uhZZ/k6HUz/P3Bwh4W3QlVZwxrdMx+yDmg1beydLRtwfQkB3a/W5uOXnrZODJehz9cT5RnVsOUKL+XoCkBCZ9V/r7R85ib8qcB80/35wLTzN5BS6qWUiab7J4FswM/M47ZqPk4+XBV+FUuSl1BaXfrXC67+WtLPSdDO9OurPB/W/gfChmj96NYUNwOkAQ7/Yt04LkKfr7dwd84yrdx0K131S2lfzE34AVLKU6b7mcAlV9IWQvQD7IHki7x+pxBilxBiV05OjpmhtWyzOs+itLqUX5N/PfeF6NFaH/zu/2nlCupj/ctQUQDjX7F+DfaA7uDbCQ7+bN046pBXkcfp8tOWm5JZWaxd3NZ5kvV/74pSD5dN+EKINUKIg3X8TD17OymlBC46xUQIEQh8BdwqpTTWtY2U8lMpZR8pZR8/v7b9JSDON46uPl35LuE75Pkzc0Y+C0G9tMVFCo5fuqGsw7BzLvS+FTrENV3A9SWENnh77PemKxvRSIn52hW2FjvDT1oDhirVnaO0GpdN+FLKMVLK7nX8LAayTIn8TELPrqsNIYQ7sAx4Rkq5zZJvoLUSQjCr8yySC5PZmbnz3Bdt7bWVsYxGbX6+oabuRqSElU+Cg5u2zGBLETcDkFqd/BbE4jN0EpaBsw907G+Z9hSliZnbpbMEuMV0/xbggj4IIYQ9sAj4UkrZcqdvWMG48HF4Onhq9XXO5x0Bk9+B9O2w8ZW6GziyRFtOb9Sz4OzdpLE2iE+UNk3xQMv6507MT8Tb0RsfJwvUFqqp0q45iFW175XWw9yE/wowVgiRCIwxPUYI0UcIMde0zXXAMODvQoh9pp94M4/bJjjaOjI9Zjrr09eTWZp54QZxMyD+Rtj0hpbYz1ZdDqueBf9uWndOSxM3A07tg9w6h2uswqIDtse2QGWh6s5RWhWzEr6UMldKOVpKGWPq+skzPb9LSnm76f7XUko7KWX8WT/7LBB7m3B9p+sxSiM/HP2h7g3Gvwo+0VpVzdLcv57f+h4UNnO9nIY4M1uohSyMYjAaSCpIsmB3znJt4ZeokZZpT1GagbrS1sqCXYMZHjKcnxJ/ospQdeEGDq5af35ZrlY/X0ooSIctb0PXaRAxtNljrhePEG1ZxQMLzS8XYQHpxelUGiotM0NHSq3/PmqUtmSlorQSKuG3ALM6zyKvIo/fjv1W9waBPWDsf0C/ErZ/Aqv/BUjtIquWLO4aOH1UW9LRyiw6YHtyLxSfVFfXKq2OSvgtwICgAYS5h51bX+d8/e+C2HHw2zNwaBEMeRg8LVQeoKl0nQbCpkUM3urz9eiEjijPKPMbS1imva/Yq8xvS1GakUr4LYBO6JjZaSb7c/ZzKPdQ3RsJAVM/Ahc/rV7NoAeaN8jGcPGFyBHaRVhW7tZJzE8kzD0MBxsH8xs7uhzCBrWsmVGKUg8q4bcQU6Kn4GTrdOmzfBcfuGsT3L7aevVyGipuhja4nLHz8ts2IYvN0MlNhuzDanaO0iqphN9CuNu7MylyEitSV1BQUXDxDV39wa1Ds8Vlts6TwMbBqt06pdWlZJRkWCbhHzWVfu6kat8rrY9K+C3IzM4zqTRUsihpkbVDsRxHd4i9Uht3uNgVw00sqSAJwDKrXCUs00pYWGO9AUUxk0r4LUisVyy9A3rz/dHvMRgN1g7HcrpfA6XZ2sVKVlA7Q8fcVa5KcuD4NrWUodJqqYTfwszqPIsTJSfYcsI6ybFJxI4De1erdevo8/S42LkQ5BJkZkMrUbXvldZMJfwWZlToKPyd/FlwtI76Oq2VnZOWJI8sgZrKZj+8Pl9PjGcMwtwSxgnLtBlSLaEqqaI0gkr4LYydzo4ZnWaw9cRWjhUds3Y4ltN9hra4edLaZj2slJLEgkTzB2wrSyB5nfbBpWrfK62USvgt0LWx12Krs730FM3WJmokOHk1e22drLIsiquKzU/4yeu0Bdo7q9k5SuulEn4L5Ovky9jQsSxOWkxZdQPXtm2pbOyg61RtWmNV6eW3t5AzA7Zm19A5uhwcPbX6QIrSSqmE30LN6jKL4upilqUus3YoltN9BlSXmRb+bh4WSfiGGi3mTuNbZmVSRaknlfBbqHi/eDp5dap7CcTWKmwQuAU2a7eOPl9PkEsQbvZujW/k+O/amsFqdo7SyqmE30KdWQJRn69nT/Yea4djGTob6HY1JK6G8vxmOWRifqL53TkJy8DWUSuHrCitmEr4LdiEyAm42bu1rcHb7teAsRqOLG3yQ1UZqkgrTDNvwFZKbbGTyJFg72K54BTFClTCb8GcbJ2YHj2dNcfWkF1W5/rwrU9wL/CKgINNfxFWamEqNbLGvISfeUAr/qa6c5Q2QCX8Fu76TtdjkAYW6q1fU94ihNDO8lM3QXFWkx7KIgO2CctA6LQBW0Vp5VTCb+FC3UMZHDyYH/U/Um2otnY4lhE3A6QRDv/SpIdJzE/ETmdHmLsZhc4SlkHHAVptf0Vp5VTCbwVmdZ7F6fLTrD3evFepNhn/LuDftcln6+jz9UR7RmOra+RUyvw0yDqgLrZS2gyV8FuBIcFDCHENYUFCG6qv0/0aSN8OBceb7BD6fL153TlnrhdQte+VNkIl/FZAJ3TM7DyTPdl7OJp31NrhWEb3a7TbJjrLz6/IJ6c8x7wB24Rl2jcRHwusg6soLYBK+K3EtOhpONo48t3RNjJF0zsCgvvAgaZJ+In5iYAZA7ZleXBsq5qdo7QpKuG3Eh4OHkyInMCylGUUVhZaOxzLiJuh9ZHnWP5bS+2iJ409w9ev1AaWVcJX2hCV8FuRmZ1mUl5TzuKkxdYOxTK6TgNEk3TrJBYk4u3ojY+jT+MaSFgG7sEQGG/RuBTFmlTCb0W6+HShl38vPvnzE1ILU60djvncAyF8iLYSloXrBenztAHbRi16UlWm1e3vNEHVvlfaFJXwW5kXh7yIrc6We9bcQ15FnrXDMV/cDMhLhlP7LNakwWggqSCp8YuWp2yAmnLVnaO0OWYlfCGEtxBitRAi0XTrdYlt3YUQGUKID8w5ZnvX0a0j7416j5zyHB5Y9wCVhuZfMtCiukwBna1Fu3UySjKoMFQ0vv8+YRk4eGjfPhSlDTH3DH82sFZKGQOsNT2+mP8Am8w8ngL09OvJf4f8l/05+3lmyzMYpdHaITWeszdEjYaDP4PRMu+jdsDWuxEJ32gA/QqIvVJbtEVR2hBzE/5UYL7p/nxgWl0bCSF6AwHAb2YeTzG5MvxKHun9CKvSVvHenvesHY554mZA0QlI32aR5vT5enRCR5RHI+bPp2+HslzVnaO0SeYm/AAp5SnT/Uy0pH4OIYQOeBN47HKNCSHuFELsEkLsysnJMTO0tu/v3f7OjNgZzDs4j5/0zbtWrEV1mgC2TtrgrQUk5icS6haKo61jw3dOWAY29hA9xiKxKEpLctmEL4RYI4Q4WMfP1LO3k9qyTHVNtbgHWC6lzLjcsaSUn0op+0gp+/j5+dX7TbRXQgie6f8Mg4MG859t/+H3k79bO6TGcXCFTuO0YmqGGrOb0+frG9d/LyUkLIXIEeBgxgpZitJCXTbhSynHSCm71/GzGMgSQgQCmG7rKto+ELhPCJEGvAHcLIR4xYLvoV2z1dnyxvA3iPSM5NENj9ZeYdrqdL9G60pJ3WBWM2XVZaQXpzfuCtvsw1rBNNWdo7RR5nbpLAFuMd2/BbjgiiAp5Q1SylApZThat86XUspLDe4qDeRq78pHoz/CydaJe9feS05ZK+wOix4LDu5ml1pIKkgCGnmFbcJyQECsqn2vtE3mJvxXgLFCiERgjOkxQog+Qoi55gan1F8Hlw58MPoDCioLuG/dfZRVl1k7pIaxc4Quk+HIr3B4sTZbphHMKqmQsBRC+oLbBUNRitImmJXwpZS5UsrRUsoYU9dPnun5XVLK2+vY/gsp5X3mHFO5uK4+XXlt2Gsk5CUwe/NsDI1MmlYz6H5tmuYPN8N78fDHR1BR1KAm9Pl6nG2dCXINatixCzO0i79Ud47ShqkrbduYER1H8ETfJ1ifvp43dr1h7XAaxr8LPLAXrv9aq2Oz6il4uxusegbyj9WricT8RGK8YtCJBv5pJyzXbjtPamDQitJ6qITfBt3Q5QZu6HIDXx/5mm+PfGvtcBpGZ6N17dy2Eu5YBzFXwraPtTP+H26B9B0X3VVK2fgZOkeXgW8s+EY3PnZFaeFUwm+jHu/zOCM6juDVna+yMX2jtcNpnODeMGMePPSn1t2Tsh7mjYW5Y+DQogumcGaVZVFUVdTwGTrl+ZC2RXXnKG2eSvhtlI3OhleHvkonr048vulxDucetnZIjecRAmNfgIcPw/jXtembP/4d3rsCfn8fKrT1Ac5MSW3wGX7iajDWqO4cpc1TCb8Nc7Zz5sPRH+Lh4MF9a+8jszTT2iGZx8EV+t8J9+2Cmd+CZyj89iy81RVWzEZ/cjvQiFWuEpaCawcI6tUEQStKy6ESfhvn5+zHh6M/pKymjHvX3ktJVYm1QzKfzkbrfrl1Gdy5Ubu/8zP0Oz+mA3a4Zx6uf3396gpIXAOdJ4BO/XdQ2jb1F94OxHrF8tbwt0guSOaxTY9RYzS/fEGLERQPV38KDx0g0TOQ2Ioy+Pwq+GyUVpvHUH3p/VM3QXUpdFL990rbpxJ+OzEoeBDPDniWrSe28t/t/0VaeIUpa6t28SPVUEpsr9th4ptav/5P/4B3e8LWd6G8oO4dE5aCvRtEDG3WeBXFGlTCb0dmxM7gtu638aP+R+Yfmn/5HVqRlMIUamQNMT5doe/tWj//rO/BOxJW/1vr51/+BOSl/LWT0QBHl0PMWLB1sF7witJMbK0dgNK8Huz1IBnFGby5+02C3YIZGzbW2iFZRGLBeTN0dDqtAmencXDqT9j2Eez6HHZ8qvX5D7hHW2mrNEdNx1TaDZXw2xmd0PHSkJfIKsviqc1PEeAcQA+/HtYOy2z6fD12OjvCPMIufDGwB0z/PxjzPOycCzvnaV05Dh6gs9PO8BWlHVBdOu2Qo60j7416Dz8nP+5fdz8ZxZddqqDF0+frifSIxE53iWUJ3TrAqGfh4UMw6W1wD9JW23L0aL5AFcWKVMJvp7wdvflwzIfUGGu4Z+09FFYWWjsksyTmJ9b/git7Z+hzG9y7TTvzV5R2QiX8dizSI5J3Rr5DenE6j2x4hOrLTWFsoQoqCsguy25cDR1FaUdUwm/n+nboy5xBc9iRuYPn/3i+VU7XPDNg26hVrhSlHVGDtgpToqaQUZzBx/s/JtQtlLt63mXtkBrErEVPFKUdUQlfAeCfPf9JenE6H+z7gBC3ECZGtp6pion5iXg5eOHr5GvtUBSlRVMJXwFACMGcQXM4VXqKf239Fx1cOtA7oLe1w6oXfb6eGK8YhBDWDkVRWjTVh6/Usrex592R7xLsGsyD6x8krTDN2iFdllEaSSpIUt05ilIPKuEr5/Bw8OCj0R+hQ8e9a+8lvyLf2iFdUkZxBuU15SrhK0o9qISvXKCje0feG/UemaWZPLj+QSoNldYO6aLODNiqGTqKcnkq4St1iveP56WhL7E3ey9fHf7K2uFclD5fj0AQ5Rll7VAUpcVTCV+5qHHh4+gT0IdFiYta7Pz8xPxEwtzDcLJ1snYoitLiqYSvXNK06GkcLz7Onuw91g6lTmdm6CiKcnkq4SuXNDZsLM62zixKXGTtUC5QVl1GenG6SviKUk8q4SuX5GznzPiI8fx27DdKq0utHc45kguSkUg1Q0dR6kklfOWypkVPo7ymnFVpq6wdyjlqSyp4qoSvKPWhEr5yWT39ehLhEcEvSb9YO5Rz6PP1ONk6EewWbO1QFKVVMCvhCyG8hRCrhRCJpluvi2wXKoT4TQhxRAhxWAgRbs5xleYlhGBa9DT2Zu8ltTDV2uHUSixIJMYrBp1Q5y2KUh/m/k+ZDayVUsYAa02P6/Il8LqUsgvQD8g287hKM5scORkbYdNizvKllNoMHU81YKso9WVuwp8KzDfdnw9MO38DIURXwFZKuRpASlkipSwz87hKM/Nz9mNo8FCWJC+hxlhj7XDILsumsLJQDdgqSgOYm/ADpJSnTPczgYA6tokFCoQQPwsh9gohXhdC2NTVmBDiTiHELiHErpycHDNDUyxtWsw0TpefZuuJrdYOpXbRE5XwFaX+LpvwhRBrhBAH6/iZevZ2UrsUs67LMW2BocBjQF8gEvh7XceSUn4qpewjpezj5+fX0PeiNLFhIcPwdvRmUZL15+SrGjqK0nCXrYcvpRxzsdeEEFlCiEAp5SkhRCB1981nAPuklCmmfX4BBgDzGheyYi12OjsmR07mmyPfkFeRh7ejt9Vi0efrCXAOwMPBw2oxKEprY26XzhLgFtP9W4DFdWyzE/AUQpw5ZR8FHDbzuIqVTIueRo2sYWnyUqvGkZifqLpzFKWBzE34rwBjhRCJwBjTY4QQfYQQcwGklAa07py1QogDgAA+M/O4ipVEe0UT5xvHoiTrFVSrNlSTUpiiunMUpYHMSvhSylwp5WgpZYyUcoyUMs/0/C4p5e1nbbdaStlDShknpfy7lLLK3MAV65kWPY2kgiQO5R6yyvFTi1KpMdaoM3xFaSB1xYrSYOMjxuNg42C1gmqJ+WqGjqI0hkr4SoO52bsxNmwsK1JXUFFT0ezH1+frsdXZEu4R3uzHVpTWTCV8pVGmR0+nuLqYtcfXNvux9fl6Ij0isdPZNfuxFaU1UwlfaZQ+HfoQ7BpslTn5aoaOojSOSvhKo+iEjqnRU9l+ajsnSk4023ELKwvJKstSCV9RGkElfKXRpkZNRSBYnFTX5RdNQ11hqyiNpxK+0mhBrkEMCBzA4qTFGKWxWY6pZugoSuOphK+YZXrMdE6WnmRH5o5mOZ4+X4+ngyd+TqrWkqI0lEr4illGhY7Czd6t2ebkJ+Zri54IIZrleIrSlqiEr5jFwcaBCRETWHNsDYWVhU16LKM0kligZugoSmOphK+YbXrMdKqMVaxMXdmkxzlRfILymnKV8BWlkVTCV8zW1bsrsV6xTT4nv3aGjlrWUFEaRSV8xWxCCKZHT+dQ7qHapNwU9AV6BIIoz6gmO4aitGUq4SsWMTFyIrY62yZd5DwxP5FQ91Cc7Zyb7BiK0pZddsUrRakPL0cvRnYcydLkpTzc62HsbCxf50afr28X3TnV1dVkZGRQUdH8hemU1sPR0ZGQkBDs7Or/f00lfMVipkVPY/Wx1WzM2MiYsIuujNko5TXlHC86zsSIiRZttyXKyMjAzc2N8PBwNf1UqZOUktzcXDIyMoiIiKj3fqpLR7GYQUGD8Hfyb5LB2+SCZCSyXczQqaiowMfHRyV75aKEEPj4+DT4W6BK+IrF2OpsmRI9hS0ntpBdVtd69o3X3mroqGSvXE5j/kZUwlcsalr0NIzSyJLkJRZtV5+vx8nWiRC3EIu2qyjtiUr4ikWFuYfRy78Xi5MWW3SR88T8RGI8Y9AJ9SfbXH755ReEECQkJFxyu3feeYeysrJGH+eLL77gvvvua/T+Sv2p/z2KxU2PmU5aURr7cvZZpD0ppTZDp51057QUCxYsYMiQISxYsOCS25mb8JXmo2bpKBZ3ZdiV/Hf7f1mUuIgr/K8wu72c8hwKKgvaZcKf8+shDp8ssmibXYPceW5yt0tuU1JSwpYtW1i/fj2TJ09mzpw5GAwGnnzySVauXIlOp+OOO+5ASsnJkycZOXIkvr6+rF+/HldXV0pKSgBYuHAhS5cu5YsvvuDXX3/lxRdfpKqqCh8fH7755hsCAgIs+t6US1MJX7E4ZztnxoWPY2XaSmb3m232hVKqBn7zW7x4MePGjSM2NhYfHx92797Njh07SEtLY9++fdja2pKXl4e3tzdvvfUW69evx9fX95JtDhkyhG3btiGEYO7cubz22mu8+eabzfSOFFAJX2ki02OmsyhpEavSVjE9ZrpZbZ2ZodMeE/7lzsSbyoIFC3jwwQcBmDlzJgsWLCA1NZW7774bW1stbXh7ezeozYyMDK6//npOnTpFVVVVg+aPK5ahEr7SJOL94gl3D+eXpF8skvD9nf3xcPCwUHTKpeTl5bFu3ToOHDiAEAKDwYAQgr59+9Zr/7OnC549T/z+++/nkUceYcqUKWzYsIHnn3/e0qErl6EGbZUmIYRgWvQ09mTvIa0wzay2EvNVDfzmtHDhQm666SaOHTtGWloa6enpRERE0LNnTz755BNqamoA7YMBwM3NjeLi4tr9AwICOHLkCEajkUWL/roIr7CwkODgYADmz5/fjO9IOUMlfKXJTImago2wYXFy4xc5rzZWk1yY3C4HbK1lwYIFTJ9+7reya665hlOnThEaGkqPHj3o2bMn3377LQB33nkn48aNY+TIkQC88sorTJo0iUGDBhEYGFjbxvPPP8+1115L7969L9vfrzQNYcm50pbUp08fuWvXLmuHoZjp3rX3kpCbwKoZq7DVNbwHMTE/kauXXM3LQ19mUuSkJoiw5Tly5AhdunSxdhhKK1DX34oQYreUsk9d25t1hi+E8BZCrBZCJJpuvS6y3WtCiENCiCNCiPeEum683ZgePZ3s8mx+P/l7o/ZXM3QUxXLM7dKZDayVUsYAa02PzyGEGAQMBnoA3YG+wHAzj6u0EsNDhuPl4NXoOvn6fD22wpYIdzWjQ1HMZW7CnwqcGX2ZD0yrYxsJOAL2gANgB2SZeVyllbCzsWNS1CTWp68nvyK/wfvr8/VEeEY0SX19RWlvzE34AVLKU6b7mcAFl81JKf8A1gOnTD+rpJRH6mpMCHGnEGKXEGJXTk6OmaEpLcX06OnUGGtYlrKswfsmFqgZOopiKZdN+EKINUKIg3X8TD17O6mN/l4wAiyEiAa6ACFAMDBKCDG0rmNJKT+VUvaRUvbx8/Nr1BtSWp4Yrxi6+3Tn56SfG1RQrbCykMzSzHaxypWiNIfLJnwp5RgpZfc6fhYDWUKIQADTbV1F0KcD26SUJVLKEmAFMNCSb0Jp+aZFTyMxP5HDeYfrvY8asFUUyzK3S2cJcIvp/i1AXROujwPDhRC2Qgg7tAHbOrt0lLZrfOR4HGwcWJRY/9WwEgtUwrcWGxsb4uPja3/S0tKsHRLQ+MqcQggeffTR2sdvvPFG7ZW+zz//PM7OzmRn/3W+6urqes7+BoOBK664gkmTWvfUYHMT/ivAWCFEIjDG9BghRB8hxFzTNguBZOAAsB/YL6X81czjKq2Mu707o0NHszx1OZWGynrto8/X427vjr+zfxNHp5zPycmJffv21f6Eh4fXa78zV+E2lcYmfAcHB37++WdOnz5d5+u+vr6XLOT27rvvtolrI8yqpSOlzAVG1/H8LuB2030DcJc5x1Hahukx01meupx1x9cxPmL8ZbfX5+uJ9Ypt38v9rZgNmQcs22aHOBj/SoN327dvH3fffTdlZWVERUXx+eef4+XlxYgRI4iPj2fLli3MmjWLESNG8Mgjj1BSUoKvry9ffPEFgYGBJCUlcffdd5OTk4ONjQ0//vgjAQEBTJ06lfz8fKqrq3nxxReZOnUqpaWlXHfddWRkZGAwGPjXv/5FVlbWBaWY68vW1pY777yTt99+m5deeumC12+77Ta++OILnnzyyQuKwmVkZLBs2TKeeeYZ3nrrrQb/3loSVVpBaTb9OvQjyCWoXt06RmkkKT9JdedYSXl5eW13zpkyCzfffDOvvvoqf/75J3FxccyZM6d2+6qqKnbt2sUDDzzA/fffz8KFC9m9eze33XYbzzzzDAA33HAD9957L/v37+f3338nMDAQR0dHFi1axJ49e1i/fj2PPvooUkpWrlxJUFAQ+/fv5+DBg4wbN44HHniAoKAg1q9f36Bkf8a9997LN998Q2Fh4QWvubq6ctttt/Huu+9e8NpDDz3Ea6+9hk7X+tOlqpapNBud0DEtehof7/+YkyUnCXINuui2J0pOUFZTpmroNOJM3BLOdOmcUVhYSEFBAcOHa9dM3nLLLVx77bW1r19//fUAHD16lIMHDzJ27FhA6/sODAykuLiYEydO1H54ODo6AlBdXc3TTz/Npk2b0Ol0nDhxgqysLOLi4nj00Ud58sknmTRpEkOH1jmxr0Hc3d25+eabee+993Bycrrg9QceeID4+Hgee+yx2ueWLl2Kv78/vXv3ZsOGDWbHYG2t/yNLaVWmRE9BIi9bUK0918BvjVxcXABtOcpu3brV9v0fOHCA33777aL7ffPNN+Tk5LB792727dtHQEAAFRUVxMbGsmfPHuLi4nj22Wd54YUXLnn87du3134jWbJkyUW3e+ihh5g3bx6lpaUXvObp6cnf/vY3Pvzww9rntm7dypIlSwgPD2fmzJmsW7eOG2+88XK/jhZLJXylWQW7BtM/sD+LkxZjlMaLbpeYn4hAEO0Z3YzRKRfj4eGBl5cXmzdvBuCrr76qPds/W6dOncjJyeGPP/4AtDP4Q4cO4ebmRkhICL/88gsAlZWVlJWVUVhYiL+/P3Z2dqxfv55jx44BcPLkSZydnbnxxht5/PHH2bNnD3BhKeYz+vfvX/shM2XKlIu+D29vb6677jrmzZtX5+uPPPLIOSWgX375ZTIyMkhLS+O7775j1KhRfP311/X8rbU8KuErzW569HROlJxgZ+bOi26jz9cT4hZi9vKIiuXMnz+fxx9/nB49erBv3z7+/e9/X7CNvb09Cxcu5Mknn6Rnz57Ex8fz++9a4byvvvqK9957jx49ejBo0CAyMzO54YYb2LVrF3FxcXz55Zd07twZgAMHDtCvXz/i4+OZM2cOzz77LHBhKebGePTRRy85W2f69OlUVtZvJllro8ojK82uoqaCUT+MYnjH4bw89OU6t5m8aDJRnlG8M/Kd5g2uBVDlkZX6atbyyIrSGI62jkyInMDqY6sprrrw63l5TTnHi4+r/ntFsTCV8BWrmB49nUpDJStSV1zwWkpBCkZpVDN0FMXCVMJXrKKrT1eiPaPrrJOvZugoStNQCV+xCiEE06Onc+D0AZLyk855TZ+vx8nWiRDXECtFpyhtk0r4itVMipqErbC94Cw/MT+RKI8obHQ21glMUdoolfAVq/F29GZExxH8mvIr1cZqQLtwR5+vJ9ZbdecoiqWphK9Y1fSY6eRV5LEpYxMAuRW55Ffmq/57K1PlkS1fHvnUqVNceeWVFzx//u/6lVe0chpLly7liiuuoGfPnnTt2pVPPvmk0cc+Q9XSUaxqUNAg/Jz8+CXxF0aHjkafpw3YqlWurOv8Wjr1VVNTg61t06WVd955hxtvvBFn54ZdkHemPPJTTz2Fr6/vBa+fKY/86quv1rn/mfLIRUVFjYobYOXKlVx11VUXPF/X77q6upo777yTHTt2EBISQmVlpUU+dFXCV6zKVmfL5KjJzD80n5yynNoZOmpKpubVHa+SkJdg0TY7e3fmyX5PNng/VR657vLI2dnZjB8/nt27d7N//37i4+M5duwYoaGhREVFceDAAZydnVm5ciXPPfdcveItLi6mpqYGHx8fQPvA6tSpU73f78WoLh3F6qZFT8MgDfya8iuJBYn4O/nj5ehl7bDaNVUe+S+XK4/s7+9PRUUFRUVFbN68mT59+rB582aOHTuGv78/zs7OGAwGjh49SteuXS/5u46Pj+f777/H29ubKVOmEBYWxqxZs/jmm28wGi9ee6q+1Bm+YnURHhFc4X8FvyT9gr3OXp3dn6UxZ+KWoMoja+pbHnnQoEFs3bqVTZs28fTTT7Ny5UqklLVxb9++nf79+9e578W6z+bOncuBAwdYs2YNb7zxBqtXr+aLL75o0Hs+nzrDV1qE6dHTSS1M5Wj+UTVg2wq19/LIw4YNqz2rnzp1Kvv372fLli21CX/FihWMGzfuku+hLnFxcTz88MOsXr2an376qcH7n08lfKVFuDL8SpxstbMudYbf8qjyyJcujzx06FC+/vprYmJi0Ol0eHt7s3z5coYMGQLA2rVrGTNmzEXjOl9JSck53yj27dtHWFhYvfe/GJXwlRbBxc6Fq8K1GQzqDL9lUuWRLy48PBwpJcOGDQNgyJAheHp64uXlRU5ODo6Ojri5udW57/l9+LNnz0ZKyWuvvUanTp2Ij4/nueeeM7s7B1R5ZKUFSS9OZ6F+IQ9c8UC7vspWlUduW77++msyMjKYPXu2xdtuaHlkNWirtBgd3TrycO+HrR2GolhUS1oSUXXpKIqitBMq4StKC9RSu1qVlqMxfyMq4StKC+Po6Ehubq5K+spFSSnJzc2tvZ6hvlQfvqK0MCEhIWRkZJCTk2PtUJQWzNHRkZCQhq0ZoRK+orQwdnZ2REREWDsMpQ1SXTqKoijthEr4iqIo7YRK+IqiKO1Ei73SVgiRAxwzowlfoO7rp1s/9d5ar7b8/tR7axnCpJR+db3QYhO+uYQQuy52eXFrp95b69WW3596by2f6tJRFEVpJ1TCVxRFaSfacsL/1NoBNCH13lqvtvz+1Htr4dpsH76iKIpyrrZ8hq8oiqKcRSV8RVGUdqLNJXwhxDghxFEhRJIQwvJLzFiREKKjEGK9EOKwEOKQEOJBa8dkaUIIGyHEXiHEUmvHYklCCE8hxEIhRIIQ4ogQYqC1Y7IkIcTDpr/Jg0KIBUKIhpVxbEGEEJ8LIbKFEAfPes5bCLFaCJFouvWyZoyN1aYSvhDCBvgQGA90BWYJIbpaNyqLqgEelVJ2BQYA97ax9wfwIHDE2kE0gXeBlVLKzkBP2tB7FEIEAw8AfaSU3QEbYKZ1ozLLF8C4856bDayVUsYAa02PW502lfCBfkCSlDJFSlkFfAdMtXJMFiOlPCWl3GO6X4yWNIKtG5XlCCFCgInAXGvHYklCCA9gGDAPQEpZJaUssGpQlmcLOAkhbAFn4KSV42k0KeUmIO+8p6cC80335wPTmjMmS2lrCT8YSD/rcQZtKCGeTQgRDlwBbLdyKJb0DvAEYLRyHJYWAeQA/zN1V80VQrhYOyhLkVKeAN4AjgOngEIp5W/WjcriAqSUp0z3M4EAawbTWG0t4bcLQghX4CfgISllkbXjsQQhxCQgW0q529qxNAFboBfwsZTyCqCUVtolUBdTf/ZUtA+2IMBFCNFyVu62MKnNZW+V89nbWsI/AXQ863GI6bk2Qwhhh5bsv5FS/mzteCxoMDBFCJGG1hU3SgjxtXVDspgMIENKeebb2EK0D4C2YgyQKqXMkVJWAz8Dg6wck6VlCSECAUy32VaOp1HaWsLfCcQIISKEEPZoA0dLrByTxQghBFo/8BEp5VvWjseSpJRPSSlDpJThaP9u66SUbeIsUUqZCaQLITqZnhoNHLZiSJZ2HBgghHA2/Y2Opg0NSpssAW4x3b8FWGzFWBqtTS1xKKWsEULcB6xCmynwuZTykJXDsqTBwE3AASHEPtNzT0spl1svJKWe7ge+MZ2IpAC3Wjkei5FSbhdCLAT2oM0k20srLkUghFgAjAB8hRAZwHPAK8APQoh/oJVtv856ETaeKq2gKIrSTrS1Lh1FURTlIlTCVxRFaSdUwlcURWknVMJXFEVpJ1TCVxRFaSdUwlcURWknVMJXFEVpJ/4floHypiVppb8AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "X_grid = np.arange(len(X_test))\n",
        "X_grid = X_grid.reshape((len(X_grid), 1))\n",
        "plt.plot(X_grid, y_test)\n",
        "\n",
        "\n",
        "plt.plot(X_grid, y_pred_nn4)\n",
        "plt.plot(X_grid, y_pred_nn4_es)\n",
        "plt.legend(('Actual','Forecast - NN4', 'Forecast - NN4 w/ ES'))\n",
        "plt.title('NeuralNet Test data')\n",
        "#plt.savefig('NeuralNet Test')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "MLP Practice - Questions.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "a0ce2025c5f5b601305daa41e5dacab6ef395755c047fd94bd2ab00fe25f9a50"
    },
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
